---
title: Сбор событий потоковой передачи — Центры событий Azure | Документация Майкрософт
description: В этой статье описывается функция сбора, которая позволяет записывать события потоковой передачи из Центров событий Azure
services: event-hubs
documentationcenter: ''
author: ShubhaVijayasarathy
manager: timlt
editor: ''
ms.assetid: e53cdeea-8a6a-474e-9f96-59d43c0e8562
ms.service: event-hubs
ms.workload: na
ms.custom: seodec18
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 02/12/2020
ms.author: shvija
ms.openlocfilehash: c166f4cace6a8cc25b36a84f4614033801e69a51
ms.sourcegitcommit: 849bb1729b89d075eed579aa36395bf4d29f3bd9
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/28/2020
ms.locfileid: "79265016"
---
# <a name="capture-events-through-azure-event-hubs-in-azure-blob-storage-or-azure-data-lake-storage"></a>Сбор событий из Центров событий Azure в хранилище BLOB-объектов Azure или Azure Data Lake Storage
Концентраторы событий Azure позволяют автоматически записывать данные потоковой передачи в концентраторы событий в [хранилище BLOB-объектов Azure](https://azure.microsoft.com/services/storage/blobs/) или Azure Data Lake Storage из любой учетной записи [поколения 1 или Gen 2](https://azure.microsoft.com/services/data-lake-store/) с дополнительной гибкостью при указании интервала времени или размера. Настройка функции "Сбор" выполняется быстро, ее использование не влечет дополнительных административных расходов, а масштабирование осуществляется автоматически на основе [единиц пропускной способности](event-hubs-scalability.md#throughput-units) Центров событий. Функция "Сбор" в Центрах событий — это самый удобный способ передачи потоковых данных в Azure. Он позволяет сосредоточиться на обработке данных, а не на их записи.

Кроме того, функция "Сбор" в Центрах событий обеспечивает обработку конвейеров в режиме реального времени и на основе пакетов в одном потоке, благодаря чему вы можете создавать решения, масштабируемые по мере необходимости. Независимо от того, что вам нужно (создать системы на основе пакетов с учетом будущих потребностей в обработке данных в режиме реального времени или добавить эффективный холодный путь к имеющемуся решению для обработки в режиме реального времени), функция "Сбор" в Центрах событий упрощает работу с потоковыми данными.


## <a name="how-event-hubs-capture-works"></a>Как работает функция "Сбор" в Центрах событий

Центры событий — это устойчивый буфер для хранения входящих данных телеметрии в течение определенного времени, подобный распределенному журналу. Масштабирование в Центрах событий выполняется в рамках [модели секционированных потребителей](event-hubs-scalability.md#partitions). Каждая секция — это независимый сегмент данных, потребление которого осуществляется отдельно. По истечении настроенного срока хранения эти данные устаревают, поэтому определенный концентратор событий никогда не заполняется полностью.

Запись концентраторов событий позволяет указать собственную учетную запись хранилища BLOB-объектов Azure, контейнер или учетную запись Azure Data Lake Storage, которая используется для хранения захваченных данных. Эти учетные записи могут находиться в том же регионе, что и концентратор событий, или в другом. Это дает гибкие возможности использования функции "Сбор" в Центрах событий.

Собранные данные записываются в формате [Apache Avro][Apache Avro] — сжатый быстрый двоичный формат, обеспечивающий эффективную структуру данных за счет встроенной схемы. Этот формат широко используется в экосистеме Hadoop, Stream Analytics и фабрике данных Azure. Работа с Avro более подробно описана далее в этой статье.

### <a name="capture-windowing"></a>Управление окнами в записи

В функции "Сбор" в Центрах событий можно настроить окно управления сбором. Это окно с минимальным размером и продолжительностью, для которого предусмотрена политика "побеждает первый". Это означает, что первый обнаруженный триггер активирует операцию записи. При наличии окна записи размером в 100 МБ и продолжительностью 15 минут для отправки данных со скоростью 1 МБ/с сначала используется окно размера, а затем — окно времени. Запись каждой секции выполняется отдельно, а запись выполненного блочного BLOB-объекта осуществляется в процессе записи. Имя блочного BLOB-объекта зависит от времени создания записи. Соглашение об именовании хранилища выглядит следующим образом:

```
{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
```

Обратите внимание, что значения даты дополняются нулями. Пример имени файла может выглядеть так:

```
https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro
```

В случае, если большой двоичный объект службы хранилища Azure временно недоступен, запись концентраторов событий сохраняет данные в течение срока хранения данных, настроенного в концентраторе событий, и заполняет данные, когда учетная запись хранения снова станет доступной.

### <a name="scaling-to-throughput-units"></a>Масштабирование единиц пропускной способности

Трафик концентраторов событий контролируется [единицами пропускной способности](event-hubs-scalability.md#throughput-units). Одна единица пропускной способности разрешает передачу до 1 МБ/с или 1000 событий/с для входящих данных или до 2 МБ/с или 2000 событий/с для исходящих данных. Для Центров событий (цен. категория "Стандартный") можно настроить от 1 до 20 единиц пропускной способности. Кроме того, можно отправить запрос на увеличение квоты в [службу поддержки][support request]. Использование единиц пропускной способности свыше приобретенного количества регулируется. Функция "Сбор" в Центрах событий копирует данные непосредственно из внутреннего хранилища Центров событий. При этом выполняется обход квоты на единицы пропускной способности для исходящего трафика, а этот трафик сохраняется для других средств обработки, например Stream Analytics или Spark.

После настройки функция "Сбор" в Центрах событий автоматически запускается при отправке первого события и продолжает работать. Чтобы позволить операции последующей обработки установить, что процесс выполняется, при отсутствии данных Центры событий записывают пустые файлы. Этот процесс обеспечивает прогнозируемую периодичность и позволяет получить маркер, необходимый для пакетных обработчиков.

## <a name="setting-up-event-hubs-capture"></a>Настройка функции "Сбор" в Центрах событий

Запись можно настроить при создании концентратора событий с помощью [портала Azure](https://portal.azure.com) или с помощью шаблонов Azure Resource Manager. Дополнительные сведения см. в следующих статьях:

- [Включение записи концентраторов событий с помощью портал Azure](event-hubs-capture-enable-through-portal.md)
- [Создание пространства имен Центров событий с концентратором событий и включение записи с помощью шаблона Azure Resource Manager](event-hubs-resource-manager-namespace-event-hub-enable-capture.md)


## <a name="exploring-the-captured-files-and-working-with-avro"></a>Просмотр собранных файлов и работа с Avro

Функция "Сбор" в Центрах событий создает файлы в формате Avro, как указано в настроенном окне времени. Эти файлы можно просмотреть в любом инструменте, например в [Azure Storage Explorer][Azure Storage Explorer]. Чтобы выполнить определенные действия с этими файлами, их можно скачать локально.

Файлы, созданные записью с помощью функции "Сбор" в Центрах событий, имеют следующую схему Avro.

![Схема Avro][3]

Файлы Avro можно легко просмотреть с помощью [инструментов Avro][Avro Tools] (JAR-файла) из Apache. Вы можете также использовать [Apache Drill][Apache Drill] для облегченного взаимодействия с SQL или [Apache Spark][Apache Spark], чтобы выполнить сложную распределенную обработку принятых данных. 

### <a name="use-apache-drill"></a>Использование Apache Drill

[Apache Drill][Apache Drill] — "механизм SQL-запросов с открытым исходным кодом для исследования больших данных", который может запрашивать структурированные и полуструктурированные данные, независимо от расположения. Этот механизм может работать как отдельный узел или как огромный кластер для высокой производительности.

Доступна встроенная поддержка хранилища BLOB-объектов Azure, которая упрощает запрос данных в файле Avro, как описано в документации:

[Переход на Apache: подключаемый модуль хранилища BLOB-объектов Azure][Apache Drill: Azure Blob Storage Plugin]

Чтобы облегчить запрос захваченных файлов, вы можете создать и запустить виртуальную машину с включенной Apache Drill через контейнер для доступа к хранилищу BLOB-объектов Azure.

https://github.com/yorek/apache-drill-azure-blob

Полный законченный пример доступен в потоковой передаче масштабируемого репозитория.

[Потоковая передача в масштабе: запись концентраторов событий]

### <a name="use-apache-spark"></a>Использование Apache Spark

[Apache Spark][Apache Spark] — это "единый аналитический механизм для крупномасштабной обработки данных". Он поддерживает разные языки, включая SQL, и может легко связываться с хранилищем BLOB-объектов Azure. Существует несколько вариантов выполнения Apache Spark в Azure, и каждый обеспечивает простой доступ к хранилищу BLOB-объектов Azure:

- [HDInsight: файлы адресов в службе хранилища Azure][HDInsight: Address files in Azure storage]
- [Azure Databricks: хранилище BLOB-объектов Azure][Azure Databricks: Azure Blob Storage]
- [Служба Azure Kubernetes (AKS)](https://docs.microsoft.com/azure/aks/spark-job) 

### <a name="use-avro-tools"></a>Использование средств Avro

[Средства Avro][Avro Tools] доступны в виде пакета JAR. После того как вы загрузили этот JAR-файл, чтобы просмотреть схему определенного файла Avro, выполните следующую команду:

```shell
java -jar avro-tools-1.9.1.jar getschema <name of capture file>
```

Эта команда возвращает следующее:

```json
{

    "type":"record",
    "name":"EventData",
    "namespace":"Microsoft.ServiceBus.Messaging",
    "fields":[
                 {"name":"SequenceNumber","type":"long"},
                 {"name":"Offset","type":"string"},
                 {"name":"EnqueuedTimeUtc","type":"string"},
                 {"name":"SystemProperties","type":{"type":"map","values":["long","double","string","bytes"]}},
                 {"name":"Properties","type":{"type":"map","values":["long","double","string","bytes"]}},
                 {"name":"Body","type":["null","bytes"]}
             ]
}
```

Средства Avro можно также использовать для преобразования файлов в формат JSON и выполнения других задач обработки.

Чтобы выполнить более расширенную обработку, скачайте и установите Avro для определенной платформы. На момент написания статьи средства Avro доступны для следующих платформ: C, C++, C\#, Java, NodeJS, Perl, PHP, Python и Ruby.

Apache Avro предоставляет руководства по началу работы для платформ [Java][Java] и [Python][Python]. Вы также можете ознакомиться со статьей [Начало работы с записью концентраторов событий](event-hubs-capture-python.md) .

## <a name="how-event-hubs-capture-is-charged"></a>Выставление счета за использование функции "Сбор" в Центрах событий

Выставление счета за использование функции "Сбор" в Центрах событий осуществляется подобно тарификации за единицы пропускной способности, то есть каждый час. Размер платы прямо пропорционален количеству единиц пропускной способности, приобретенных для пространства имен. Так же как и с единицами пропускной способности, единицы измерения при использовании функции "Сбор" в Центрах событий можно регулировать, чтобы обеспечить соответствующую производительность. Единицы измерения действуют совместно. Дополнительные сведения о ценах см. на странице цен на [Центры событий](https://azure.microsoft.com/pricing/details/event-hubs/). 

Обратите внимание, что захват не потребляет квоту исходящего трафика, так как счет выставляется отдельно. 

## <a name="integration-with-event-grid"></a>Интеграция со службой "Сетка событий" 

Можно создать подписку на Сетку событий Azure с пространством имен Центров событий в качестве источника. Сведения о том, как создать подписку на Сетку событий с помощью концентратора событий в качестве источника и приложения Функций Azure в качестве приемника, см. в руководстве [Обработка и перемещение записанных данных из концентраторов событий в хранилище данных SQL с помощью служб "Сетка событий" и "Функции Azure"](store-captured-data-data-warehouse.md).

## <a name="next-steps"></a>Дальнейшие шаги
Функция "Сбор" в Центрах событий — это самый быстрый способ передать данные в Azure. С помощью знакомых средств и платформ (Azure Data Lake, фабрики данных Azure и Azure HDInsight) можно выполнять необходимую пакетную обработку и другие операции анализа в любом масштабе.

Сведения о том, как включить эту функцию с помощью шаблона портал Azure и Azure Resource Manager:

- [Использование портала Azure для включения функции "Сбор" в Центрах событий](event-hubs-capture-enable-through-portal.md)
- [Использование шаблона Azure Resource Manager для включения записи концентраторов событий](event-hubs-resource-manager-namespace-event-hub-enable-capture.md)


[Apache Avro]: https://avro.apache.org/
[Apache Drill]: https://drill.apache.org/
[Apache Spark]: https://spark.apache.org/
[support request]: https://portal.azure.com/?#blade/Microsoft_Azure_Support/HelpAndSupportBlade
[Azure Storage Explorer]: https://azurestorageexplorer.codeplex.com/
[3]: ./media/event-hubs-capture-overview/event-hubs-capture3.png
[Avro Tools]: https://www.apache.org/dist/avro/stable/java/avro-tools-1.9.2.jar
[Java]: https://avro.apache.org/docs/current/gettingstartedjava.html
[Python]: https://avro.apache.org/docs/current/gettingstartedpython.html
[Event Hubs overview]: event-hubs-what-is-event-hubs.md
[HDInsight: Address files in Azure storage]:https://docs.microsoft.com/azure/hdinsight/hdinsight-hadoop-use-blob-storage
[Azure Databricks: Azure Blob Storage]:https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[Apache Drill: Azure Blob Storage Plugin]:https://drill.apache.org/docs/azure-blob-storage-plugin/
[Потоковая передача в масштабе: запись концентраторов событий]:https://github.com/yorek/streaming-at-scale/tree/master/event-hubs-capture
