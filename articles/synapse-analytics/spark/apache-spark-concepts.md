---
title: Основные принципы работы Apache Spark
description: Общие сведения об использовании Apache Spark в Azure Synapse Analytics, включая разные понятия.
services: synapse-analytics
author: euangMS
ms.service: synapse-analytics
ms.topic: overview
ms.subservice: spark
ms.date: 04/15/2020
ms.author: euang
ms.reviewer: euang
ms.openlocfilehash: 74e85906742207d6cde0b7c4cc5c021c23ee4c7b
ms.sourcegitcommit: 32c521a2ef396d121e71ba682e098092ac673b30
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/25/2020
ms.locfileid: "91260144"
---
# <a name="apache-spark-in-azure-synapse-analytics-core-concepts"></a>Ключевые концепции Apache Spark в Azure Synapse Analytics

Apache Spark — это платформа параллельной обработки, которая поддерживает обработку в памяти, чтобы повысить производительность приложений для анализа больших данных. Apache Spark в Azure Synapse Analytics — это одна из реализаций Apache Spark в облаке, предоставляемая корпорацией Майкрософт. 

Azure Synapse упрощает создание и настройку компонентов Spark в Azure. Azure Synapse предоставляет различные реализации компонентов Spark, которые здесь описаны.

## <a name="spark-pools-preview"></a>Пулы Spark (предварительная версия)

Пул Spark (предварительной версии) создается на портале Azure. Это определение пула Spark, при реализации которого создается экземпляр Spark, обрабатывающий данные. Созданный пул Spark существует только в виде метаданных. Соответственно, ресурсы не потребляются и не выполняются, а значит за них не взимается плата. В пуле Spark есть ряд свойств, управляющих характеристиками экземпляра Spark. Эти характеристики среди прочего включают имя, размер, поведение масштабирования и время жизни.

Для создания пулов Spark не нужны средства или ресурсы, поэтому их можно создавать в любом количестве с любыми конфигурациями. К пулам Spark также можно применять разрешения, чтобы предоставлять пользователям доступ только к определенным из них.

Рекомендуется создавать пулы Spark малого размера для разработки и отладки, а более крупные пулы — для производственных рабочих нагрузок.

Вы можете узнать, как создать пул Spark в Synapse Analytics и просмотреть все его свойства, [здесь](../quickstart-create-apache-spark-pool-portal.md).

## <a name="spark-instances"></a>Экземпляры Spark

При подключении к пулу Spark, создании сеанса и выполнении задания создаются экземпляры Spark. У нескольких пользователей может быть доступ к одному и тому же пулу Spark, поэтому для каждого пользователя, который подключается, создается экземпляр Spark. 

Если при отправке второго задания в пуле есть емкость, существующий экземпляр Spark также будет иметь емкость. Затем существующий экземпляр обработает задание. В противном случае, если емкость доступна на уровне пула, будет создан новый экземпляр Spark.

## <a name="examples"></a>Примеры

### <a name="example-1"></a>Пример 1

- Создайте пул Spark с именем SP1 и фиксированным размером кластера — 20 узлов.
- Если вы отправите задание записной книжки J1, использующее 10 узлов, будет создан экземпляр Spark с именем SI1 для обработки задания.
- Если вы отправите еще одно задание J2, использующее 10 узлов, в пуле и экземпляре все еще будет емкость и SI1 обработает J2.
- Если бы J2 были необходимы 11 узлов, в SP1 и SI1 не хватило бы емкости. В таком случае если J2 поступит из записной книжки, задание будет отклонено. Если J2 поступит из пакетного задания, то будет поставлено в очередь.

### <a name="example-2"></a>Пример 2

- Создайте пул Spark с именем SP2 и со включенным автомасштабированием 10–20 узлов.
- Если вы отправите задание записной книжки с именем J1, в котором используются 10 узлов, будет создан экземпляр Spark с именем SI1 для обработки задания.
- Если вы отправите еще одно задание с именем J2, в котором используются 10 узлов, в пуле все еще будет емкость, поэтому экземпляр автоматически увеличится до 20 узлов и обработает J2.

### <a name="example-3"></a>Пример 3

- Создайте пул Spark с именем SP1 и фиксированным размером кластера — 20 узлов.
- Если вы отправите задание записной книжки J1, использующее 10 узлов, будет создан экземпляр Spark с именем SI1 для обработки задания.
- Если другой пользователь с именем U2 отправит задание J3, в котором используются 10 узлов, будет создан экземпляр Spark с именем SI2 для обработки задания.
- Если сейчас вы отправите еще одно задание, J2, использующее 10 узлов, в пуле все еще будет емкость и экземпляр, поэтому J2 будет обработано SI1.

## <a name="next-steps"></a>Дальнейшие действия

- [Azure Synapse Analytics](https://docs.microsoft.com/azure/synapse-analytics)
- [Документация по Apache Spark](https://spark.apache.org/docs/2.4.4/)
