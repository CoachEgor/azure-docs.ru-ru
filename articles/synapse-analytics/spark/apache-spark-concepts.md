---
title: 'Apache Spark в Azure Synapse Analytics: ключевые концепции'
description: В этой статье содержатся общие сведения об Apache Spark в Azure Synapse Analytics и различные концепции.
services: synapse-analytics
author: euangMS
ms.service: synapse-analytics
ms.topic: overview
ms.subservice: ''
ms.date: 04/15/2020
ms.author: euang
ms.reviewer: euang
ms.openlocfilehash: 6276d198e547efec3d2e3cb88816da5e2b593aae
ms.sourcegitcommit: fdec8e8bdbddcce5b7a0c4ffc6842154220c8b90
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/19/2020
ms.locfileid: "83644684"
---
# <a name="apache-spark-in-azure-synapse-analytics-core-concepts"></a>Ключевые концепции Apache Spark в Azure Synapse Analytics

Apache Spark — это платформа параллельной обработки, которая поддерживает обработку в памяти, чтобы повысить производительность приложений для анализа больших данных. Apache Spark в Azure Synapse Analytics — это одна из реализаций Apache Spark в облаке, предоставляемая корпорацией Майкрософт. 

Azure Synapse упрощает создание и настройку компонентов Spark в Azure. Azure Synapse предоставляет различные реализации компонентов Spark, которые здесь описаны.

## <a name="spark-pools-preview"></a>Пулы Spark (предварительная версия)

Пул Spark (предварительной версии) создается на портале Azure. Это определение пула Spark, при реализации которого создается экземпляр Spark, обрабатывающий данные. Созданный пул Spark существует только в виде метаданных. Соответственно ресурсы не потребляются, не запускаются, а значит за них не взимается плата. В пуле Spark предусмотрен ряд свойств для управления характеристиками экземпляра Spark. Эти характеристики включают в себя имя, размер, поведение масштабирования и время жизни, но не ограничиваются ими.

Создание пулов Spark не связано с денежными или ресурсными затратами, поэтому их можно создавать сколько угодно с различными конфигурациями. К пулам Spark также можно применять разрешения, чтобы предоставлять пользователям доступ только к определенным из них.

Рекомендуется создавать пулы Spark малого размера для разработки и отладки, а более крупные пулы — для производственных рабочих нагрузок.

Вы можете узнать, как создать пул Spark в Synapse Analytics и просмотреть все его свойства, [здесь](../quickstart-create-apache-spark-pool-portal.md).

## <a name="spark-instances"></a>Экземпляры Spark

При подключении к пулу Spark, создании сеанса и выполнении задания создаются экземпляры Spark. У нескольких пользователей может быть доступ к одному и тому же пулу Spark, поэтому для каждого пользователя, который подключается, создается экземпляр Spark. 

При отправке второго задания и при наличии емкости в пуле и в существующем экземпляре Spark, существующий экземпляр обработает задание. В противном случае и при наличии емкости на уровне пула будет создан еще один экземпляр Spark.

## <a name="examples"></a>Примеры

### <a name="example-1"></a>Пример 1

- Создайте пул Spark с именем SP1 и фиксированным размером кластера — 20 узлов.
- Если вы отправите задание записной книжки J1, использующее 10 узлов, будет создан экземпляр Spark с именем SI1 для обработки задания.
- Если вы отправите еще одно задание J2, использующее 10 узлов, в пуле и экземпляре все еще будет емкость и SI1 обработает J2.
- Если бы J2 были необходимы 11 узлов, в SP1 и SI1 не хватило бы емкости. В таком случае если J2 поступит из записной книжки, задание будет отклонено. Если J2 поступит из пакетного задания, то будет поставлено в очередь.

### <a name="example-2"></a>Пример 2

- Создайте пул Spark с именем SP2 и со включенным автомасштабированием 10–20 узлов.
- Если вы отправите задание записной книжки с именем J1, в котором используются 10 узлов, будет создан экземпляр Spark с именем SI1 для обработки задания.
- Если вы отправите еще одно задание с именем J2, в котором используются 10 узлов, в пуле все еще будет емкость, поэтому экземпляр автоматически увеличится до 20 узлов и обработает J2.

### <a name="example-3"></a>Пример 3

- Создайте пул Spark с именем SP1 и фиксированным размером кластера — 20 узлов.
- Если вы отправите задание записной книжки J1, использующее 10 узлов, будет создан экземпляр Spark с именем SI1 для обработки задания.
- Если другой пользователь с именем U2 отправит задание J3, в котором используются 10 узлов, будет создан экземпляр Spark с именем SI2 для обработки задания.
- Теперь если отправить еще одно задание, J2, использующее 10 узлов, в пуле все еще будет емкость и экземпляр, поэтому J2 будет обработано SI1.

## <a name="next-steps"></a>Дальнейшие действия

- [Azure Synapse Analytics](https://docs.microsoft.com/azure/synapse-analytics)
- [Документация по Apache Spark](https://spark.apache.org/docs/2.4.4/)
