---
title: 'Руководство. Apache Spark для Azure Synapse Analytics: определение задания Apache Spark для Synapse'
description: Руководство. Использование Azure Synapse Analytics для создания определений заданий Spark и их отправки в пул Apache Spark для Azure Synapse Analytics.
author: hrasheed-msft
ms.author: jejiang
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive
ms.topic: tutorial
ms.date: 04/15/2020
ms.openlocfilehash: 5fc9dffaa73d195c842381b6682a00e9834c0fe7
ms.sourcegitcommit: bb0afd0df5563cc53f76a642fd8fc709e366568b
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/19/2020
ms.locfileid: "83587941"
---
# <a name="tutorial-use-azure-synapse-analytics-to-create-apache-spark-job-definitions-for-synapse-spark-pools"></a>Руководство по Использование Azure Synapse Analytics для создания определений заданий Apache Spark для пулов Synapse Spark

В этом руководстве показано, как использовать Azure Synapse Analytics для создания определений заданий Spark и их отправки в пул Synapse Spark. С помощью подключаемого модуля можно выполнять следующие действия:

* Разработка определения задания Spark и его отправка в пул Synapse Spark.
* Просмотр сведений о задании после отправки.

В этом руководстве описано следующее:

> [!div class="checklist"]
>
> * Разработка определения задания Spark и его отправка в пул Synapse Spark.
> * Просмотр сведений о задании после отправки.

## <a name="prerequisites"></a>Предварительные требования

* Рабочая область Azure Synapse Analytics. См. руководство по [созданию рабочей области Azure Synapse Analytics](../../machine-learning/how-to-manage-workspace.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json#create-a-workspace).

## <a name="get-started"></a>Начало работы

Для отправки определения задания Spark требуется роль владельца данных BLOB-объектов службы хранилища для файловой системы ADLS 2-го поколения, с которой вы хотите работать. Если у вас нет этой роли, добавьте разрешение вручную.

### <a name="scenario-1-add-permission"></a>Сценарий 1. Добавление разрешения

1. Откройте [Microsoft Azure](https://ms.portal.azure.com), а затем откройте учетную запись службы хранилища.

2. Щелкните элемент **Контейнеры** и создайте **файловую систему**. В этом учебнике используется `sparkjob`.

    ![Нажмите кнопку "Отправить", чтобы отправить определение задания Spark.](./media/apache-spark-job-definitions/open-azure-container.png)

    ![Диалоговое окно Spark Submission (Отправка в Spark)](./media/apache-spark-job-definitions/create-new-filesystem.png)

3. Откройте `sparkjob`, щелкните **Управление доступом (IAM)** , нажмите кнопку **Добавить** и выберите **Добавить назначение ролей**.

    ![Нажмите кнопку "Отправить", чтобы отправить определение задания Spark.](./media/apache-spark-job-definitions/add-role-assignment-01.png)

    ![Нажмите кнопку "Отправить", чтобы отправить определение задания Spark.](./media/apache-spark-job-definitions/add-role-assignment-02.png)

4. Щелкните **Назначения ролей**, введите имя пользователя, а затем проверьте роль пользователя.

    ![Нажмите кнопку "Отправить", чтобы отправить определение задания Spark.](./media/apache-spark-job-definitions/verify-user-role.png)

### <a name="scenario-2-prepare-folder-structure"></a>Сценарий 2. Подготовка структуры папок

Перед отправкой определения задания Spark необходимо передать файлы в ADLS 2-го поколения и подготовить структуру папок. Для хранения файлов в Synapse Studio используется узел службы хранилища.

1. Откройте [Azure Synapse Analytics](https://web.azuresynapse.net/).

2. Щелкните вкладку **Данные**, выберите **Учетные записи службы хранилища** и отправьте соответствующие файлы в файловую систему ADLS 2-го поколения. Мы поддерживаем Scala, Java, .NET и Python. В этом руководстве используется пример, показанный на рисунке. Вы можете изменить структуру проекта по своему усмотрению.

    ![Выбор значения для определения задания Spark](./media/apache-spark-job-definitions/prepare-project-structure.png)

## <a name="create-a-spark-job-definition"></a>Создание определения задания Spark

1. Откройте [Azure Synapse Analytics](https://web.azuresynapse.net/) и выберите **Разработка**.

2. В области слева выберите **Определения заданий Spark**.

3. Щелкните узел **Действия** справа от элемента "Определения заданий Spark".

     ![Создание определения задания Spark](./media/apache-spark-job-definitions/create-new-definition-01.png)

4. В раскрывающемся списке **Действия** выберите **Создать определение задания Spark**.

     ![Создание определения задания Spark](./media/apache-spark-job-definitions/create-new-definition-02.png)

5. В окне создания определения задания Spark выберите язык, а затем укажите следующие сведения.  

   * Выберите **Spark(Scala)** в раскрывающемся списке **Язык**.

    |  Свойство   | Описание   |  
    | ----- | ----- |  
    |Имя определения задания| Введите имя определения задания Spark.  В этом учебнике используется `job definition sample`. Это имя можно изменить в любое время до публикации.|  
    |Основной файл определения| Основной файл, используемый для задания. Выберите JAR-файл в хранилище. Для отправки файла в учетную запись хранения можно выбрать **Отправить файл**. |
    |Имя главного класса| Полный идентификатор или основной класс, который находится в основном файле определения.|
    |Аргументы командной строки| Дополнительные аргументы для задания.|
    |Файлы ссылок| Дополнительные файлы, используемые для ссылки в основном файле определения. Для отправки файла в учетную запись хранения можно выбрать **Отправить файл**.|
    |Пул Spark| Задание будет отправлено в выбранный пул Spark.|
    |Версия Spark| Версия Spark, которая используется в пуле Spark.|
    |Исполнители| Количество исполнителей, которые будут предоставлены для задания в определенном пуле Spark.|
    |Размер исполнителя| Количество ядер и памяти, которые будут использоваться для исполнителей, предоставленных для задания в определенном пуле Spark.|  
    |Размер драйвера| Количество ядер и объем памяти, которые будут использоваться для драйвера, предоставленного для задания в указанном пуле Spark.|

    ![Выбор значения для определения задания Spark](./media/apache-spark-job-definitions/create-scala-definition.png)

   * Выберите **PySpark(Python)** в раскрывающемся списке **Язык**.

    |  Свойство   | Описание   |  
    | ----- | ----- |  
    |Имя определения задания| Введите имя определения задания Spark.  В этом учебнике используется `job definition sample`. Это имя можно изменить в любое время до публикации.|  
    |Основной файл определения| Основной файл, используемый для задания. Выберите PY-файл в хранилище. Для отправки файла в учетную запись хранения можно выбрать **Отправить файл**.|
    |Аргументы командной строки| Дополнительные аргументы для задания.|
    |Файлы ссылок| Дополнительные файлы, используемые для ссылки в основном файле определения. Для отправки файла в учетную запись хранения можно выбрать **Отправить файл**.|
    |Пул Spark| Задание будет отправлено в выбранный пул Spark.|
    |Версия Spark| Версия Spark, которая используется в пуле Spark.|
    |Исполнители| Количество исполнителей, которые будут предоставлены для задания в определенном пуле Spark.|
    |Размер исполнителя| Количество ядер и памяти, которые будут использоваться для исполнителей, предоставленных для задания в определенном пуле Spark.|  
    |Размер драйвера| Количество ядер и объем памяти, которые будут использоваться для драйвера, предоставленного для задания в указанном пуле Spark.|

    ![Выбор значения для определения задания Spark](./media/apache-spark-job-definitions/create-py-definition.png)

   * Выберите **.NET Spark(C#/F#)** в раскрывающемся списке **Язык**.

    |  Свойство   | Описание   |  
    | ----- | ----- |  
    |Имя определения задания| Введите имя определения задания Spark.  В этом учебнике используется `job definition sample`. Это имя можно изменить в любое время до публикации.|  
    |Основной файл определения| Основной файл, используемый для задания. Выберите ZIP-файл, содержащий приложение .NET для Spark (то есть основной исполняемый файл, библиотеки DLL с пользовательскими функциями и другие необходимые файлы) из хранилища. Для отправки файла в учетную запись хранения можно выбрать **Отправить файл**.|
    |Основной исполняемый файл| Основной исполняемый файл — это основной ZIP-файл определения.|
    |Аргументы командной строки| Дополнительные аргументы для задания.|
    |Файлы ссылок| Дополнительные файлы, необходимые рабочим узлам для запуска приложения .NET для Spark, не включенного в основной ZIP-файл определения (то есть зависимые JAR-файлы, дополнительные библиотеки DLL с пользовательскими функциями и другие файлы конфигурации). Для отправки файла в учетную запись хранения можно выбрать **Отправить файл**.|
    |Пул Spark| Задание будет отправлено в выбранный пул Spark.|
    |Версия Spark| Версия Spark, которая используется в пуле Spark.|
    |Исполнители| Количество исполнителей, которые будут предоставлены для задания в определенном пуле Spark.|
    |Размер исполнителя| Количество ядер и памяти, которые будут использоваться для исполнителей, предоставленных для задания в определенном пуле Spark.|  
    |Размер драйвера| Количество ядер и объем памяти, которые будут использоваться для драйвера, предоставленного для задания в указанном пуле Spark.|

    ![Выбор значения для определения задания Spark](./media/apache-spark-job-definitions/create-net-definition.png)

6. Выберите **Опубликовать**, чтобы сохранить определение задания Spark.

    ![Публикация определения задания Spark](./media/apache-spark-job-definitions/publish-net-definition.png)

## <a name="submit-a-spark-job-definition"></a>Отправка определения задания Spark

Созданное определение задания Spark можно отправить в пул Synapse Spark. Прежде чем пытаться выполнить примеры в этой части, обязательно выполните действия, описанные в разделе **Начало работы**.

### <a name="scenario-1-submit-spark-job-definition"></a>Сценарий 1. Отправка определения задания Spark.

1. Откройте окно определения задания Spark, щелкнув его.

      ![Открытие определения задания Spark для отправки ](./media/apache-spark-job-definitions/open-spark-definition.png)

2. Щелкните значок **отправить**, чтобы отправить проект в выбранный пул Spark. Можете щелкнуть вкладку **URL-адрес мониторинга Spark**, чтобы просмотреть LogQuery приложения Spark.

    ![Нажмите кнопку "Отправить", чтобы отправить определение задания Spark.](./media/apache-spark-job-definitions/submit-spark-definition.png)

    ![Диалоговое окно Spark Submission (Отправка в Spark)](./media/apache-spark-job-definitions/submit-definition-result.png)

### <a name="scenario-2-view-spark-job-running-progress"></a>Сценарий 2. Просмотр хода выполнения задания Spark

1. Щелкните вкладку **Монитор** и выберите **Приложения Spark**. В списке будет отправленное приложение Spark.

    ![Просмотр приложения Spark](./media/apache-spark-job-definitions/view-spark-application.png)

2. Щелкните приложение Spark. Отобразится окно **LogQuery**. Ход выполнения задания можно просмотреть в окне **LogQuery**.

    ![Просмотр LogQuery приложения Spark](./media/apache-spark-job-definitions/view-job-log-query.png)

### <a name="scenario-3-check-output-file"></a>Сценарий 3. Проверка выходного файла

 1. Щелкните вкладку **Данные** и выберите **Учетные записи службы хранилища**. После успешного выполнения можно переходить к хранилищу ADLS 2-го поколения, чтобы проверить выходные файлы.

    ![Просмотр выходного файла](./media/apache-spark-job-definitions/view-output-file.png)

## <a name="next-steps"></a>Дальнейшие действия

В этом руководстве показано, как использовать Azure Synapse Analytics для создания определений заданий Spark и их отправки в пул Synapse Spark. Далее вы можете использовать Azure Synapse Analytics для создания наборов данных Power BI и управления данными Power BI. 

- [Подключение к данным в Power BI Desktop](https://docs.microsoft.com/power-bi/desktop-quickstart-connect-to-data)
- [Визуализация с помощью Power BI](../sql-data-warehouse/sql-data-warehouse-get-started-visualize-with-power-bi.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json)
