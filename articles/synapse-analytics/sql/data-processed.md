---
title: Данные, обработанные с помощью бессерверного пула SQL
description: В этом документе описывается, как вычисляется объем обработанных данных при запросе данных в Data Lake.
services: synapse analytics
author: filippopovic
ms.service: synapse-analytics
ms.topic: conceptual
ms.subservice: sql
ms.date: 11/05/2020
ms.author: fipopovi
ms.reviewer: jrasnick
ms.openlocfilehash: a108e5fdd30c21cdb7771e3f683dad22773653a4
ms.sourcegitcommit: 8a1ba1ebc76635b643b6634cc64e137f74a1e4da
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/09/2020
ms.locfileid: "94381207"
---
# <a name="data-processed-by-using-serverless-sql-pool-in-azure-synapse-analytics"></a>Данные, обработанные с помощью бессерверного пула SQL в Azure синапсе Analytics

*Обработанные данные* — это объем данных, которые система временно сохраняет во время выполнения запроса. Обработанные данные состоят из следующих количеств:

- Объем данных, считанных из хранилища. Эта сумма включает:
  - Чтение данных при чтении данных.
  - Чтение данных при чтении метаданных (для форматов файлов, содержащих метаданные, например Parquet).
- Объем данных в промежуточных результатах. Эти данные передаются между узлами во время выполнения запроса. Она включает в себя перенос данных в конечную точку в несжатом формате. 
- Объем данных, записываемых в хранилище. Если для экспорта результирующего набора в хранилище используется CETAS, то объем записанных данных добавляется к объему данных, обрабатываемых для части SELECT CETAS.

Чтение файлов из хранилища обеспечивает высокую степень оптимизации. Процесс использует:

- Упреждающая выборка, которая может добавить некоторую нагрузку на объем считанных данных. Если запрос считывает файл целиком, дополнительная нагрузка не взимается. Если файл читается частично, как в первых N запросах, то с помощью функции предварительного получения считывается еще несколько данных.
- Оптимизированное средство синтаксического анализа значений с разделителями-запятыми (CSV). Если для чтения CSV-файлов используется PARSER_VERSION = "2.0", то объем данных, считанных из хранилища, немного увеличивается. Оптимизированное средство синтаксического анализа CSV считывает файлы параллельно, в виде фрагментов одинакового размера. Фрагменты не обязательно содержат целые строки. Чтобы обеспечить синтаксический анализ всех строк, оптимизированный синтаксический анализатор CSV также считывает небольшие фрагменты смежных фрагментов. В этом процессе добавляется небольшой объем издержек.

## <a name="statistics"></a>Статистика

Оптимизатор запросов пула SQL, не являющийся сервером, использует статистику для создания оптимальных планов выполнения запросов. Статистику можно создать вручную. В противном случае серверный пул SQL автоматически создает их. В любом случае статистика создается путем выполнения отдельного запроса, возвращающего конкретный столбец с указанной частотой выборки. Этот запрос имеет связанный объем обработанных данных.

Если вы запускаете тот же или любой другой запрос, который может выиграть от создания статистики, то по возможности повторно использует статистику. Дополнительные данные для создания статистики не обработаны.

При создании статистики для столбца Parquet из файлов считывается только соответствующий столбец. При создании статистики для столбца CSV все файлы считываются и анализируются.

## <a name="rounding"></a>Округление

Объем обработанных данных округляется до ближайших МЕГАБАЙТов на запрос. Каждый запрос содержит не менее 10 МБ обрабатываемых данных.

## <a name="what-data-processed-doesnt-include"></a>Какие данные не включаются в обработку

- Метаданные уровня сервера (например, имена для входа, роли и учетные данные уровня сервера).
- Базы данных, создаваемые в конечной точке. Эти базы данных содержат только метаданные (такие как пользователи, роли, схемы, представления, встроенные функции с табличным значением [возвращающие табличное], хранимые процедуры, учетные данные уровня базы данных, внешние источники данных, форматы внешних файлов и внешние таблицы).
  - Если вы используете вывод схемы, то фрагменты файлов считываются для определения имен столбцов и типов данных, а объем считанных данных добавляется к объему обработанных данных.
- Инструкции языка описания данных DDL, за исключением инструкции CREATE STATISTICS, поскольку она обрабатывает данные из хранилища на основе указанного процента выборки.
- Запросы только с метаданными.

## <a name="reducing-the-amount-of-data-processed"></a>Уменьшение объема обработанных данных

Можно оптимизировать объем обрабатываемых данных по запросу и повысить производительность путем секционирования и преобразования данных в сжатый формат на основе столбцов, например Parquet.

## <a name="examples"></a>Примеры

Представьте себе три таблицы.

- Таблица population_csv создана на 5 ТБ CSV-файлов. Файлы упорядочены по пяти столбцам одинакового размера.
- Таблица population_parquet содержит те же данные, что и таблица population_csv. Он имеет 1 ТБ файлов Parquet. Эта таблица меньше предыдущей, так как данные сжимаются в формате Parquet.
- Таблица very_small_csv создана на базе файлов CSV размером 100 КБ.

**Запрос 1**. Выбор суммы (совокупности) из population_csv

Этот запрос считывает и анализирует файлы целиком, чтобы получить значения для столбца Population. Узлы обрабатывают фрагменты этой таблицы, а сумма совокупности для каждого фрагмента передается между узлами. Окончательная сумма передается в конечную точку. 

Этот запрос обрабатывает 5 ТБ данных и небольшой объем ресурсов для передачи сумм фрагментов.

**Запрос 2**. Выбор суммы (совокупности) из population_parquet

При запросе сжатых и основанных на столбцах форматов, таких как Parquet, данные считываются меньше, чем в запросе 1. Вы видите этот результат, так как несерверный пул SQL считывает один сжатый столбец, а не весь файл. В этом случае считывается 0,2 ТБ. (Пять столбцов одинакового размера — 0,2 ТБ.) Узлы обрабатывают фрагменты этой таблицы, а сумма совокупности для каждого фрагмента передается между узлами. Окончательная сумма передается в конечную точку. 

Этот запрос обрабатывает 0,2 ТБ плюс небольшой объем издержек для передачи сумм фрагментов.

**Запрос 3**. Выберите * из population_parquet

Этот запрос считывает все столбцы и передает все данные в несжатом формате. Если формат сжатия 5:1, то запрос обрабатывает 6 ТБ, так как считывает 1 ТБ и передает 5 ТБ несжатых данных.

**Запрос 4**. Выбор счетчика (*) из very_small_csv

Этот запрос считывает файлы целиком. Общий размер файлов в хранилище для этой таблицы — 100 КБ. Узлы обрабатывают фрагменты этой таблицы, а сумма для каждого фрагмента передается между узлами. Окончательная сумма передается в конечную точку. 

Этот запрос обрабатывает немного более 100 КБ данных. Объем данных, обработанных для этого запроса, округляется до 10 МБ, как указано в разделе [округления](#rounding) этой статьи.

## <a name="next-steps"></a>Дальнейшие действия

Сведения о том, как оптимизировать запросы на производительность, см. в статье рекомендации [по использованию бессерверного пула SQL](best-practices-sql-on-demand.md).
