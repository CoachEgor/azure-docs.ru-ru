---
title: Данные, обработанные с помощью несерверного пула SQL
description: В этом документе описывается вычисление обработанных данных при запросе данных в службе хранилища Azure с помощью бессерверного пула SQL.
services: synapse analytics
author: filippopovic
ms.service: synapse-analytics
ms.topic: conceptual
ms.subservice: sql
ms.date: 11/05/2020
ms.author: fipopovi
ms.reviewer: jrasnick
ms.openlocfilehash: 06eb02aa3dd4d5fc8bd3605dac480d5afa52d5fa
ms.sourcegitcommit: 7cc10b9c3c12c97a2903d01293e42e442f8ac751
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/06/2020
ms.locfileid: "93424433"
---
# <a name="data-processed-with-serverless-sql-pool-in-azure-synapse-analytics"></a>Данные, обработанные с помощью несерверного пула SQL в Azure синапсе Analytics

Обработанные данные — это объем данных, временно сохраняемых в системе при выполнении запроса и состоящий из следующих компонентов:

- Объем данных, считанных из хранилища, включая:
  - Объем данных, считанных при чтении данных
  - Объем данных, считанных при чтении метаданных (для форматов файлов, содержащих метаданные, например Parquet)
- Объем данных в промежуточных результатах — данные, передаваемые между узлами во время выполнения запроса, включая передачу данных в конечную точку, в несжатом формате. 
- Объем данных, записываемых в хранилище. Если вы используете CETAS для экспорта результирующего набора в хранилище, вы получаете оплату за записанные байты и объем данных, обработанных для выбранного фрагмента CETAS.

Чтение файлов из хранилища обеспечивает высокую степень оптимизации и использует:

- Предварительная выборка, которая может добавить небольшие издержки на объем считанных данных. Если запрос считывает файл целиком, издержки не изменяются. Если файл читается частично, как в первых N запросах, то при выборке будет прочитано немного больше данных.
- Оптимизированное средство синтаксического анализа CSV. Если для чтения CSV-файлов используется PARSER_VERSION = "2.0", это приведет к значительному увеличению объема данных, считываемых из хранилища.  Оптимизированное средство синтаксического анализа CSV считывает файлы параллельно с блоками одинакового размера. Нет никакой гарантии, что фрагменты будут содержать целые строки. Чтобы убедиться, что все строки анализируются, будут также считаны небольшие фрагменты смежных фрагментов, что приведет к небольшому объему издержек.

## <a name="statistics"></a>Статистика

Оптимизатор запросов пула SQL, не являющийся сервером, использует статистику для создания оптимальных планов выполнения запросов. Можно создать статистику вручную или автоматически создать с помощью бессерверного пула SQL. В любом случае статистика создается путем выполнения отдельного запроса, возвращающего конкретный столбец в указанной частоте выборки. Этот запрос имеет связанный объем обработанных данных.

Если выполняется тот же или любой другой запрос, который может выиграть от создания статистики, статистика будет использована повторно, если это возможно, а дополнительные данные для создания статистики не будут обработаны.

Создание статистики для столбца Parquet приведет к считыванию только соответствующего столбца из файлов. Создание статистики для столбца CSV приведет к считыванию и анализу целых файлов.

## <a name="rounding"></a>Округление

Объем обработанных данных округляется до ближайших МЕГАБАЙТов на запрос, при этом для каждого запроса обрабатывается не менее 10 МБ данных.

## <a name="what-is-not-included-in-data-processed"></a>Что не включено в обработанные данные

- Метаданные уровня сервера (например, имена для входа, роли, учетные данные уровня сервера)
- Базы данных, создаваемые в конечной точке, так как эти базы данных содержат только метаданные (например, пользователи, роли, схемы, представления, встроенные возвращающие табличное, хранимые процедуры, учетные данные для базы данных, внешние источники данных, внешние форматы файлов, внешние таблицы).
  - Если вы используете вывод схемы, фрагменты файлов будут считываться, чтобы вывести имена столбцов и типы данных.
- Инструкции DDL, за исключением создания статистики, так как она будет обрабатывать данные из хранилища на основе указанного образца в процентах
- Запросы только с метаданными

## <a name="reduce-amount-of-data-processed"></a>Уменьшение объема обработанных данных

Вы можете оптимизировать количество обработанных данных для каждого запроса и повысить производительность, выполнив секционирование и преобразование данных в формат со сжатыми столбцами, например Parquet.

## <a name="examples"></a>Примеры

Предположим, что существует две таблицы, каждая из которых имеет одни и те же данные в пяти столбцах одинакового размера:

- population_csv таблица, созданная на 5 ТБ CSV-файлов
- population_parquet таблица с Parquet файлов размером 1 ТБ — эта таблица меньше, чем предыдущая, Parquet содержит сжатые данные
- very_small_csv таблица, поддерживаемая 100 КБ из CSV-файлов

**#1 запроса** : выберите сумма (заполнение) из population_csv

Этот запрос будет считывать и анализировать все файлы, чтобы получить значения для столбца заполнения. Узлы будут обрабатывать фрагменты этой таблицы, сумма совокупности для каждого фрагмента будет передаваться между узлами, а окончательная сумма будет передана в конечную точку. Этот запрос будет обрабатывать 5 ТБ данных, а также небольшие издержки для передачи сумм фрагментов.

**#2 запроса** : выберите сумма (заполнение) из population_parquet

При запросе сжатых и столбцовых форматов, таких как Parquet, считывается меньше данных, чем в предыдущем запросе, так как серверный пул SQL будет считывать один сжатый столбец, а не весь файл. В данном случае 0,2 ТБ будет считаться (пять столбцов одинакового размера, 0,2 ТБ). Узлы будут обрабатывать фрагменты этой таблицы, сумма совокупности для каждого фрагмента будет передаваться между узлами, а окончательная сумма будет передана в конечную точку. Этот запрос будет обрабатывать 0,2 ТБ плюс небольшие издержки на передачу сумм фрагментов.

**#3 запроса** : выберите * из population_parquet

Этот запрос будет считывать все столбцы и передавать все данные в несжатом формате. Если используется формат сжатия 5:1, то он будет обрабатывать 6 ТБ, так как он считывает 1 ТБ + передает 5 ТБ несжатых данных.

**#4 запроса** : выберите Count (*) из very_small_csv

Этот запрос будет считывать файлы целиком. Общий размер файлов в хранилище для этой таблицы — 100 КБ. Узлы будут обрабатывать фрагменты этой таблицы, сумма для каждого фрагмента будет передаваться между узлами, а окончательная сумма будет передана в конечную точку. Этот запрос будет обрабатывать немного более 100 КБ данных. Объем данных, обработанных для этого запроса, будет округлен до 10 МБ, как указано в [округлении](#rounding).

## <a name="next-steps"></a>Дальнейшие шаги

Чтобы узнать, как оптимизировать запросы на производительность, ознакомьтесь с рекомендациями [по использованию бессерверного пула SQL](best-practices-sql-on-demand.md).
