---
title: Руководство по Начало работы с Azure Synapse Analytics
description: Пошаговые инструкции для быстрого ознакомления с основными понятиями в Azure Synapse
services: synapse-analytics
author: saveenr
ms.author: saveenr
manager: julieMSFT
ms.reviewer: jrasnick
ms.service: synapse-analytics
ms.topic: quickstart
ms.date: 05/19/2020
ms.openlocfilehash: dcad90713227e55437523c91997175242078e9e4
ms.sourcegitcommit: 0b80a5802343ea769a91f91a8cdbdf1b67a932d3
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/25/2020
ms.locfileid: "83836487"
---
# <a name="getting-started-with-azure-synapse-analytics"></a>Начало работы с Azure Synapse Analytics

В этом руководстве описываются все основные шаги, необходимые для настройки и использования Azure Synapse Analytics.

## <a name="prepare-a-storage-account-for-use-with-a-synapse-workspace"></a>Подготовка учетной записи хранения для использования с рабочей областью Synapse

1. Откройте [портал Azure](https://portal.azure.com).
1. Создайте учетную запись хранения, выполнив следующие действия:
    * На вкладке **Основные сведения**

    |Параметр | Рекомендуемое значение | Описание |
    |---|---|---|
    |**Имя учетной записи хранения**| Можно выбрать любое имя.|В этом документе он будет называться `contosolake`.
    |**Account kind** (Тип учетной записи)|Нужно задать значение `StorageV2`||
    |**Расположение**|Вы можете выбрать для него любое имя.| Рекомендуем располагать рабочую область Synapse и учетную запись Azure Data Lake Storage (ADLS) Gen2 в одном регионе.|
    ||||
    
    * На вкладке **Дополнительно**
    
    |Параметр | Рекомендуемое значение | Описание |
    |---|---|---|
    |**Data Lake Storage 2-го поколения**|`Enabled`| Azure Synapse работает только с учетными записями хранения, в которых этот параметр включен.|
    ||||

1. После создания учетной записи хранения выберите **Управление доступом (IAM)** в области навигации слева. Затем назначьте следующие роли или убедитесь, что они уже назначены. 
    а. * Назначьте себе роль **Владелец** для учетной записи хранения b. * В учетной записи хранения назначьте себе роль **Владелец данных BLOB-объектов хранилища**.
1. В области навигации слева щелкните **Контейнеры** и создайте контейнер. Можно выбрать любое имя. Примите значение по умолчанию **Уровень общего доступа**. В этом документе будет вызываться контейнер `users`. Нажмите кнопку **создания**. 

## <a name="create-a-synapse-workspace"></a>Создание рабочей области Synapse

1. Откройте [портал Azure](https://portal.azure.com) и в поле поиска наверху выполните поиск `Synapse`.
1. В результатах поиска в разделе **Службы** выберите **Azure Synapse Analytics (workspaces preview)** (Azure Synapse Analytics (предварительная версия рабочих областей))
1. Выберите **+ Добавить**.
1. Вкладка **Основные сведения**:

    |Параметр | Рекомендуемое значение | Описание |
    |---|---|---|
    |**Имя рабочей области**|Можно присвоить любое имя.| В этом документе используется `myworkspace`
    |**Регион**|Совпадает с регионом учетной записи хранения||
    |||

1. В разделе **Select Data Lake Storage Gen 2** (Выбрать Data Lake Storage Gen 2) выберите учетную запись и контейнер, созданные ранее.
    > [!NOTE]
    > Мы будем называть учетную запись хранения, выбранную здесь, "основной" учетной записью хранения рабочей области Synapse. Эта учетная запись используется для хранения данных в таблицах Apache Spark и для журналов, созданных при создании пулов Spark или при запуске приложений Spark.

1. Выберите **Review + create** (Просмотреть и создать). Нажмите кнопку **создания**. Рабочая область должна быть готова через несколько минут.

## <a name="verify-the-synapse-workspace-msi-has-access-to-the-storage-account"></a>Убедитесь, что у MSI рабочей области Synapse есть доступ к учетной записи хранения.

Это, возможно, уже было сделано. В любом случае, в этом следует убедиться.

1. Откройте [портал Azure](https://portal.azure.com) и откройте основную учетную запись хранения, выбранную для вашей рабочей области.
1. Выберите **Управление доступом (IAM)** в области навигации слева. Затем назначьте следующие роли или убедитесь, что они уже назначены. 
    а. В учетной записи хранения назначьте удостоверение рабочей области пользователю с ролью **Участник для данных BLOB-объектов хранилища**. Удостоверение рабочей области совпадает с именем рабочей области. В этом документе именем рабочей области будет `myworkspace`, поэтому удостоверение рабочей области будет иметь имя `myworkspaced`
1. Щелкните **Сохранить**.
    
## <a name="launch-synapse-studio"></a>Запуск Synapse Studio

После создания рабочей области Synapse можно открыть Synapse Studio двумя способами:
* Откройте рабочую область Synapse на [портале Azure](https://portal.azure.com) и в верхней части раздела **Обзор** выберите **Запустить Synapse Studio.**
* Перейдите непосредственно на https://web.azuresynapse.net и войдите в рабочую область.

## <a name="create-a-sql-pool"></a>Создание пула SQL

1. В Synapse Studio в области навигации слева выберите **Управление > Пулы SQL**.

    > [!NOTE] 
    > Все рабочие области Synapse поставляются с предварительно созданным пулом, который называется **SQL on-demand** (SQL по запросу).

1. Выберите **+Создать** и введите следующие параметры:

    |Параметр | Рекомендуемое значение | 
    |---|---|---|
    |**Имя пула SQL**| `SQLDB1`|
    |**Уровень производительности**|`DW100C`|
    |||

1. Выберите **Просмотр и создание**, а затем щелкните **Создать**.
1. Пул SQL будет готов через несколько минут.

    > [!NOTE]
    > Пул Synapse SQL соответствует тому, что раньше называлось "хранилище данных SQL Azure".

Пул SQL использует оплачиваемые ресурсы пока он работает. Таким образом, при необходимости пул можно приостановить, чтобы снизить затраты.

Созданный пул SQL будет связан с базой данных пула SQL, также называемой **SQLDB1**.

## <a name="create-an-apache-spark-pool"></a>Создание пула Apache Spark

1. В окне Synapse Studio с левой стороны выберите **Управление > Пулы Apache Spark**
1. Выберите **+Создать** и введите следующие параметры:

    |Параметр | Рекомендуемое значение | 
    |---|---|---|
    |**Имя пула Apache Spark**|`Spark1`
    |**Размер узла**| `Small`|
    |**Количество узлов**| Задайте для минимума и максимума значение 3.|
    |||

1. Выберите **Просмотр и создание**, а затем щелкните **Создать**.
1. Ваш пул Apache Spark будет готов через несколько секунд.

> [!NOTE]
> Несмотря на свое имя, пул Apache Spark не является пулом SQL. Это лишь базовые метаданные, которые используются для информирования рабочей области Synapse о том, как взаимодействовать со Spark. 

Так как это метаданные, пулы Spark нельзя запустить или остановить. 

При выполнении каких-либо действий со Spark в Synapse необходимо указать используемый пул Spark. Пул информирует Synapse, сколько ресурсов Spark использовать. Вы платите только за те ресурсы, которые используете. При активной отмене использования пула ресурсы автоматически истекут и будут перераспределены.

> [!NOTE]
> Базы данных Spark создаются независимо от пулов Spark. В рабочей области всегда имеется база данных Spark, которая называется **default** и вдобавок к ней вы можете создавать дополнительные базы данных Spark.

## <a name="the-sql-on-demand-pool"></a>Пул SQL по запросу

Каждая рабочая область поставляется с предварительно созданным и неудаляемым пулом, который называется **SQL on-demand**. Пул SQL по запросу позволяет работать с SQL без необходимости создавать пул Synapse SQL или заниматься его управлением. В отличие от других типов пулов, выставление счетов за использование SQL по запросу зависит от объема данных, просканированных для выполнения запроса, а не от количества ресурсов, используемых для выполнения запроса.

* SQL по запросу также имеет собственные базы данных "SQL on-demand", которые существуют независимо от любого пула SQL по запросу.
* В настоящее время Рабочая область всегда имеет ровно один пул SQL по запросу с именем **SQL on-demand**.

## <a name="load-the-nyc-taxi-sample-data-into-the-sqldb1-database"></a>Загрузка образца данных нью-йоркского такси в базу данных SQLDB1

1. В Synapse Studio в самом верхнем синем меню выберите **?** .
1. Выберите **Начало работы > Центр начала работы**
1. В карточке, помеченной как **Query sample data** (Образец данных запроса), выберите пул SQL с именем `SQLDB1`
1. Выберите **Данные запроса**. Вы увидите уведомление "Loading sample data" (Загрузка образца данных), которое затем исчезнет.
1. В верхней части Synapse Studio появится светло-синяя панель уведомлений, показывающая, что данные загружаются в SQLDB1. Дождитесь пока она не станет зеленой, а затем закройте ее.

## <a name="explore-the-nyc-taxi-data-in-the-sql-pool"></a>Изучение данных нью-йоркского такси в пуле SQL

1. В Synapse Studio перейдите в центр **Данные**
1. Перейдите в раздел **SQLDB1 > Таблицы**. Вы увидите, что были загружены несколько таблиц.
1. Щелкните правой кнопкой мыши таблицу **dbo.Trip** и выберите **Создать скрипт SQL > Выбрать первые 100 строк**
1. Будет создан и автоматически запущен новый скрипт SQL.
1. Обратите внимание, что в верхней части скрипта SQL для параметра **Connect to** (Куда подключаться) автоматически задано значение пула SQL, именуемого SQLDB1.
1. Замените текст скрипта SQL этим кодом и запустите его.

    ```sql
    SELECT PassengerCount,
          SUM(TripDistanceMiles) as SumTripDistance,
          AVG(TripDistanceMiles) as AvgTripDistance
    FROM  dbo.Trip
    WHERE TripDistanceMiles > 0 AND PassengerCount > 0
    GROUP BY PassengerCount
    ORDER BY PassengerCount
    ```

1. Этот запрос показывает, как общее время поездки и среднее расстояние поездки связаны с количеством пассажиров
1. В окне результатов скрипта SQL измените представление с **Обзор** на **График**, чтобы увидеть визуализацию результатов в виде графика

## <a name="load-the-nyc-taxi-sample-data-into-the-spark-nyctaxi-database"></a>Загрузка образца данных нью-йоркского такси в базу данных Spark "nyctaxi"

В таблице есть данные, доступные в `SQLDB1`. Теперь эти данные загружаются в базу данных Spark с именем "nyctaxi".

1. В Synapse Studio перейдите в центр **Разработка**
1. Выберите **+** и выберите **Записная книжка**
1. В верхней части записной книжки задайте для параметра **Присоединить к** значение `Spark1`
1. Выберите **Добавить код**, чтобы добавить ячейку кода записной книжки, и вставьте следующий текст:

    ```scala
    %%spark
    spark.sql("CREATE DATABASE IF NOT EXISTS nyctaxi")
    val df = spark.read.sqlanalytics("SQLDB1.dbo.Trip") 
    df.write.mode("overwrite").saveAsTable("nyctaxi.trip")
    ```

1. Перейдите в центр **Данные**, щелкните правой кнопкой мыши **Базы данных** и выберите **Обновить**.
1. Должны отображаться такие базы данных:
    - SQLDB (пул SQL)
    - nyctaxi (Spark)
      
## <a name="analyze-the-nyc-taxi-data-using-spark-and-notebooks"></a>Анализ данных нью-йоркского такси с помощью Spark и записных книжек

1. Вернитесь к своей записной книжке
1. Создайте новую ячейку кода, введите приведенный ниже текст и выполните эту ячейку, чтобы получить данные нью-йоркского такси, загруженные в базу данных Spark `nyctaxi`.

   ```py
   %%pyspark
   df = spark.sql("SELECT * FROM nyctaxi.trip") 
   display(df)
   ```

1. Выполните следующий код, чтобы провести тот же анализ, который был сделан ранее с помощью пула SQL `SQLDB1`. Этот код также сохраняет результаты анализа в таблицу с именем `nyctaxi.passengercountstats` и визуализирует результаты.

   ```py
   %%pyspark
   df = spark.sql("""
      SELECT PassengerCount,
          SUM(TripDistanceMiles) as SumTripDistance,
          AVG(TripDistanceMiles) as AvgTripDistance
      FROM nyctaxi.trip
      WHERE TripDistanceMiles > 0 AND PassengerCount > 0
      GROUP BY PassengerCount
      ORDER BY PassengerCount
    """) 
    display(df)
    df.write.saveAsTable("nyctaxi.passengercountstats")
    ```

1. В результатах ячейки выберите пункт **График**, чтобы просмотреть отображаемые данные
 
## <a name="customize-data-visualization-data-with-spark-and-notebooks"></a>Настройка данных визуализации данных с помощью Spark и записных книжек

С помощью записных книжек можно управлять отображением графика. В следующем коде показан простой пример с использованием популярных библиотек `matplotlib` и `seaborn`. Он реализует тот же вид графика, который использовался при выполнении запросов SQL ранее.

```py
%%pyspark
import matplotlib.pyplot
import seaborn

seaborn.set(style = "whitegrid")
df = spark.sql("SELECT * FROM nyctaxi.passengercountstats")
df = df.toPandas()
seaborn.lineplot(x="PassengerCount", y="SumTripDistance" , data = df)
seaborn.lineplot(x="PassengerCount", y="AvgTripDistance" , data = df)
matplotlib.pyplot.show()
```
    
## <a name="load-data-from-a-spark-table-into-a-sql-pool-table"></a>Загрузка данных из таблицы Spark в таблицу пула SQL

Ранее мы скопировали данные из таблицы пула SQL `SQLDB1.dbo.Trip` в таблицу Spark `nyctaxi.trip`. Затем, используя Spark, данные были агрегированы в таблицу Spark `nyctaxi.passengercountstats`. Теперь данные из `nyctaxi.passengercountstats` будут скопированы в таблицу пула SQL с именем `SQLDB1.dbo.PassengerCountStats`. 

Выполните следующую ячейку в записной книжке. Сводная таблица Spark будет скопирована обратно в таблицу пула SQL.

```scala
%%spark
val df = spark.sql("SELECT * FROM nyctaxi.passengercountstats")
df.write.sqlanalytics("SQLDB1.dbo.PassengerCountStats", Constants.INTERNAL )
```

## <a name="analyze-nyc-taxi-data-in-spark-databases-using-sql-on-demand"></a>Анализ данных нью-йоркского такси в базах данных Spark с помощью SQL по запросу 

Таблицы в базах данных Spark автоматически будут видимы и данные в них доступны для SQL по запросу.

1. В Synapse Studio перейдите в центр **Разработка** и создайте новый скрипт SQL.
1. Установите для параметра **Connect to** (Куда подключаться) значение **SQL on-demand** 
1. Вставьте в скрипт следующий текст и выполните скрип.

    ```sql
    SELECT *
    FROM nyctaxi.dbo.passengercountstats
    ```
    > [!NOTE]
    > При первом выполнении запроса, использующего SQL по запросу, SQL по запросу потребуется примерно 10 секунд на сбор ресурсов SQL, необходимых для выполнения запросов. Последующим запросам не понадобиться столько времени и они будут выполняться гораздо быстрее.
  
## <a name="orchestrate-activities-with-pipelines"></a>Оркестрация действий с помощью конвейеров

В Azure Synapse можно управлять множеством задач. В этом разделе вы увидите, насколько это просто.

1. В Synapse Studio перейдите в центр **Оркестрация**.
1. Выберите **+** , а затем выберите **Конвейер**. Будет создан новый конвейер.
1. Перейдите в центр разработки и найдите созданную ранее записную книжку.
1. Перетащите эту записную книжку в конвейер.
1. В конвейере выберите **Добавить триггер > Создать или изменить**.
1. В окне **Выбор триггера** выберите **Создать**, а затем для повторения задайте ежечасный запуск триггера.
1. Щелкните **ОК**.
1. Выберите команду **Опубликовать все** и далее конвейер будет выполняться каждый час.
1. Если вы хотите, чтобы конвейер выполнился сейчас, не дожидаясь следующего часа, выберите **Добавить триггер > Создать или изменить**.

## <a name="working-with-data-in-a-storage-account"></a>Работа с данными в учетной записи хранилища

До сих пор были рассмотрены ситуации, когда данные находятся в базах данных в рабочей области. Теперь мы покажем, как работать с файлами в учетных записях хранения. В этом сценарии мы будем использовать основную учетную запись хранения для рабочей области и контейнера, которые мы указали при создании рабочей области.

* Имя учетной записи хранения: `contosolake`
* Имя контейнера в учетной записи хранения: `users`

### <a name="creating-csv-and-parquet-files-in-your-storage-account"></a>Создание файлов CSV и Parquet в учетной записи хранения

Выполните приведенный ниже код в записной книжке. Он создает CSV-файл и Parquet-файл в учетной записи хранения

```py
%%pyspark
df = spark.sql("SELECT * FROM nyctaxi.passengercountstats")
df = df.repartition(1) # This ensure we'll get a single file during write()
df.write.mode("overwrite").csv("/NYCTaxi/PassengerCountStats.csv")
df.write.mode("overwrite").parquet("/NYCTaxi/PassengerCountStats.parquet")
```

### <a name="analyzing-data-in-a-storage-account"></a>Анализ данных в учетной записи хранения

1. В Synapse Studio перейдите в центр **Данные**
1. Выберите **Связано**
1. Перейдите в раздел **Учетные записи хранения > myworkspace (Основная — contosolake).**
1. Выберите **пользователи (Основной)"**
1. Вы должны увидеть папку с именем "NYCTaxi". Внутри вы увидите две папки "PassengerCountStats.csv" и "PassengerCountStats.parquet".
1. Перейдите в папку "PassengerCountStats.parquet".
1. Щелкните правой кнопкой мыши файл Parquet внутри нее и выберите **Создать записную книжку**, после чего будет создана записная книжка с ячейкой следующего вида:

    ```py
    %%pyspark
    data_path = spark.read.load('abfss://users@contosolake.dfs.core.windows.net/NYCTaxi/PassengerCountStats.parquet/part-00000-1f251a58-d8ac-4972-9215-8d528d490690-c000.snappy.parquet', format='parquet')
    data_path.show(100)
    ```

1. Запустите эту ячейку.
1. Щелкните правой кнопкой мыши файл Parquet внутри и выберите команду **Создать скрипт SQL > Выбрать первые 100 строк**, после чего будет создан такой скрипт SQL:

    ```sql
    SELECT TOP 100 *
    FROM OPENROWSET(
        BULK 'https://contosolake.dfs.core.windows.net/users/NYCTaxi/PassengerCountStats.parquet/part-00000-1f251a58-d8ac-4972-9215-8d528d490690-c000.snappy.parquet',
        FORMAT='PARQUET'
    ) AS [r];
    ```
    
1. В скрипте для поля **Присоединить к** будет установлено значение **SQL on-demand**.
1. Выполните скрипт.

## <a name="visualize-data-with-power-bi"></a>Визуализация данных с помощью Power BI

Из данных нью-йоркского такси мы создали агрегированные наборы данных в двух таблицах:
* `nyctaxi.passengercountstats`
* `SQLDB1.dbo.PassengerCountStats`

Рабочую область Power BI можно связать с рабочей областью Synapse. Это позволяет легко получать данные в рабочую область Power BI, а также изменять отчеты Power BI непосредственно в рабочей области Synapse.

### <a name="create-a-power-bi-workspace"></a>Создание рабочей области Power BI

1. Войдите на портал [powerbi.microsoft.com](https://powerbi.microsoft.com/).
1. Создайте новую рабочую область Power BI с именем `NYCTaxiWorkspace1`.

### <a name="link-your-synapse-workspace-to-your-new-power-bi-workspace"></a>Связывание рабочей области Synapse с новой рабочей областью Power BI

1. В Synapse Studio перейдите в раздел **Управление > Связанные службы**.
1. Выберите команду **+ Создать**, затем выберите **Подключиться к Power BI** и задайте следующие поля:

    |Параметр | Рекомендуемое значение | 
    |---|---|---|
    |**имя**;|`NYCTaxiWorkspace1`|
    |**Имя рабочей области**|`NYCTaxiWorkspace1`|
    |||
    
1. Нажмите кнопку **создания**.

### <a name="create-a-power-bi-dataset-that-uses-data-in-your-synapse-workspace"></a>Создание набора данных Power BI, использующего данные в рабочей области Synapse

1. В Synapse Studio перейдите в центр **Разработка > Power BI**.
1. Перейдите в раздел **NYCTaxiWorkspace1 > наборы данных Power BI** и выберите **Новый набор данных Power BI**.
1. Наведите указатель мыши на базу данных `SQLDB1` и выберите **Download .pbids file** (Загрузить PBIDS-файл).
1. Откройте загруженный файл `.pbids`. 
1. Это приведет к запуску Power BI Desktop и его автоматическому подключению к `SQLDB1` в рабочей области Synapse.
1. Если откроется диалоговое окно с именем **База данных SQL Server**: a. Выберите **Учетная запись Майкрософт**. 
    b. Нажмите кнопку **Вход** и выполните вход.
    c. Выберите **Подключиться**.
1. Откроется диалоговое окно **Навигатор**. В этом случае проверьте таблицу **PassengerCountStats** и выберите **Загрузить**.
1. Появится диалоговое окно **Параметры подключения**. Выберите **DirectQuery** и нажмите кнопку **ОК**.
1. Нажмите кнопку **Отчет** слева.
1. Добавьте в отчет **График**.
    а. Перетащите столбец **PasssengerCount** в **Визуализации > Ось** b. Перетащите столбцы **SumTripDistance** и **AvgTripDistance** в **Визуализации > Значения**.
1. На вкладке **Главная** нажмите кнопку **Опубликовать**.
1. Вы увидите запрос на сохранение изменений. Щелкните **Сохранить**.
1. Вы увидите запрос на выбор имени файла. Выберите `PassengerAnalysis.pbix` и нажмите кнопку **Сохранить**.
1. Вы увидите запрос на **Выберите конечную папку** — выберите `NYCTaxiWorkspace1`, затем нажмите кнопку **Выбрать**.
1. Дождитесь завершения публикации.

### <a name="configure-authentication-for-your-dataset"></a>Настройка проверки подлинности для набора данных

1. Откройте [powerbi.microsoft.com](https://powerbi.microsoft.com/) и выполните **Вход**
1. Слева в разделе **Рабочие области** выберите рабочую область `NYCTaxiWorkspace1`.
1. В этой рабочей области вы увидите набор данных с именем `Passenger Analysis` и отчет с именем `Passenger Analysis`.
1. Наведите указатель мыши на набор данных `PassengerAnalysis`, щелкните значок с тремя точками и выберите **Параметры**.
1. В разделе **Учетные данные источника данных** задайте для параметра **Способ проверки подлинности** значение **OAuth2** и выберите **Вход**.

### <a name="edit-a-report-in-synapse-studio"></a>Изменение отчета в Synapse Studio

1. Вернитесь в Synapse Studio и выберите **Закрыть и обновить** 
1. Использование центра **Разработка** 
1. Наведите указатель мыши на **Power BI** и щелкните, чтобы обновить узел **Отчеты Power BI**.
1. Теперь в **Power BI** должны отобразиться: a. * В разделе **NYCTaxiWorkspace1 > Наборы данных Power BI**, новый набор с именем **PassengerAnalysis**.
    b. * В разделе **NYCTaxiWorkspace1 > Отчеты Power BI**, новый отчет с именем **PassengerAnalysis**.
1. Выберите отчет **PassengerAnalysis**. 
1. Отчет откроется, и теперь можно будет изменить его непосредственно в Synapse Studio.

## <a name="monitor-activities"></a>Действия Monitor

1. В Synapse Studio перейдите в центр "Монитор".
1. Здесь вы можете просмотреть журнал всех действий, выполняемых в рабочей области, и какие из них сейчас активны.
1. Изучите разделы **Выполнение конвейера**, **Приложения Apache Spark** и **Запросы SQL**, а также просмотрите, что вы уже сделали в рабочей области.

## <a name="next-steps"></a>Дальнейшие действия

Дополнительные сведения о [Azure Synapse Analytics (предварительная версия)](overview-what-is.md)

