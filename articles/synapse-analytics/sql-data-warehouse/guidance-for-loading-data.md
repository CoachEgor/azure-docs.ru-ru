---
title: Загрузка данных лучшие практики для бассейна Synapse S'L
description: Рекомендации и оптимизация производительности для загрузки данных с помощью пула Synapse S'L.
services: synapse-analytics
author: kevinvngo
manager: craigg
ms.service: synapse-analytics
ms.topic: conceptual
ms.subservice: ''
ms.date: 02/04/2020
ms.author: kevin
ms.reviewer: igorstan
ms.custom: azure-synapse
ms.openlocfilehash: e170a789727fb0de36705895245cc638d30ee3d7
ms.sourcegitcommit: bd5fee5c56f2cbe74aa8569a1a5bce12a3b3efa6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/06/2020
ms.locfileid: "80745508"
---
# <a name="best-practices-for-loading-data-using-synapse-sql-pool"></a>Рекомендации по загрузке данных с помощью пула Synapse S'L

В этой статье вы узнаете рекомендации и оптимизацию производительности для загрузки данных с помощью пула S'L.

## <a name="preparing-data-in-azure-storage"></a>Подготовка данных в службе хранилища Azure

Чтобы свести к минимуму задержку, свяжете слой хранилища и пул S'L.

При экспорте данных в файл формата ORC могут возникнуть ошибки, связанные с нехваткой памяти в Java, если есть текстовые столбцы больших размеров. Чтобы обойти это ограничение, экспортируйте только подмножество столбцов.

PolyBase не может загружать строки, которые имеют более 1 000 000 байт данных. Объем данных, загружаемых в текстовые файлы в хранилище BLOB-объектов Azure или Azure Data Lake Store, не должен превышать 1 000 000 байтов. Это ограничение байтов не зависит от определенной схемы таблицы.

Все форматы файлов имеют разные характеристики производительности. Чтобы максимально ускорить загрузку, используйте сжатые текстовые файлы с разделителями. Разница между производительностью UTF-8 и UTF-16 будет минимальной.

Разбейте большие сжатые файлы на сжатые файлы меньшего размера.

## <a name="running-loads-with-enough-compute"></a>Выполнение загрузок с достаточным объемом вычислений

Чтобы максимально ускорить загрузку, выполняйте только одно задание загрузки за раз. Если это невозможно, заполните минимальное количество нагрузок одновременно. Если вы ожидаете большую загрузку, подумайте о расширении пула перед загрузкой.

Чтобы запускать загрузки в соответствующих вычислительных ресурсах, создайте пользователей, назначенных для выполнения загрузок. Назначайте каждого пользователя загрузки определенному классу ресурсов или группе рабочей нагрузки. Чтобы запустить нагрузку, вопием в качестве одного из пользователей загрузки, а затем запустите нагрузку. Загрузка выполняется с помощью класса ресурсов пользователя.  

> [!NOTE]
> Использовать этот метод проще, чем пытаться изменить класс ресурсов пользователя в соответствии с текущими потребностями.

### <a name="example-of-creating-a-loading-user"></a>Пример создания пользователя, выполняющего загрузку

В этом примере создается пользователь, выполняющий загрузку, для класса ресурсов staticrc20. На первом шаге устанавливается **подключение к главному серверу** и создается имя входа.

```sql
   -- Connect to master
   CREATE LOGIN LoaderRC20 WITH PASSWORD = 'a123STRONGpassword!';
```

Подключитесь к пулу S'L и создайте пользователя. Следующий код предполагает, что вы подключены к базе данных под названием mySampleDataWarehouse. Он показывает, как создать пользователя под названием LoaderRC20 и дает разрешение управления пользователем в базе данных. Затем он добавляет пользователя в качестве члена роли базы данных staticrc20.  

```sql
   -- Connect to the database
   CREATE USER LoaderRC20 FOR LOGIN LoaderRC20;
   GRANT CONTROL ON DATABASE::[mySampleDataWarehouse] to LoaderRC20;
   EXEC sp_addrolemember 'staticrc20', 'LoaderRC20';
```

Чтобы запустить нагрузку с ресурсами для классов ресурсов staticRC20, вопийте как LoaderRC20 и запустите нагрузку.

Запускайте загрузку в статических (не динамических) классах ресурсов. Статические классы ресурсов гарантируют, что вы используете одни и те же ресурсы, независимо от [единиц использования хранилища данных](what-is-a-data-warehouse-unit-dwu-cdwu.md). Если вы используете динамический класс ресурсов, ресурсы будут разными на разных уровнях обслуживания.

При использовании динамических классов более низкий уровень обслуживания означает, что, возможно, для пользователя, выполняющего загрузку, потребуется более высокий класс ресурса.

## <a name="allowing-multiple-users-to-load"></a>Разрешение выполнения загрузки нескольким пользователям

Часто возникает необходимость в том, чтобы несколько пользователев загружали данные в пул S'L. Для загрузки с помощью инструкции [CREATE TABLE AS SELECT (Transact-SQL)](/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json&view=azure-sqldw-latest) требуются разрешения CONTROL базы данных.  Разрешение CONTROL предоставляет возможность контроля доступа ко всем схемам.

Возможно, потребуется, чтобы не все пользователи, выполняющие загрузку, имели возможность контролировать доступ ко всем схемам. Чтобы ограничить разрешения, можно использовать инструкцию DENY CONTROL.

Пример. Рассмотрим схемы базы данных schema_A для отдела A и schema_B для отдела B. Задайте пользователей базы данных user_A и user_B в качестве пользователей для загрузки PolyBase в отделе A и B соответственно. Им обоим предоставлены разрешения CONTROL для базы данных. Теперь создатели схем A и B блокируют свои схемы, используя инструкцию DENY:

```sql
   DENY CONTROL ON SCHEMA :: schema_A TO user_B;
   DENY CONTROL ON SCHEMA :: schema_B TO user_A;
```

User_A и user_B теперь заблокированы из схемы другого отдела.

## <a name="loading-to-a-staging-table"></a>Загрузка в промежуточную таблицу

Для достижения самой быстрой скорости загрузки для перемещения данных в бильярдный стол S'L загрузите данные в промежуточную таблицу.  Определите промежуточную таблицу в виде кучи и используйте циклический перебор как вариант распространения.

Учтите, что загрузка обычно представляет собой двухэтапный процесс, в ходе которого вы сначала загружаете на плацдарм, а затем вставляете данные в таблицу пула производства S'L. Если в рабочей таблице используется хэш-распределение, общее время загрузки и вставки можно сократить, если определить промежуточную таблицу с хэш-распределением.

Загрузка в промежуточную таблицу занимает больше времени, но на втором шаге вставки строк в рабочую таблицу перемещение данных между распределениями не происходит.

## <a name="loading-to-a-columnstore-index"></a>Загрузка в индекс columnstore

Индексы columnstore требуют огромные объемы памяти для сжатия данных в группы строк высокого качества. Чтобы обеспечить лучшее сжатие и эффективность индексов, индекс columnstore должен сжать в каждую группу строк не более 1 048 576 строк.

При нехватке памяти индекс columnstore не сможет добиться максимального сжатия. Этот сценарий, в свою очередь, влияет на производительность запроса. Дополнительные сведения см. в статье [Максимальное повышение качества группы строк для индекса columnstore](sql-data-warehouse-memory-optimizations-for-columnstore-compression.md).

- Чтобы пользователям, выполняющим загрузку, хватало памяти для максимального сжатия, они должны быть членами средних и крупных классов ресурсов.
- Загрузите достаточно строк, чтобы полностью заполнить новые группы строк. При массовой загрузке каждые 1 048 576 строк сжимаются непосредственно в индекс columnstore в виде полной группы строк. При загрузках менее 102 400 строк строки отправляются в deltastore, где они хранятся в индексе сбалансированного дерева.

> [!NOTE]
> Если вы загружаете слишком мало строк, они могут быть сжаты в формате магазина столбцов.

## <a name="increase-batch-size-when-using-sqlbulkcopy-api-or-bcp"></a>Увеличьте размер партии при использовании API SqLBulkCopy или bcp

Загрузка с Помощью PolyBase обеспечит самую высокую пропускную выливку с помощью пула S'L. Если вы не можете использовать PolyBase для загрузки и должны использовать [API SqLBulkCopy](/dotnet/api/system.data.sqlclient.sqlbulkcopy?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json) или [bcp,](/sql/tools/bcp-utility?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json&view=azure-sqldw-latest)вы должны рассмотреть вопрос об увеличении размера партии для лучшей пропускной всей входной.

> [!TIP]
> Размер партии от 100 K до 1M строк является рекомендуемым базовым уровнем для определения оптимальной емкости размера партии.

## <a name="handling-loading-failures"></a>Обработка ошибок загрузки

Загрузка с помощью внешней таблицы может завершиться ошибкой *Запрос прерван — достигнуто максимальное пороговое значение отклонения при чтении из внешнего источника*. Это сообщение означает, что внешние данные содержат "грязные" записи.

Запись данных считается грязной, если она соответствует одному из следующих условий:

- Типы данных и количество столбцов не соответствуют определениям столбца внешней таблицы.
- Данные не соответствуют указанному внешнему формату файла.

Чтобы устранить "грязные" записи, убедитесь в правильности определений внешней таблицы и формата внешнего файла, а также в том, что внешние данные соответствуют этим определениям.

Если подмножество внешних записей данных является грязным, вы можете отклонить эти записи для ваших запросов, используя варианты отклонения в [CREATE EXTERNAL TABLE (Transact-S'L).](/sql/t-sql/statements/create-external-table-transact-sql?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json&view=azure-sqldw-latest)

## <a name="inserting-data-into-a-production-table"></a>Вставка данных в рабочую таблицу

Одноразовую загрузку в небольшую таблицу или даже периодическую перезагрузку результатов поиска рекомендуется выполнять, используя такой [синтаксис инструкции INSERT](/sql/t-sql/statements/insert-transact-sql?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json&view=azure-sqldw-latest): `INSERT INTO MyLookup VALUES (1, 'Type 1')`.  Тем не менее единоразовые множественные вставки не так эффективны, как массовая загрузка.

Если вы выполняете тысячи или более вставок в течение дня, объедините вставки в пакет, чтобы выполнить массовую загрузку.  Разработайте процессы для добавления единоразовых вставок в файл, а затем создайте другой процесс, который периодически загружает файл.

## <a name="creating-statistics-after-the-load"></a>Создание статистики после загрузки

Чтобы повысить производительность запросов, важно сформировать статистические данные для всех столбцов всех таблиц после первой загрузки или после значительных изменений в данных. Создание статистики может быть сделано вручную или вы можете включить [AUTO_CREATE_STATISTICS](sql-data-warehouse-tables-statistics.md#automatic-creation-of-statistic).

Дополнительные сведения см. в статье об [управлении статистикой](sql-data-warehouse-tables-statistics.md). Ниже приводится следующий пример, как вручную создавать статистику по пяти столбцам таблицы Customer_Speed.

```sql
create statistics [SensorKey] on [Customer_Speed] ([SensorKey]);
create statistics [CustomerKey] on [Customer_Speed] ([CustomerKey]);
create statistics [GeographyKey] on [Customer_Speed] ([GeographyKey]);
create statistics [Speed] on [Customer_Speed] ([Speed]);
create statistics [YearMeasured] on [Customer_Speed] ([YearMeasured]);
```

## <a name="rotate-storage-keys"></a>Смена ключей к хранилищу данных

В целях безопасности рекомендуем регулярно менять ключ доступа к хранилищу BLOB-объектов. У вас есть два ключа к хранилищу данных для учетной записи хранения больших двоичных объектов, что дает возможность перемещать ключи.

Для смены ключей учетной записи службы хранилища Azure выполните следующие действия.

Для каждой учетной записи хранения с измененным ключом выполните [ALTER DATABASE SCOPED CREDENTIAL](/sql/t-sql/statements/alter-database-scoped-credential-transact-sql?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json&view=azure-sqldw-latest).

Пример

Исходный ключ создан

```sql
CREATE DATABASE SCOPED CREDENTIAL my_credential WITH IDENTITY = 'my_identity', SECRET = 'key1'
```

Замена ключа 1 ключом 2

```sql
ALTER DATABASE SCOPED CREDENTIAL my_credential WITH IDENTITY = 'my_identity', SECRET = 'key2'
```

Вносить изменения в базовые внешние источники данных больше не нужно.

## <a name="next-steps"></a>Дальнейшие действия

- Дополнительные сведения о PolyBase и проектировании процесса извлечения, загрузки и преобразования (ELT) см. в статье о [проектировании ELT для хранилища данных SQL](design-elt-data-loading.md).
- Руководство по загрузке см. в статье [Загрузка данных из хранилища BLOB-объектов Azure в хранилище данных SQL Azure с помощью PolyBase](load-data-from-azure-blob-storage-using-polybase.md).
- Инструкции по мониторингу загрузки данных см. в статье [Мониторинг рабочей нагрузки с помощью динамических административных представлений](sql-data-warehouse-manage-monitor.md).
