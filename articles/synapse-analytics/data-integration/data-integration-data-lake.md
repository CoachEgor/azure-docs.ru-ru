---
title: Прием в Azure Data Lake Storage 2-го поколения в Azure синапсе Analytics
description: Узнайте, как принимать данные в Azure Data Lake Storage 2-го поколения в Azure синапсе Analytics.
services: synapse-analytics
author: djpmsft
ms.service: synapse-analytics
ms.topic: conceptual
ms.subservice: ''
ms.date: 04/15/2020
ms.author: daperlov
ms.reviewer: jrasnick
ms.openlocfilehash: 4d7d7be523749797e5dbce0e50c307fc682974f2
ms.sourcegitcommit: 849bb1729b89d075eed579aa36395bf4d29f3bd9
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/28/2020
ms.locfileid: "81430582"
---
# <a name="ingesting-data-into-azure-data-lake-storage-gen2"></a>Прием данных в Azure Data Lake Storage 2-го поколения 

В этой статье вы узнаете, как принимать данные из одного расположения в другое в учетной записи хранения Azure Data Lake Gen 2 (Azure Data Lake Gen 2) с помощью Azure синапсе Analytics.

## <a name="prerequisites"></a>Предварительные условия

* **Подписка Azure**. Если у вас еще нет подписки Azure, создайте [бесплатную учетную запись Azure](https://azure.microsoft.com/free/) , прежде чем начинать работу.
* **Учетная запись хранения Azure**. в качестве *исходного* хранилища данных используется Azure Data Lake Gen 2. Если у вас нет учетной записи хранения, создайте ее, как описано в статье [Создание учетной записи хранения Azure](../../storage/blobs/data-lake-storage-quickstart-create-account.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json) .

## <a name="create-linked-services"></a>Создание связанных служб

В Azure синапсе Analytics связанная служба — это место, где вы определяете сведения о соединении с другими службами. В этом разделе вы добавите Azure синапсе Analytics и Azure Data Lake Gen 2 в качестве связанных служб.

1. Откройте Azure синапсе Analytics UX и перейдите на вкладку **Управление** .
1. В разделе **внешние подключения**выберите **связанные службы**.
1. Чтобы добавить связанную службу, нажмите кнопку **создать**.
1. Выберите плитку Azure Data Lake Storage 2-го поколения из списка и нажмите кнопку **продолжить**.
1. Введите учетные данные для проверки подлинности. Ключ учетной записи, субъект-служба и управляемое удостоверение в настоящее время поддерживаются типами проверки подлинности. Нажмите кнопку Проверить подключение, чтобы проверить правильность учетных данных. 
1. После завершения нажмите **Создать**.

## <a name="create-pipeline"></a>Создание конвейера

Конвейер содержит логический поток для выполнения набора действий. В этом разделе вы создадите конвейер, содержащий действие копирования, которое принимает данные из Azure Data Lake Gen 2 в пул SQL.

1. Перейдите на вкладку " **orchestration** ". Щелкните значок "плюс" рядом с заголовком конвейеров и выберите **конвейер**.
1. В разделе **Перемещение и преобразование** на панели действия перетащите элемент **Копировать данные** на холст конвейера.
1. Щелкните действие копирования и перейдите на вкладку **источник** . Нажмите кнопку **создать** , чтобы создать новый исходный набор данных.
1. Выберите Azure Data Lake Storage 2-го поколения в качестве хранилища данных и нажмите кнопку продолжить.
1. Выберите DelimitedText в качестве формата и нажмите кнопку продолжить.
1. На панели Set Properties (задание свойств) выберите созданную связанную службу ADLS. Укажите путь к исходным данным и укажите, содержит ли первая строка заголовок. Схему можно импортировать из хранилища файлов или из образца файла. Щелкните "OK", когда все будет готово.
1. Перейдите на вкладку **приемник** . Нажмите кнопку **создать** , чтобы создать новый набор данных приемника.
1. Выберите Azure Data Lake Storage Gen2 в качестве хранилища данных и нажмите кнопку продолжить.
1. Выберите DelimitedText в качестве формата и нажмите кнопку продолжить.
1. На панели Set Properties (задание свойств) выберите созданную связанную службу ADLS. Укажите путь к папке, в которую вы хотите записать данные. Щелкните "OK", когда все будет готово.

## <a name="debug-and-publish-pipeline"></a>Отладка и публикация конвейера

После завершения настройки конвейера можно выполнить отладку перед публикацией артефактов, чтобы убедиться, что все правильно.

1. Чтобы выполнить отладку конвейера, на панели инструментов щелкните **Отладка**. Состояние выполнения конвейера вы можете найти на вкладке **Выходные данные** в нижней части окна. 
1. После успешного выполнения конвейера на верхней панели инструментов выберите **опубликовать все**. Это действие публикует сущности (наборы данных и конвейеры), созданные в службе синапсе Analytics.
1. Дождитесь сообщения **Successfully published** (Публикация выполнена). Чтобы просмотреть сообщения с уведомлениями, нажмите кнопку колокольчика в правом верхнем углу. 


## <a name="trigger-and-monitor-the-pipeline"></a>Активация и мониторинг конвейера

На этом шаге вы вручную активируете конвейер, опубликованный на предыдущем шаге. 

1. Выберите **Добавить триггер** на панели инструментов, а затем **Trigger Now** (Запустить сейчас). На странице **Pipeline Run** (Запуск конвейера) нажмите кнопку **Готово**.  
1. Перейдите на вкладку **монитор** , расположенную на левой боковой панели. Вы увидите выполнение конвейера, которое вы только что активировали вручную. Ссылки в столбце **действия** можно использовать для просмотра сведений о действиях и повторного запуска конвейера.
1. Чтобы просмотреть запуски действий, связанные с этим запуском конвейера, щелкните ссылку **View Activity Runs** (Просмотр запусков действий) в столбце **Действия**. В нашем примере определено только одно действие, поэтому в списке вы увидите только одну запись. Чтобы увидеть сведения об операции копирования, щелкните ссылку **Сведения** (значок очков) в столбце **Действия**. Выберите **Конвейеры Runs** (Запуски конвейера) в верхней части окна, чтобы вернуться к представлению Pipeline Runs (Запуски конвейера). Чтобы обновить список, нажмите кнопку **Обновить**.
1. Убедитесь, что данные правильно написаны в пуле SQL.


## <a name="next-steps"></a>Дальнейшие шаги

Дополнительные сведения об интеграции данных для синапсе Analytics см. в статье прием [данных в пул SQL](data-integration-sql-pool.md) .
