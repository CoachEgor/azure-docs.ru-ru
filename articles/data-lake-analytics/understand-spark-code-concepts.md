---
title: Поймите концепции кода Apache Spark для разработчиков Azure Data Lake.
description: В этой статье описаны концепции Apache Spark, которые помогут разработчикам U-S'Sl понять концепции кода Spark.
author: guyhay
ms.author: guyhay
ms.reviewer: jasonh
ms.service: data-lake-analytics
ms.topic: conceptual
ms.custom: Understand-apache-spark-code-concepts
ms.date: 10/15/2019
ms.openlocfilehash: bdb38e36a9f1344a3adde15d349a2ec176c0fe95
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/27/2020
ms.locfileid: "74424005"
---
# <a name="understand-apache-spark-code-for-u-sql-developers"></a>Понять код Apache Spark для разработчиков U-S'SL

В этом разделе содержатся рекомендации высокого уровня по преобразованию скриптов U-S'L в Apache Spark.

- Она начинается с [сравнения парадигм обработки двух языков](#understand-the-u-sql-and-spark-language-and-processing-paradigms)
- Предоставляет советы о том, как:
   - [Преобразование скриптов,](#transform-u-sql-scripts) включая [выражения строки](#transform-u-sql-rowset-expressions-and-sql-based-scalar-expressions) U-S'SL
   - [Код .NET](#transform-net-code)
   - [Типы данных](#transform-typed-values)
   - [Каталог объектов](#transform-u-sql-catalog-objects).

## <a name="understand-the-u-sql-and-spark-language-and-processing-paradigms"></a>Понимание языковых и обрабатывающих парадигм U-S'S'L и Spark

Перед тем, как начать миграцию скриптов U-S'L компании Azure Data Lake Analytics в Spark, полезно понять общую философию и философию обработки двух систем.

U-S'L — это язык декларативного запроса, похожий на S'L, который использует парадигму потока данных и позволяет легко встраивать и масштабировать пользовательский код, написанный в .NET (например, в C), Python и R. Расширения пользователя могут реализовывать простые выражения или функции, определенные пользователем, но также могут предоставить пользователю возможность реализовать так называемые пользовательские операторы, которые реализуют пользовательских операторов для выполнения преобразований уровня строк, извлечений и вывода записи.

Spark — это масштабная платформа, предлагающая несколько языковых привязок в Scala, Java, Python, .NET и т.д., где вы в основном пишете свой код на одном из этих языков, создаете абстракции данных, называемые устойчивыми распределенными наборами данных (RDD), кадрами данных и наборами данных и затем используйте язык, похожий на ленобласть, для преобразования. Он также предоставляет SparkS'L в качестве декларативного подязыка на фрейме данных и абстракциях набора данных. DSL предоставляет две категории операций, преобразований и действий. Применение преобразований к абстракциям данных не будет выполнять преобразование, а наоборот, наращивать план выполнения, который будет отправлен для оценки с действием (например, запись результата во временную таблицу или файл или печать результат).

Таким образом, при переводе сценария U-S'L в программу Spark, вы должны решить, какой язык вы хотите использовать, по крайней мере, для генерации абстракции кадра данных (которая в настоящее время является наиболее часто используемой абстракцией данных) и хотите ли вы написать декларативную преобразования потока данных с использованием DSL или SparkS'L. В некоторых более сложных случаях может потребоваться разделить сценарий U-S'L на последовательность Spark и другие действия, реализованные с azure Batch или Azure Functions.

Кроме того, Azure Data Lake Analytics предлагает U-S'L в среде службы без серверов, в то время как Azure Databricks и Azure HDInsight предлагают Spark в виде кластерного сервиса. При преобразовании приложения необходимо учитывать последствия создания, масштабирования, масштабирования и вывода из эксплуатации кластеров.

## <a name="transform-u-sql-scripts"></a>Преобразование скриптов U-S'L

Скрипты U-S'L следуют следующему шаблону обработки:

1. Данные считывается либо из `EXTRACT` неструктурированных файлов, используя выписку, спецификацию местоположения или набора файлов, а также встроенный или пользовательский экстрактор и желаемую схему, либо из таблиц U-S'L (управляемые или внешние таблицы). Он представлен как ряд.
2. Наборы строк преобразуются в несколько инструкций U-S'L, которые применяют выражения U-S'L к рядовые панели и производят новые ряды.
3. Наконец, полученные ряды выделяются в файлы с использованием `OUTPUT` оператора, определяющего местоположение (ы) и встроенного или определяемого пользователем выходного, или в таблицу U-S'L.

Сценарий оценивается лениво, что означает, что каждый шаг извлечения и преобразования состоит в дерево выражения и глобально оценивается (поток данных).

Программы Spark похожи тем, что вы будете использовать разъемы Spark для чтения данных и создания фреймов данных, а затем применять преобразования на кадрах данных, используя либо LIN-как DSL или SparkS'L, а затем написать результат в файлы, временные таблицы Spark, некоторые типы языка программирования или консоль.

## <a name="transform-net-code"></a>Преобразование кода .NET

Язык выражения U-S'L — это язык выражения , и он предлагает различные способы масштабирования пользовательского кода .NET.

Поскольку Spark в настоящее время не поддерживает выполнение кода .NET, вам придется либо переписать свои выражения в эквивалентное выражение Spark, Scala, Java или Python, либо найти способ зайти в свой код .NET. Если в скрипте используются библиотеки .NET, у вас есть следующие варианты:

- Переведите код .NET в Scala или Python.
- Разделите свой скрипт U-S'L на несколько этапов, где для применения преобразований .NET (если вы можете получить приемлемую шкалу)
- Используйте привязку языка .NET, доступную в Открытом исходном коде под названием Moebius. Этот проект не находится в состоянии поддержки.

В любом случае, если у вас есть большое количество логики .NET в ваших сценариях U-S'L, пожалуйста, свяжитесь с нами через ваш представитель учетной записи Майкрософт для получения дальнейших указаний.

Следующие сведения приведены в различных случаях использования .NET и C-L в сценариях U-S'L.

### <a name="transform-scalar-inline-u-sql-c-expressions"></a>Преобразование внеочередных выражений U-S'SL C

Язык выражения U-S'L — СЗ. Многие из рядовых выражений U-S'L реализованы на месте для повышения производительности, в то время как более сложные выражения могут выполняться через вызов в фреймворк .NET.

Spark имеет свой собственный язык скалярного выражения (либо как часть DSL, либо в SparkS'L) и позволяет заскакить на пользовательские функции, написанные на языке хостинга.

Если у вас есть скалярные выражения в U-S'L, вы должны сначала найти наиболее подходящее наиболее понятное из них скалярное выражение Spark scalar, чтобы получить максимальную производительность, а затем сопоставить другие выражения в функцию, определяемую пользователем, на языке хостинга Spark по вашему выбору.

Имейте в виду, что .NET и C з имеют различные семантики типа, чем языки хостинга Spark и DSL Spark. Ниже [below](#transform-typed-values) приведены более подробные сведения о различиях в системе типов.

### <a name="transform-user-defined-scalar-net-functions-and-user-defined-aggregators"></a>Преобразование функций scalar .NET, определяемых пользователем, и агрегаторы, определяемые пользователем

U-S'L предоставляет способы вызова произвольных функций scalar .NET и вызова пользовательских агрегаторов, написанных в .NET.

Spark также предлагает поддержку пользовательских функций и пользовательских агрегаторов, написанных на большинстве его хостинговых языков, которые можно назвать из DSL и SparkSL.

### <a name="transform-user-defined-operators-udos"></a>Трансформация операторов, определяемых пользователями (UDOs)

U-S'L предоставляет несколько категорий пользователей,определенных операторов (UDOs), таких как экстракторы, выходные, редукторы, процессоры, аппликаторы и комбайны, которые могут быть написаны в .NET (и - в некоторой степени - в Python и R).

Spark не предлагает такую же модель расширяемости для операторов, но имеет эквивалентные возможности для некоторых.

Spark эквивалент экстракторов и выходов является РазъемsSSPark. Для многих экстракторов U-S'L вы можете найти эквивалентный разъем в сообществе Spark. Для других, вам придется написать пользовательский разъем. Если экстрактор U-S'L является сложным и использует несколько библиотек .NET, возможно, предпочтительнее построить разъем в Scala, который использует interop для вызова в библиотеку .NET, которая делает фактическую обработку данных. В этом случае вам придется развернуть время выполнения .NET Core в кластер Spark и убедиться, что упомянутые библиотеки .NET соответствуют требованиям .NET Standard 2.0.

Другие типы U-S'L UDOs должны быть переписаны с помощью пользовательских функций и агрегаторов, а также семантически соответствующего выражения Spark DLS или SparkS'L. Например, процессор можно отобразить в SELECT различных вызовов UDF, упакованных в функцию, которая принимает фрейм данных в качестве аргумента и возвращает фрейм данных.

### <a name="transform-u-sqls-optional-libraries"></a>Преобразование дополнительных библиотек U-S'SL

U-S'S'L предоставляет набор дополнительных и демо-библиотек, которые предлагают [Python](data-lake-analytics-u-sql-python-extensions.md), [R](data-lake-analytics-u-sql-r-extensions.md), [JSON, XML, AVRO поддержки](https://github.com/Azure/usql/tree/master/Examples/DataFormats), а также некоторые [возможности когнитивных услуг.](data-lake-analytics-u-sql-cognitive.md)

Spark предлагает собственные Python и R интеграции, pySpark и SparkR соответственно, и предоставляет разъемы для чтения и записи JSON, XML, и AVRO.

Если вам необходимо преобразовать скрипт, отсылающий к библиотекам когнитивных услуг, мы рекомендуем связаться с нами через представителя учетной записи Майкрософт.

## <a name="transform-typed-values"></a>Преобразование набранных значений

Поскольку система типа U-S'L основана на системе типа .NET, а Spark имеет свою собственную систему типа, которая зависит от связывания языка хоста, вам придется убедиться, что типы, на которых вы работаете, близки, а для определенных типов диапазоны типа, точность и/или масштаб могут быть немного разными. Кроме того, U-S'L `null` и Spark по-разному относятся к значениям.

### <a name="data-types"></a>Типы данных

В следующей таблице приведены эквивалентные типы в Spark, Scala и PySpark для данного типа U-S'SL.

| U-SQL | Spark |  Scala | PySpark |
| ------ | ------ | ------ | ------ |
|`byte`       ||||
|`sbyte`      |`ByteType` |`Byte` | `ByteType`|
|`int`        |`IntegerType` |`Int` | `IntegerType`|
|`uint`       ||||
|`long`       |`LongType` |`Long` | `LongType`|
|`ulong`      ||||
|`float`      |`FloatType` |`Float` | `FloatType`|
|`double`     |`DoubleType` |`Double` | `DoubleType`|
|`decimal`    |`DecimalType` |`java.math.BigDecimal` | `DecimalType`|
|`short`      |`ShortType` |`Short` | `ShortType`|
|`ushort`     ||||
|`char`   | |`Char`||
|`string` |`StringType` |`String` |`StringType` |
|`DateTime`   |`DateType`, `TimestampType` |`java.sql.Date`, `java.sql.Timestamp` | `DateType`, `TimestampType`|
|`bool`   |`BooleanType` |`Boolean` | `BooleanType`|
|`Guid`   ||||
|`byte[]` |`BinaryType` |`Array[Byte]` | `BinaryType`|
|`SQL.MAP<K,V>`   |`MapType(keyType, valueType, valueContainsNull)` |`scala.collection.Map` | `MapType(keyType, valueType, valueContainsNull=True)`|
|`SQL.ARRAY<T>`   |`ArrayType(elementType, containsNull)` |`scala.collection.Seq` | `ArrayType(elementType, containsNull=True)`|

Дополнительные сведения см. в разделе:

- [org.apache.spark.sql.types](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.package)
- [Типы искры S'L и DataFrame](https://spark.apache.org/docs/latest/sql-reference.html#data-types)
- [Типы значений Scala](https://www.scala-lang.org/api/current/scala/AnyVal.html)
- [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)

### <a name="treatment-of-null"></a>Лечение NULL

В Spark типы по умолчанию позволяют значенияnull, в то время как в U-S'L вы явно помечаете scalar, не объект как недействительный. Хотя Spark позволяет определить столбец как не подлежащий нулю, он не будет применять ограничение и [может привести к неправильному результату.](https://medium.com/@weshoffman/apache-spark-parquet-and-troublesome-nulls-28712b06f836)

В Spark, NULL указывает, что значение неизвестно. Значение Spark NULL отличается от любого значения, включая себя. Сравнения между двумя значениями Spark NULL, или между значением NULL и любым другим значением, возвращаются неизвестные, поскольку значение каждого NULL неизвестно.  

Такое поведение отличается от U-S'L, который `null` следует семантике СИ, где отличается от любого значения, но равна себе.  

Таким образом, заявление `SELECT` `WHERE column_name = NULL` SparkS'L, используювнулевое значение `column_name`нулевых строк, даже если в U-S'L есть значения NULL, в то время как в U-S'L, оно возвращает строки, где `column_name` `null`установлено. Аналогичным образом, `SELECT` заявление Spark, используюееееено `WHERE column_name != NULL` нулевые `column_name`строки, даже если в U-S'L есть значения, не являющиеся нулевыми, оно возвращает строки, не являющиеся нулевыми. Таким образом, если вы хотите семантики нулевой проверки U-S'L, вы должны использовать [isnull](https://spark.apache.org/docs/2.3.0/api/sql/index.html#isnull) и [isnotnull](https://spark.apache.org/docs/2.3.0/api/sql/index.html#isnotnull) соответственно (или их эквивалент DSL).

## <a name="transform-u-sql-catalog-objects"></a>Преобразование объектов каталога U-S'L

Одним из основных отличий является то, что U-S'L Scripts могут использовать свои объекты каталога, многие из которых не имеют прямого эквивалента Spark.

Spark действительно обеспечивает поддержку концепций магазина Hive Meta, в основном баз данных и таблиц, так что вы можете отобразить базы данных и схемы U-S'L в базы данных Hive, а также таблицы U-S'L в таблицах Spark (см. [Перемещение данных, хранящихся в таблицах U-S'L),](understand-spark-data-formats.md#move-data-stored-in-u-sql-tables)но у него нет поддержки представлений, ценных функций (TVFs), сохраненных процедур, U-S'L, внешних регистраторов данных.

Объекты кода U-S'L, такие как представления, TVFs, сохраненные процедуры и сборки, могут быть смоделированы через функции кода и библиотеки в Spark и ссылаться на использование функции узла и механизмов процедурной абстракции (например, путем импорта Модули Python или ссылки на функции Scala).

Если каталог U-S'L использовался для обмена данными и кодовыми объектами между проектами и группами, то необходимо использовать эквивалентные механизмы совместного использования (например, Maven для совместного использования объектов кода).

## <a name="transform-u-sql-rowset-expressions-and-sql-based-scalar-expressions"></a>Преобразование выражений строки На основе U-S'L и скалярных выражений на основе S'L

Основным языком U-S'L является преобразование рядов и основано на S'L. Ниже приводится неполный список наиболее распространенных выражений строк, предлагаемых в U-S'L:

- `SELECT`/`FROM`/`WHERE`/`GROUP BY`(Агрегаты)`HAVING`/`ORDER BY`+`FETCH`
- `INNER`/`OUTER`/`CROSS`/`SEMI``JOIN` выражения
- `CROSS`/`OUTER``APPLY` выражения
- `PIVOT`/`UNPIVOT`Выражения
- `VALUES`конструктор рядов

- Набор выражений`UNION`/`OUTER UNION`/`INTERSECT`/`EXCEPT`

Кроме того, U-S'L предоставляет различные

- `OVER`выражения оконирования
- различные встроенные агрегаторы и рейтинговые функции (и`SUM` `FIRST` т.д.)
- Некоторые из наиболее знакомых scalar выражений `IN` `AND`S'L: `CASE`, `LIKE`, (`NOT`) , `OR` и т.д.

Spark предлагает эквивалентные выражения в форме DSL и SparkS'L для большинства из этих выражений. Некоторые выражения, не поддерживаемые в Spark, должны быть переписаны с помощью сочетания родных выражений Spark и семантически эквивалентных шаблонов. Например, `OUTER UNION` необходимо будет перевести в эквивалентное сочетание прогнозов и союзов.

Из-за разной обработки значений NULL, соединение U-S'L всегда совпадает с строкой, если оба сравниваемых столбца содержат значение NULL, в то время как соединение в Spark не будет соответствовать таким столбикам, если не будут добавлены явные нулевые проверки.

## <a name="transform-other-u-sql-concepts"></a>Преобразование других концепций U-S'L

U-S'L также предлагает множество других функций и концепций, таких как федеративные запросы в отношении баз данных s'L Server, параметров, scalar и переменных выражения лямбды, системных переменных, `OPTION` подсказок.

### <a name="federated-queries-against-sql-server-databasesexternal-tables"></a>Федеративные запросы на базе данных S'L Server/внешние таблицы

U-S'L предоставляет исходные данные и внешние таблицы, а также прямые запросы в отношении базы данных Azure S'L. Хотя Spark не предлагает те же абстракции объектов, он предоставляет [разъем Spark для базы данных Azure S'L,](../sql-database/sql-database-spark-connector.md) который может быть использован для запроса баз данных S'L.

### <a name="u-sql-parameters-and-variables"></a>Параметры и переменные U-S'L

Параметры и пользовательские переменные имеют эквивалентные понятия в Spark и их хостинговых языках.

Например, в Scala можно определить `var` переменную с помощью ключевого слова:

```
var x = 2 * 3;
println(x)
```

Системные переменные U-S'L (переменные, начиная с) `@@`можно разделить на две категории:

- Переменные settable системы, которые могут быть установлены на определенные значения, чтобы повлиять на поведение скриптов
- Информационные системные переменные, которые запрашивает информацию о системе и уровне работы

Большинство переменных settable системы не имеют прямого эквивалента в Spark. Некоторые переменные информационной системы можно смоделировать, передав информацию в качестве аргументов во время выполнения задания, другие могут иметь эквивалентную функцию на языке хостинга Spark.

### <a name="u-sql-hints"></a>Подсказки U-S'L

U-S'S'L предлагает несколько синтаксических способов предоставления подсказок оптимизатору запроса и движку исполнения:  

- Установка переменной системы U-S'L
- положение, `OPTION` связанное с выражением строки для предоставления подсказки данных или плана
- подсказка соединения в синтаксисе выражения соединения `BROADCASTLEFT`(например, )

Оптимизатор затрат spark, основанный на затратах, имеет свои собственные возможности для предоставления подсказок и настройки производительности запроса. Пожалуйста, обратитесь к соответствующей документации.

## <a name="next-steps"></a>Дальнейшие действия

- [Понимание форматов данных Spark для разработчиков U-S'SL](understand-spark-data-formats.md)
- [.NET для Apache Spark](https://docs.microsoft.com/dotnet/spark/what-is-apache-spark-dotnet)
- [Обновление решений для аналитики больших данных с Azure Data Lake Storage 1-го поколения до Azure Data Lake Storage 2-го поколения](../storage/blobs/data-lake-storage-upgrade.md)
- [Преобразование данных с помощью действия Spark в фабрике данных Azure](../data-factory/transform-data-using-spark.md)
- [Преобразование данных с помощью действия Hadoop Hive в фабрике данных Azure](../data-factory/transform-data-using-hadoop-hive.md)
- [Apache Spark в Azure HDInsight](../hdinsight/spark/apache-spark-overview.md)
