---
title: Оптимизация производительности виртуальных машин серии Azure Lsv2 - Хранение
description: Узнайте, как оптимизировать производительность для решения на виртуальных машинах серии Lsv2.
services: virtual-machines-linux
author: laurenhughes
ms.service: virtual-machines-linux
ms-subservice: sizes
ms.topic: article
ms.tgt_pltfrm: vm-linux
ms.workload: infrastructure-services
ms.date: 08/05/2019
ms.author: joelpell
ms.openlocfilehash: 7a0d5e29097bc9a672e142fcffb0ebe879fe2475
ms.sourcegitcommit: 31e9f369e5ff4dd4dda6cf05edf71046b33164d3
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/22/2020
ms.locfileid: "81757691"
---
# <a name="optimize-performance-on-the-lsv2-series-virtual-machines"></a>Оптимизация производительности на виртуальных машинах серии Lsv2

Виртуальные машины серии Lsv2 поддерживают различные рабочие нагрузки, которые требуют высокого ввода-от и пропускной памяти на локальном хранении в широком диапазоне приложений и отраслей.  Серия Lsv2 идеально подходит для баз данных Big Data, S'L, NoS'L, складирования данных и больших транзакционных баз данных, включая Cassandra, MongoDB, Cloudera и Redis.

Конструкция виртуальных машин серии Lsv2 (VMs) максимизирует процессор AMD EPYC™ 7551, чтобы обеспечить лучшую производительность между процессором, памятью, устройствами NVMe и VMs. Работая с партнерами в Linux, несколько сборок доступны Azure Marketplace, которые оптимизированы для производительности серии Lsv2 и в настоящее время включают в себя:

- Ubuntu 18.04
- Ubuntu 16.04
- РХЕ 8,0
- Debian 9
- Debian 10

В этой статье содержатся советы и предложения, которые обеспечивают максимальную производительность, предназначенную для ввоза. Информация на этой странице будет постоянно обновляться по мере добавления в Azure Marketplace большего количества оптимизированных изображений Lsv2.

## <a name="amd-eypc-chipset-architecture"></a>АРХИТЕКТУРа чипсетов AMD EYPC™

VMs-серии Lsv2 используют AMD EYPC™ серверных процессоров на основе микроархитектуры Дзэн. AMD разработала Infinity Fabric (IF) для EYPC™ как масштабируемое соединение для своей модели NUMA, которая может быть использована для связи на уровне смерти, на упаковке и мультипакете. По сравнению с «PI» (быстрый путь interconnect) и UPI (Ultra-Path Interconnect), используемыми на современных монолитных процессорах Intel, архитектура AMD с небольшой смертью может принести как преимущества производительности, так и проблемы. Фактическое влияние ограничений на пропускную способность памяти и задержки может варьироваться в зависимости от типа выполнения рабочих нагрузок.

## <a name="tips-to-maximize-performance"></a>Советы по максимальной производительности

* Если вы загружаете пользовательский Linux GuestOS для рабочей нагрузки, обратите внимание, что ускоренная сеть будет **OFF** по умолчанию. Если вы собираетесь включить ускоренную сеть, включите ее во время создания VM для лучшей производительности.

* Оборудование, которое питает VMs-технологий серии Lsv2, использует устройства NVMe с восемью парами вво/с. Каждая очередь ВВМ-Устройства NVMe на самом деле является парой: очередью представления и очередью завершения. Драйвер NVMe настроен для оптимизации использования этих восьми вхыщных кВ-, распределяя вволок и от входе в круглом графике робина. Чтобы получить максимальную производительность, запустите восемь заданий на устройство, чтобы соответствовать.

* Избегайте смешивания команд NVMe admin (например, запрос информации NVMe SMART и т.д.) с командами NVMe I/O во время активных рабочих нагрузок. Устройства Lsv2 NVMe поддерживаются технологией Hyper-V NVMe Direct, которая переключается в "медленный режим" всякий раз, когда какие-либо команды NVMe-интерфейса находятся на рассмотрении. Пользователи Lsv2 могли видеть резкое падение производительности производительности NVMe I/O, если это произойдет.

* Пользователи Lsv2 не должны полагаться на информацию numA устройства (все 0), представленную из ВМ для дисков данных, чтобы решить сродство NUMA для своих приложений. Рекомендуемый способ повышения производительности — по возможности распределить рабочие нагрузки по процессорам по всем процессорам.

* Максимальная поддерживаемая глубина очереди на пару очередей ВВ2 для устройства Lsv2 VM NVMe составляет 1024 (против лимита Amazon i3 32). Пользователи Lsv2 должны ограничить свои (синтетические) бенчмаркинговые рабочие нагрузки глубиной очереди 1024 или ниже, чтобы избежать запуска полной очереди, что может снизить производительность.

## <a name="utilizing-local-nvme-storage"></a>Использование локального хранилища NVMe

Местное хранение на диске 1.92 TB NVMe на всех ВМ Lsv2 является эфемерным. Во время успешной стандартной перезагрузки VM данные на локальном диске NVMe сохранятся. Данные не будут сохраняться на NVMe, если VM будет передислоцирован, удален или удален. Данные не сохранятся, если другая проблема приведет к тому, что VM или используемое им оборудование станет неработоспособным. Когда это происходит, любые данные о старом узлах надежно стираются.

Будут также случаи, когда VM необходимо перенести в другую принимающую машину, например, во время плановой операции по техническому обслуживанию. Плановые операции по техническому обслуживанию и некоторые аппаратные сбои можно предвидеть с [помощью запланированных событий.](scheduled-events.md) Запланированные события следует использовать для обновления любых прогнозируемых операций по техническому обслуживанию и восстановлению.

В случае, если запланированное событие обслуживания требует воссоздания VM на новом узле с пустыми локальными дисками, данные должны быть ресинхронизированы (опять же, при надежном удалении данных о старом хобе). Это происходит потому, что VMs-м визажает Lv2 серии VMs, которые в настоящее время не поддерживают миграцию в реальном времени на локальном диске NVMe.

Существует два режима планового технического обслуживания.

### <a name="standard-vm-customer-controlled-maintenance"></a>Стандартное обслуживание, контролируемое клиентами VM

- VM перемещается в обновленный хост в течение 30-дневного окна.
- Локальные данные хранения Lv2 могут быть утеряны, поэтому рекомендуется резервное копирование данных до начала мероприятия.

### <a name="automatic-maintenance"></a>Автоматическое техническое обслуживание

- Происходит, если клиент не выполняет обслуживание, контролируемое клиентом, или в случае чрезвычайных процедур, таких как событие нулевого дня безопасности.
- Предназначен для сохранения данных клиентов, но существует небольшой риск замораживания или перезагрузки VM.
- Локальные данные хранения Lv2 могут быть утеряны, поэтому рекомендуется резервное копирование данных до начала мероприятия.

Для любых предстоящих событий службы используйте процесс контролируемого обслуживания, чтобы выбрать наиболее удобное для вас время для обновления. Перед событием вы можете создать резервную копию данных в премиум-хранилище. После завершения события обслуживания вы можете вернуть свои данные в обновленное локальное хранилище Lv2 VMs.

Сценарии, которые поддерживают данные о локальных дисках NVMe, включают:

- VM работает и здоров.
- VM перезагружается на месте (вы или Azure).
- VM приостанавливается (остановлен без де-распределения).
- Большинство запланированных операций по техническому обслуживанию.

Сценарии, которые надежно стирают данные для защиты клиента, включают:

- VM передислоцируется, останавливается (де-выделено) или удаляется (вы).
- VM становится нездоровым и должен обслуживать исцелить другой узла из-за аппаратной проблемы.
- Небольшое число запланированных операций по техническому обслуживанию, которые требуют, чтобы ВМ был перераспределен на другой уместитель для обслуживания.

Чтобы узнать больше о вариантах резервного копирования данных в локальном хранилище, см. [Резервное копирование и аварийное восстановление для дисков Azure IaaS.](backup-and-disaster-recovery-for-azure-iaas-disks.md)

## <a name="frequently-asked-questions"></a>Часто задаваемые вопросы

* **Как начать развертывание VMs-кратных возбуждающих сявков серии?**  
   Как и любой другой VM, используйте [portal,](quick-create-portal.md) [Azure CLI](quick-create-cli.md)или [PowerShell](quick-create-powershell.md) для создания VM.

* **Приведет ли сбой диска NVMe к сбою всех вмвм на хосте?**  
   Если сбой диска обнаружен на аппаратном узло, аппаратное обеспечение находится в неисправном состоянии. Когда это происходит, все ВМ на узлах автоматически девыделяются и перемещаются в здоровый узла. Для VMs-м визажает серия Lsv2, это означает, что данные клиента о сбоенный узла также надежно стерты и должны быть воссозданы клиентом на новом узлах. Как отмечается, до того, как миграция в реальном времени станет доступна на Lsv2, данные о сбойу узла будут активно перемещаться с помощью ВМ по мере их передачи в другой узла.

* **Нужно ли вносить какие-либо коррективы в rq_affinity для производительности?**  
   Установка rq_affinity является незначительной корректировкой при использовании абсолютного максимального объема входных/выходных операций в секунду (IOPS). После того, как все остальное работает хорошо, а затем попытаться установить rq_affinity до 0, чтобы увидеть, если это делает разницу.

* **Нужно ли менять настройки blk_mq?**  
   RHEL/CentOS 7.x автоматически использует blk-mq для устройств NVMe. Никаких изменений конфигурации или настроек не требуется. Настройка scsi_mod.use_blk_mq предназначена только для SCSI и использовалась во время просмотра Lsv2, поскольку устройства NVMe были видны в гостевых ВМ как устройства SCSI. В настоящее время устройства NVMe видны как устройства NVMe, поэтому настройка SCSI blk-mq не имеет значения.

* **Нужно ли менять "фио"?**  
   Чтобы получить максимальный IOPS с инструментом измерения производительности, как "fio" в L64v2 и L80v2 VM размеров, установить "rq_affinity" до 0 на каждом устройстве NVMe.  Например, эта командная строка установит "rq_affinity" до нуля для всех 10 устройств NVMe в L80v2 VM:

   ```console
   for i in `seq 0 9`; do echo 0 >/sys/block/nvme${i}n1/queue/rq_affinity; done
   ```

   Также обратите внимание, что наилучшая производительность получена, когда ввг/о делается непосредственно к каждому из необработанных устройств NVMe без раздела, без файловых систем, без конфигурации RAID 0 и т.д. Перед началом сеанса тестирования убедитесь, что конфигурация `blkdiscard` находится в известном свежем/чистом состоянии, запустив на каждом из устройств NVMe.
   
## <a name="next-steps"></a>Следующие шаги

* Просмотрите спецификации для всех [VMs, оптимизированные для производительности хранилища](sizes-storage.md) в Azure
