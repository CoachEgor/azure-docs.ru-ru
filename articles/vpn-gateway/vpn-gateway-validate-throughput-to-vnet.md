---
title: Проверка пропускной связи VPN в виртуальной сети Microsoft Azure
description: Этот документ помогает пользователю проверить пропускную способность сети от своих локальных ресурсов до виртуальной машины Azure.
titleSuffix: Azure VPN Gateway
services: vpn-gateway
author: cherylmc
ms.service: vpn-gateway
ms.topic: troubleshooting
ms.date: 05/29/2019
ms.author: radwiv
ms.reviewer: chadmat;genli
ms.openlocfilehash: a88e339e82484c2ec1cd2276f6218fa718b990f9
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/27/2020
ms.locfileid: "75860492"
---
# <a name="how-to-validate-vpn-throughput-to-a-virtual-network"></a>Порядок проверки пропускной способности VPN для виртуальной сети

Подключение к VPN-шлюзу позволяет установить защищенное распределенное соединение между виртуальной сетью в Azure и локальной ИТ-инфраструктурой.

В этой статье описано, как проверить пропускную способность сети от локальных ресурсов до виртуальной машины Azure.

> [!NOTE]
> Эта статья помогает диагностировать и устранить распространенные проблемы. Если вам не удается решить проблему с помощью приведенных ниже сведений, [обратитесь в службу поддержки](https://portal.azure.com/?#blade/Microsoft_Azure_Support/HelpAndSupportBlade).

## <a name="overview"></a>Обзор

Подключение к VPN-шлюзу состоит из следующих компонентов:

* На месте VPN устройства (Посмотреть список [проверенных устройств VPN](vpn-gateway-about-vpn-devices.md#devicetable).)
* Общественный интернет
* VPN-шлюз Azure
* Azure

На следующей схеме показано логическое подключение между локальной сетью и виртуальной сетью Azure через VPN.

![Логическое подключение сети клиента к сети MSFT с помощью VPN](./media/vpn-gateway-validate-throughput-to-vnet/VPNPerf.png)

## <a name="calculate-the-maximum-expected-ingressegress"></a>Расчет максимального ожидаемого исходящего и входящего трафика

1. Определите базовые требования приложения к пропускной способности.
1. Определите предел пропускной способности для VPN-шлюза Azure. Для справки, см. [About VPN Gateway](vpn-gateway-about-vpngateways.md#gwsku)
1. Определите [рекомендованную пропускную способность](../virtual-machines/virtual-machines-windows-sizes.md) для используемого размера виртуальной машины Azure.
1. Определите пропускную способность поставщика услуг Интернета (ISP).
1. Рассчитайте ожидаемую пропускную способность, взяв наименьшую пропускную способность либо VM, VPN Gateway, или ISP; который измеряется в Мегабитах в секунду (/) делится на восемь (8).

Если расчетная пропускная способность не соответствует базовым требованиям приложения к пропускной способности, необходимо увеличить пропускную способность ресурса, который вы определили как узкое место. Чтобы изменить размер VPN-шлюза Azure, см. статью [Изменение SKU шлюза](vpn-gateway-about-vpn-gateway-settings.md#gwsku). Чтобы изменить размер виртуальной машины, см. статью [Изменение размера виртуальной машины](../virtual-machines/virtual-machines-windows-resize-vm.md). Если вы не испытываете ожидаемую пропускную способность Интернета, вы также можете связаться с вашим интернет-президентом.

> [!NOTE]
> Пропускная часть VPN Gateway представляет собой совокупность всех соединений от сайта к сайту VNET-to-VNET или "точка к сайту".

## <a name="validate-network-throughput-by-using-performance-tools"></a>Проверка пропускной способности сети с помощью средств повышения производительности

Эту проверку следует выполнять в часы пониженной нагрузки, так как насыщенность пропускной способности туннеля VPN при тестировании не позволяет получить точные результаты.

Для этого теста мы используем средство iPerf, которое работает в Windows и Linux, а также имеет режимы клиента и сервера. Он ограничен 3 Gbps для Windows VMs.

Это средство не выполняет операции чтения и записи на диск. Оно лишь создает самостоятельно сформированный TCP-трафик с одного конца на другой. Он генерирует статистику на основе экспериментов, которые измерят пропускную способность, доступную между узлами клиента и сервера. При тестировании между двумя узлами один узла выступает в качестве сервера, а другой узла — как клиент. После завершения этого теста мы рекомендуем изменить роли узлов для проверки как загрузки, так и загрузки пропускной связи на обоих узлах.

### <a name="download-iperf"></a>Скачивание iPerf

Скачать [iPerf](https://iperf.fr/download/iperf_3.1/iperf-3.1.2-win64.zip). Дополнительные сведения см. в [документации по iPerf](https://iperf.fr/iperf-doc.php).

 > [!NOTE]
 > Сторонние продукты, обсуждаемые в этой статье, производятся компаниями, независимыми от корпорации Майкрософт. Корпорация Майкрософт не дает никаких гарантий, явных или подразумеваемых, относительно производительности или надежности таких продуктов.

### <a name="run-iperf-iperf3exe"></a>Запуск iPerf (iperf3.exe)

1. Включите правило NSG/ACL, разрешающее такой трафик (для тестирования общедоступного IP-адреса на виртуальной машине Azure).

1. На обоих узлах включите исключение брандмауэра для порта 5001.

   **Windows.** Выполните следующую команду от имени администратора:

   ```CMD
   netsh advfirewall firewall add rule name="Open Port 5001" dir=in action=allow protocol=TCP localport=5001
   ```

   Чтобы удалить правило после окончания тестирования, выполните следующую команду:

   ```CMD
   netsh advfirewall firewall delete rule name="Open Port 5001" protocol=TCP localport=5001
   ```

   **Лазурный Linux:** Изображения Azure Linux имеют разрешительные брандмауэры. Когда приложение прослушивает порт, прохождение трафика разрешается. Для защищенных пользовательских образов может потребоваться явно открыть нужные порты. В число распространенных брандмауэров уровня ОС для Linux входят `iptables`, `ufw` и `firewalld`.

1. На узле сервера перейдите в каталог, куда извлекается iperf3.exe. Затем запустите iPerf в режиме сервера и установите его для прослушивания в порту 5001 в качестве следующих команд:

   ```CMD
   cd c:\iperf-3.1.2-win65

   iperf3.exe -s -p 5001
   ```

   > [!Note]
   > Порт 5001 настраивается для учета определенных ограничений брандмауэра в вашей среде.

1. На узле клиента перейдите в каталог, куда извлекается средство iperf, и выполните следующую команду:

   ```CMD
   iperf3.exe -c <IP of the iperf Server> -t 30 -p 5001 -P 32
   ```

   Клиент направляет тридцать секунд трафика на порт 5001, на сервер. Флаг '-P ' указывает на то, что мы делаем 32 одновременных подключения к серверу.

   Ниже представлены выходные данные для этого примера:

   ![Выходные данные](./media/vpn-gateway-validate-throughput-to-vnet/06theoutput.png)

1. (НЕОБЯЗАТЕЛЬНО) Чтобы сохранить результаты тестирования, выполните следующую команду:

   ```CMD
   iperf3.exe -c IPofTheServerToReach -t 30 -p 5001 -P 32  >> output.txt
   ```

1. После завершения предыдущих этапов выполните те же шаги с обратными ролями, так что узла сервера теперь будет узловом клиента, и наоборот.

> [!Note]
> Iperf не единственный инструмент. [NTTTCP является альтернативным решением для тестирования.](https://docs.microsoft.com/azure/virtual-network/virtual-network-bandwidth-testing)

## <a name="test-vms-running-windows"></a>Тестовых VMs под управлением Windows

### <a name="load-latteexe-onto-the-vms"></a>Нагрузка Latte.exe на VMs

Скачать последнюю версию [Latte.exe](https://gallery.technet.microsoft.com/Latte-The-Windows-tool-for-ac33093b)

Рассмотрим возможность поместить Latte.exe в отдельную папку, например,`c:\tools`

### <a name="allow-latteexe-through-the-windows-firewall"></a>Разрешить Latte.exe через брандмауэр Windows

На приемнике создайте правило «Разрешить» на брандмауэре Windows, чтобы можно было прибыть трафик latte.exe. Проще всего разрешить всю программу Latte.exe по имени, а не разрешать входящие порты TCP.

### <a name="allow-latteexe-through-the-windows-firewall-like-this"></a>Разрешить Latte.exe через брандмауэр Windows, как это

`netsh advfirewall firewall add rule program=<PATH>\latte.exe name="Latte" protocol=any dir=in action=allow enable=yes profile=ANY`

Например, если вы скопировали latte.exe в папку "c:'tools", это будет команда

`netsh advfirewall firewall add rule program=c:\tools\latte.exe name="Latte" protocol=any dir=in action=allow enable=yes profile=ANY`

### <a name="run-latency-tests"></a>Выполнить тесты задержки

Начало latte.exe на RECEIVER (бег от CMD, а не от PowerShell):

`latte -a <Receiver IP address>:<port> -i <iterations>`

Около 65k итераций достаточно долго, чтобы вернуть репрезентативные результаты.

Любой доступный номер порта в порядке.

Если У VM есть IP-адрес 10.0.0.4, это будет выглядеть следующим образом

`latte -c -a 10.0.0.4:5005 -i 65100`

Запустите latte.exe на SENDER (бег от CMD, а не от PowerShell)

`latte -c -a <Receiver IP address>:<port> -i <iterations>`

Полученная команда такая же, как на приемнике, за исключением добавления "-c", чтобы указать, что это "клиент" или отправитель

`latte -c -a 10.0.0.4:5005 -i 65100`

Дождитесь результатов. В зависимости от того, как далеко друг от друга VMs, это может занять несколько минут, чтобы завершить. Рассмотрите возможность начать с меньшего количества итераций для проверки на успех перед проведением более длительных тестов.

## <a name="test-vms-running-linux"></a>Тестовые VMs под управлением Linux

Используйте [SockPerf](https://github.com/mellanox/sockperf) для тестирования VMs.

### <a name="install-sockperf-on-the-vms"></a>Установка SockPerf на vMs

На Linux VMs (как SENDER, так и RECEIVER) запустите эти команды для подготовки SockPerf на ваших VM:

#### <a name="centos--rhel---install-git-and-other-helpful-tools"></a>CentOS / RHEL - Установка GIT и других полезных инструментов

`sudo yum install gcc -y -q`
`sudo yum install git -y -q`
`sudo yum install gcc-c++ -y`
`sudo yum install ncurses-devel -y`
`sudo yum install -y automake`

#### <a name="ubuntu---install-git-and-other-helpful-tools"></a>Ubuntu - Установка GIT и других полезных инструментов

`sudo apt-get install build-essential -y`
`sudo apt-get install git -y -q`
`sudo apt-get install -y autotools-dev`
`sudo apt-get install -y automake`

#### <a name="bash---all"></a>Bash - все

От командной строки Bash (предполагает, что git установлен)

`git clone https://github.com/mellanox/sockperf`
`cd sockperf/`
`./autogen.sh`
`./configure --prefix=`

Сделать медленнее, может занять несколько минут

`make`

Сделать установку быстро

`sudo make install`

### <a name="run-sockperf-on-the-vms"></a>Выполнить SockPerf на VMs

#### <a name="sample-commands-after-installation-serverreceiver---assumes-servers-ip-is-10004"></a>Примеры команд после установки. Сервер/приемник - предполагает, что IP сервера составляет 10.0.0.4

`sudo sockperf sr --tcp -i 10.0.0.4 -p 12345 --full-rtt`

#### <a name="client---assumes-servers-ip-is-10004"></a>Клиент - предполагает, что IP сервера составляет 10.0.0.4

`sockperf ping-pong -i 10.0.0.4 --tcp -m 1400 -t 101 -p 12345  --full-rtt`

> [!Note]
> Убедитесь, что во время тестирования пропускной записи между VM и Шлюзом нет промежуточного хмеля (например, виртуального прибора).
> Если есть плохие результаты (с точки зрения общей пропускной их выгоды), исходя из iPERF / NTTTCP испытаний выше, пожалуйста, обратитесь к следующей статье, чтобы понять ключевые факторы, лежащие в основе возможных коренных причин проблемы:https://docs.microsoft.com/azure/virtual-network/virtual-network-tcpip-performance-tuning

В частности, анализ следов захвата пакетов (Wireshark/Network Monitor), собранных параллельно с клиентом и сервером во время этих тестов, поможет в оценке плохой производительности. Эти следы могут включать потерю пакета, высокую задержку, размер MTU. фрагментация, TCP 0 Окно, Из порядка фрагменты, и так далее.

## <a name="address-slow-file-copy-issues"></a>Решение проблем с низкой скоростью при копировании файлов

Даже если общая пропускная их часть, оцениваемая с предыдущими шагами (iPERF/NTTTCP/etc..). была хорошей, вы можете испытывать медленное копирование файлов при использовании Windows Explorer или перетаскивания и падения через сеанс RDP. Обычно эта проблема вызвана одним или обоими следующими факторами:

* Приложения для копирования файлов, такие как проводник и RDP, не используют несколько потоков при копировании. Для повышения производительности используйте многопоточное приложение, например [Richcopy](https://technet.microsoft.com/magazine/2009.04.utilityspotlight.aspx), которое копирует файлы с помощью 16 или 32 потоков. Чтобы изменить номер потока для копии файла в Richcopy, нажмите **Action** > **Copy параметры** > **файла копию**.

   ![Проблемы с низкой скоростью при копировании файлов](./media/vpn-gateway-validate-throughput-to-vnet/Richcopy.png)<br>

   > [!Note]
   > Не все приложения работают одинаково, и не все приложения/процесс использует все потоки. При запуске теста можно увидеть, что некоторые потоки пусты и не обеспечивают точных результатов пропускной работы.
   > Чтобы проверить производительность передачи файла приложения, используйте многопоточные, увеличивая поток подряд или уменьшая, чтобы найти оптимальную пропускную способность приложения или передачи файлов.

* Недостаточная скорость чтения и записи виртуальной машины. Дополнительные сведения см. в статье [Устранение неполадок службы хранилища Azure](../storage/common/storage-e2e-troubleshooting.md).

## <a name="on-premises-device-external-facing-interface"></a>Внешний интерфейс для локального устройства

Упомянутые подсети локальных диапазонов, которые вы хотели бы, чтобы Azure достиг ажиотаж через VPN на локальном сетевом шлюзе. Одновременно определите адресное пространство VNET в Azure на локальным устройством.

* **Шлюз на основе маршрута**: Политика или селектор трафика для VPN на основе маршрутов настроены как любые-к-любым (или дикие карты).

* Портал на **основе политики**: VPN на основе политики шифруют и направляют пакеты через туннели IPsec на основе комбинаций адресных префиксов между вашей предварительной сетью и Azure VNet. Политика (или селектор трафика) обычно определяется как список доступа в конфигурации VPN.

* **UsePolicyBasedTrafficSelector** соединения: ("UsePolicyBasedTrafficTrafficSelectors" для $True на соединение будет настроить Azure VPN шлюз для подключения к политике на основе VPN брандмауэра на территории. Если вы включите PolicyBasedTrafficSelectors, вам необходимо убедиться, что ваше VPN-устройство имеет соответствующие селекторы трафика, определенные со всеми комбинациями локальной сети (локальный сетевой шлюз) префиксы к и из префиксов виртуальной сети Azure, а не любой к любому.

Ненадлежащая конфигурация может привести к частым отключениям в туннеле, перепадениям пакетов, плохой пропускной связи и задержке.

## <a name="check-latency"></a>Проверка задержки

Вы можете проверить задержку с помощью следующих инструментов:

* WinMTR
* TCPTraceroute
* `ping`и `psping` (Эти инструменты могут обеспечить хорошую оценку RTT, но они не могут быть использованы во всех случаях.)

![Проверка задержки](./media/vpn-gateway-validate-throughput-to-vnet/08checkinglatency.png)

Если вы заметили высокий всплеск задержки на любом из хмеля перед вводом MS Network позвоночника, вы можете продолжить дальнейшие исследования с вашим поставщиком услуг Интернета.

Если большой, необычный всплеск задержки замечен из хмеля в пределах "msn.net", пожалуйста, свяжитесь со поддержкой MS для дальнейших исследований.

## <a name="next-steps"></a>Дальнейшие действия

Для получения дополнительной информации или справки, ознакомьтесь со следующей ссылкой:

* [Служба технической поддержки Майкрософт](https://portal.azure.com/?#blade/Microsoft_Azure_Support/HelpAndSupportBlade)
