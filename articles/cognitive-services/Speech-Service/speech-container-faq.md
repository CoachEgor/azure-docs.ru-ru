---
title: Контейнеры для речевых услуг часто задают вопросы (ВСЗ)
titleSuffix: Azure Cognitive Services
description: Установка и запуск речевых контейнеров. от речи к тексту транскрибирует аудиопотоки в текст в режиме реального времени, которые ваши приложения, инструменты или устройства могут потреблять или отображать. Преобразование текста в речь преобразует вводимый текст в синтезированную речь, похожую на человеческую.
services: cognitive-services
author: aahill
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 04/14/2020
ms.author: aahi
ms.openlocfilehash: 17582244aef173da6ac700c980f7bd7fb0fec307
ms.sourcegitcommit: ea006cd8e62888271b2601d5ed4ec78fb40e8427
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/14/2020
ms.locfileid: "81383086"
---
# <a name="speech-service-containers-frequently-asked-questions-faq"></a>Контейнеры для речевых услуг часто задают вопросы (ВСЗ)

При использовании службы речи с контейнерами, полагаться на эту коллекцию часто задаваемые вопросы, прежде чем эскалации для поддержки. В этой статье рассматриваются вопросы различной степени, от общего до технического. Чтобы расширить ответ, нажмите на вопрос.

## <a name="general-questions"></a>Общие вопросы

<details>
<summary>
<b>Как работают контейнеры речевой речи и как их настроить?</b>
</summary>

**Ответ:** При настройке производственного кластера необходимо учитывать несколько вещей. Во-первых, настройка одного языка, нескольких контейнеров на одной машине не должна быть большой проблемой. Если у вас возникли проблемы, это может быть проблема, связанная с оборудованием - так что мы бы сначала посмотреть на ресурс, то есть; Спецификации процессора и памяти.

Рассмотрим на мгновение, `ja-JP` контейнер и последнюю модель. Акустическая модель является наиболее требовательной частью процессора-мудрым, в то время как языковая модель требует большей части памяти. Когда мы сравнивали использование, требуется около 0,6 процессора ядер для обработки одного запроса речи к тексту, когда звук течет в режиме реального времени (например, с микрофона). Если вы кормите аудио быстрее, чем в режиме реального времени (например, из файла), что использование может удвоиться (1,2x ядра). Между тем, память, перечисленные ниже, работает память для расшифровки речи. При этом *не* учитывается фактический полный размер языковой модели, которая будет проживать в кэше файлов. Для `ja-JP` этого дополнительные 2 ГБ; для, `en-US`это может быть больше (6-7 ГБ).

Если у вас есть машина, где память не хватает, и вы пытаетесь развернуть несколько языков на нем, вполне возможно, что кэш файлов полон, и ОС вынуждена страницы моделей и выход. Для запущенной транскрипции это может привести к катастрофическим последствиям и другим последствиям для производительности.

Кроме того, мы предварительно упаковываем исполнители для машин с расширенным набором инструкций [по наращиванию векторных знаний (AVX2).](speech-container-howto.md#advanced-vector-extension-support) Машина с набором инструкций AVX512 потребует генерации кода для этой цели, и запуск 10 контейнеров для 10 языков может временно исчерпать процессор. Сообщение, подобное этому, будет отображаться в журналах докеров:

```console
2020-01-16 16:46:54.981118943 
[W:onnxruntime:Default, tvm_utils.cc:276 LoadTVMPackedFuncFromCache]
Cannot find Scan4_llvm__mcpu_skylake_avx512 in cache, using JIT...
```

Наконец, можно установить количество декодеров, *single* которые `DECODER MAX_COUNT` вы хотите внутри одного контейнера, используя переменную. Таким образом, в основном, мы должны начать с вашего SKU (CPU / память), и мы можем предложить, как получить лучшее из него. Отличной отправной точкой является ссылка на рекомендуемые спецификации ресурсов хоста.

<br>
</details>

<details>
<summary>
<b>Не могли бы вы помочь с планированием емкости и оценкой затрат контейнеров on-prem Speech?</b>
</summary>

**Ответ:** Для емкости контейнера в режиме обработки batch, каждый декодер может обрабатывать 2-3x в режиме реального времени, с двумя ядрами процессора, для одного распознавания. Мы не рекомендуем хранить более двух одновременных признаний в экземпляре контейнера, но рекомендуем запускать больше экземпляров контейнеров по причинам надежности/доступности, за балансером нагрузки.

Хотя мы могли бы каждый экземпляр контейнера работает с более декодеров. Например, мы можем настроить 7 декодеров на экземпляр контейнера на восьми основных машинах (более чем в 2 раз каждый), принося пропускную стоимость 15x. Существует `DECODER_MAX_COUNT` пара, чтобы быть в курсе. В крайнем случае возникают проблемы с надежностью и задержкой, при этом пропускная высозание значительно увеличивается. Для микрофона, это будет в 1x реального времени. Общее использование должно быть примерно на одном ядре для одного распознавания.

Для сценария обработки 1 K часов в день в режиме обработки пакетов, в крайнем случае, 3 ВМ может обрабатывать его в течение 24 часов, но не гарантируется. Для обработки дней спайков, сбоев, обновления и обеспечения минимального резервного копирования/BCP мы рекомендуем 4-5 машин вместо 3 на кластер и с кластерами 2 ".

Для оборудования мы используем стандартный `DS13_v2` Azure VM в качестве эталона (каждое ядро должно быть 2,6 ГГц или лучше, с включенным набором инструкций AVX2).

| Экземпляр  | vCPU (ы) | ОЗУ    | Температура хранения | Оплата по мере того как-вы-госили с AHB | 1-летний резерв с AHB (% Экономия) | 3-летний зарезервирован с AHB (% Экономия) |
|-----------|---------|--------|--------------|------------------------|-------------------------------------|--------------------------------------|
| `DS13 v2` | 8       | 56 GiB | 112 ГиБ      | $0.598/час            | $0.3528/час (41%)                 | $0.2333/час (61%)                  |

На основе разработки ссылки (два кластера из 5 ВМ для обработки 1 K часов / день аудио пакетобработки), 1-летний аппаратных затрат будет:

> 2 (кластеры) - 5 (ВМ на кластер) - $0,3528/час - 365 (дней) - 24 (часы) - $31Тыс / год

При отображении физической машины общая оценка составляет 1 vCPU 1 Физическое ядро процессора. В действительности, 1vCPU является более мощным, чем одно ядро.

Для на-prem, все эти дополнительные факторы приходят в игру:

- На каком типе физический процессор и сколько ядер на нем
- Сколько процессоров работает вместе на одной коробке/машине
- Как настраиваются vM
- Как используется гипер-резьба / многопоточные
- Как обменивается память
- ОС и т.д.

Обычно это не так хорошо настроено, как окружающая среда Azure. Учитывая другие накладные расходы, я бы сказал, безопасная оценка 10 физических ядер процессора 8 Azure vCPU. Хотя популярные процессоры имеют только восемь ядер. При развертывании на прем едят затраты будут выше, чем при использовании VMs-иносов Azure. Кроме того, рассмотрим амортизацию.

Стоимость обслуживания такая же, как и онлайн-сервис: $1/час за речевой текст. Стоимость услуги речи:

> 1 $ 1000 и 365 $ 365K

Стоимость обслуживания, уплачиваемая корпорации Майкрософт, зависит от уровня обслуживания и содержания службы. Это различные от $ 29,99 / месяц для базового уровня до сотен тысяч, если на месте службы участие. Грубое число составляет $ 300/hour для обслуживания / обслуживания. Стоимость людей не включена. Другие затраты на инфраструктуру (такие как хранение, сети и баланселимы нагрузки) не включены.

<br>
</details>

<details>
<summary>
<b>Почему пунктуация отсутствует в транскрипции?</b>
</summary>

**Ответ:** Следует `speech_recognition_language=<YOUR_LANGUAGE>` явно настроить запрос, если они используют клиента Carbon.

Пример:

```python
if not recognize_once(
    speechsdk.SpeechRecognizer(
        speech_config=speechsdk.SpeechConfig(
            endpoint=template.format("interactive"),
            speech_recognition_language="ja-JP"),
            audio_config=audio_config)):

    print("Failed interactive endpoint")
    exit(1)
```
Результат выглядит так:

```cmd
RECOGNIZED: SpeechRecognitionResult(
    result_id=2111117c8700404a84f521b7b805c4e7, 
    text="まだ早いまだ早いは猫である名前はまだないどこで生まれたかとんと見当を検討をなつかぬ。
    何でも薄暗いじめじめした所でながら泣いていた事だけは記憶している。
    まだは今ここで初めて人間と言うものを見た。
    しかも後で聞くと、それは書生という人間中で一番同額同額。",
    reason=ResultReason.RecognizedSpeech)
```

<br>
</details>

<details>
<summary>
<b>Могу ли я использовать пользовательскую акустическую модель и языковую модель с контейнером Speech?</b>
</summary>

В настоящее время мы можем сдать только один идентификатор модели, либо пользовательскую языковую модель, либо пользовательскую акустическую модель.

**Ответ:** Было принято решение *не* поддерживать одновременно как акустические, так и языковые модели. Это будет действовать до тех пор, пока не будет создан единый идентификатор для уменьшения перерывов API. Так что, к сожалению, это не поддерживается прямо сейчас.

<br>
</details>

<details>
<summary>
<b>Не могли бы вы объяснить эти ошибки из пользовательского речевого контейнера?</b>
</summary>

**Ошибка 1:**

```cmd
Failed to fetch manifest: Status: 400 Bad Request Body:
{
    "code": "InvalidModel",
    "message": "The specified model is not supported for endpoint manifests."
}
```

**Ответ 1:** Если вы тренируетесь с последней пользовательской моделью, мы в настоящее время не поддерживаем это. Если вы тренируетесь со старой версией, она должна быть возможность использования. Мы все еще работаем над поддержкой последних версий.

По существу, пользовательские контейнеры не поддерживают акустические модели на основе Halide или ONNX (что по умолчанию находится на пользовательском портале обучения). Это связано с тем, что пользовательские модели не зашифрованы, и мы не хотим, чтобы разоблачить модели ONNX, однако; языковые модели в порядке. Клиент должен будет явно выбрать старую модель non-ONNX для пользовательского обучения. Точность не будет затронута. Размер модели может быть больше (на 100 МБ).

> Модель поддержки > 20190220 (v4.5 Unified)

**Ошибка 2:**

```cmd
HTTPAPI result code = HTTPAPI_OK.
HTTP status code = 400.
Reason:  Synthesis failed.
StatusCode: InvalidArgument,
Details: Voice does not match.
```

**Ответ 2:** Вы должны указывать правильное имя голоса в запросе, который является чувствительным к делу. Обратитесь к полному отображению имени службы. Вы должны `en-US-JessaRUS`использовать, `en-US-JessaNeural` как это не доступно прямо сейчас в контейнерной версии текста к речи.

**Ошибка 3:**

```json
{
    "code": "InvalidProductId",
    "message": "The subscription SKU \"CognitiveServices.S0\" is not supported in this service instance."
}
```

**Ответ 3:** Тмыка для создания ресурса речи, а не ресурса Когнитивных Служб.


<br>
</details>

<details>
<summary>
<b>Какие протоколы API поддерживаются, REST или WS?</b>
</summary>

**Ответ:** Для речевых и пользовательских контейнеров от речи к тексту, мы в настоящее время только поддержка websocket на основе протокола. SDK поддерживает только вызов в WS, но не REST. Есть план, чтобы добавить поддержку REST, но не ETA на данный момент. Всегда ссылайтесь на [query prediction endpoints](speech-container-howto.md#query-the-containers-prediction-endpoint)официальную документацию, см.

<br>
</details>

<details>
<summary>
<b>Поддерживается ли CentOS для контейнеров с речевыми речевыми речевыми?</b>
</summary>

**Ответ:** CentOS 7 пока не поддерживается Python SDK, также Ubuntu 19.04 не поддерживается.

Пакет SDK службы "Речь" для Python доступен для таких операционных систем:
- **Окна** - x64 и x86
- **Mac** - macOS X версия 10.12 или позже
- **Linux** - Ubuntu 16.04, Ubuntu 18.04, Debian 9 на x64

Для получения дополнительной информации [Python platform setup](quickstarts/setup-platform.md?pivots=programming-language-python)о настройке среды см. На данный момент, Ubuntu 18.04 является рекомендуемой версией.

<br>
</details>

<details>
<summary>
<b>Почему я получаю ошибки при попытке вызова конечных точек прогнозирования LUIS?</b>
</summary>

Я использую контейнер LUIS в развертывании IoT Edge и пытаюсь вызвать конечную точку прогнозирования LUIS из другого контейнера. Контейнер LUIS слушает сяочим на порту 5001, и URL я использую это:

```csharp
var luisEndpoint =
    $"ws://192.168.1.91:5001/luis/prediction/v3.0/apps/{luisAppId}/slots/production/predict";
var config = SpeechConfig.FromEndpoint(new Uri(luisEndpoint));
```

Ошибка, которую я получаю:

```cmd
WebSocket Upgrade failed with HTTP status code: 404 SessionId: 3cfe2509ef4e49919e594abf639ccfeb
```

Я вижу запрос в журналах контейнеров LUIS и сообщение говорит:

```cmd
The request path /luis//predict" does not match a supported file type.
```

Что это означает? Чего мне не хватает? Я следил пример для речи SDK, [отсюда](https://github.com/Azure-Samples/cognitive-services-speech-sdk). Сценарий заключается в том, что мы обнаруживаем звук непосредственно из микрофона ПК и пытаемся определить намерения, на основе приложения LUIS, которое мы обучили. Пример, с которым я связан, делает именно это. И он хорошо работает с облачным сервисом LUIS. Использование речи SDK, казалось, спасло нас от необходимости делать отдельный явный вызов API от речи к тексту, а затем второй звонок в LUIS.

Итак, все, что я пытаюсь сделать, это перейти от сценария использования LUIS в облаке с использованием контейнера LUIS. Я не могу себе представить, если речь SDK работает для одного, он не будет работать для других.

**Ответ:** Речь SDK не должна использоваться против контейнера LUIS. Для использования контейнера LUIS следует использовать API LUIS SDK или LUIS REST. Речь SDK должна быть использована против речевого контейнера.

Облако отличается от контейнера. Облако может состоять из нескольких агрегированных контейнеров (иногда называемых микрослужбами). Так что есть контейнер LUIS, а затем есть речь контейнер - два отдельных контейнеров. Контейнер речи делает только речь. Контейнер LUIS делает только LUIS. В облаке, поскольку оба контейнера, как известно, развернуты, и это плохая производительность для удаленного клиента, чтобы перейти к облаку, делать речь, вернуться, затем перейти к облаку снова и сделать LUIS, мы предоставляем функцию, которая позволяет клиенту перейти к речи, остаться в облаке, перейдите к LUIS затем вернуться к клиенту. Таким образом, даже в этом сценарии Speech SDK переходит в облачный контейнер Speech с аудио, а затем облачный контейнер Speech разговаривает с облачным контейнером LUIS с текстом. Контейнер LUIS не имеет понятия приема аудио (это не имеет смысла для контейнера LUIS принимать потоковое аудио - LUIS является текстовым сервисом). С on-prem, мы не имеем никакой определенности наш клиент развернул оба контейнера, мы не предполагаем оркестровать между контейнерами в предпосылках наших клиентов, и если оба контейнера развернуты on-prem, то, котор дали они более местны к клиенту, то нет тяготения пойти SR сперва, назад к клиенту, и иметь клиента после этого принять тот текст и пойти к LUIS.

<br>
</details>

<details>
<summary>
<b>Почему мы получаем ошибки с macOS, речевой контейнер и Python SDK?</b>
</summary>

Когда мы отправляем файл *.wav* для расшифровки, результат возвращается с:

```cmd
recognition is running....
Speech Recognition canceled: CancellationReason.Error
Error details: Timeout: no recognition result received.
When creating a websocket connection from the browser a test, we get:
wb = new WebSocket("ws://localhost:5000/speech/recognition/dictation/cognitiveservices/v1")
WebSocket
{
    url: "ws://localhost:5000/speech/recognition/dictation/cognitiveservices/v1",
    readyState: 0,
    bufferedAmount: 0,
    onopen: null,
    onerror: null,
    ...
}
```

Мы знаем, что websocket настроен правильно.

**Ответ:** Если это так, то смотрите [этот вопрос GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/310). У нас есть обходной путь, [предложенный здесь](https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/310#issuecomment-527542722).

Углерод зафиксировал это в версии 1.8.


<br>
</details>

<details>
<summary>
<b>Каковы различия в конечных точках контейнера Speech?</b>
</summary>

Не могли бы вы помочь заполнить следующие тестовые метрики, в том числе какие функции для тестирования, и как протестировать SDK и REST AIS? Особенно, различия в "интерактивный" и "разговор", который я не видел из существующих док / образец.

| Конечная точка                                                | Функциональный тест                                                   | SDK | REST API |
|---------------------------------------------------------|-------------------------------------------------------------------|-----|----------|
| `/speech/synthesize/cognitiveservices/v1`               | Синтезировать текст (текст к речи)                                  |     | Да      |
| `/speech/recognition/dictation/cognitiveservices/v1`    | Когнитивные услуги на prem диктовки v1 websocket конечная точка        | Да | нет       |
| `/speech/recognition/interactive/cognitiveservices/v1`  | Когнитивные услуги на prem интерактивные v1 websocket конечная точка  |     |          |
| `/speech/recognition/conversation/cognitiveservices/v1` | Когнитивные услуги на прем разговор v1 websocket конечная точка |     |          |

**Ответ:** Это слияние:
- Люди пытаются диктовки конечной точки для контейнеров, (я не уверен, как они получили, что URL)
- Конечная точка<sup>1-й</sup> партии, наминая в контейнере.
- <sup>1-я конечная</sup> точка партии возвращает speech.fragment сообщения вместо `speech.hypothesis` сообщений<sup>3-й</sup> части конечных точек возвращения для конечной точки диктовки.
- Углеродные quickstarts `RecognizeOnce` все использовать (интерактивный режим)
- Углерод, имеющий `speech.fragment` утверждение, что для сообщений, требующих, они не возвращаются в интерактивном режиме.
- Углерод имея утверждает пожар в строениях отпуска (убивая процесс).

Обходной путь либо переключается на непрерывное распознавание в коде, либо (быстрее) подключаться к интерактивным или непрерывным конечным точкам в контейнере.
Для кода установите конечную точку <хосте: порт>/речи/признании/интерактивный/когнитивный сервис/v1

Для различных режимов, см.

[!INCLUDE [speech-modes](includes/speech-modes.md)]

Надлежащее исправление идет с SDK 1.8, который имеет поддержку на prem (выберет правую конечную точку, поэтому мы будем не хуже, чем онлайн-сервис). В то же время, есть пример для непрерывного признания, почему бы нам не указать на это?

https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/6805d96bf69d9e95c9137fe129bc5d81e35f6309/samples/python/console/speech_sample.py#L196

<br>
</details>

<details>
<summary>
<b>Какой режим я должен использовать для различных аудиофайлов?</b>
</summary>

**Ответ:** Вот [быстрый старт с помощью Python](quickstarts/speech-to-text-from-microphone.md?pivots=programming-language-python). Вы можете найти другие языки, связанные на сайте документов.

Просто чтобы прояснить для интерактивного, разговор, и диктовку; это продвинутый способ определения конкретного способа обработки запроса речи. К сожалению, для контейнеров на прем мы должны указать полный URI (так как она включает в себя местную машину), так что эта информация просочилась из абстракции. Мы работаем с командой SDK, чтобы сделать его более пригодным для удовечий в будущем.

<br>
</details>

<details>
<summary>
<b>Как мы можем сравнивать грубую меру транзакций/второй/основной?</b>
</summary>

**Ответ:** Вот некоторые из грубых чисел ожидать от существующей модели (будет меняться в лучшую сторону в том, что мы будем поставлять в ГА):

- Для файлов, регулирование будет в речи SDK, в 2x. Первые пять секунд звука не задушены. Декодер способен делать около 3x реального времени. Для этого общее использование процессора будет близко к 2 ядрам для одного распознавания.
- Для микрофона, это будет в 1раз в реальном времени. Общее использование должно быть около 1 ядра для одного распознавания.

Все это может быть проверено из журналов докеров. Мы на самом деле свалки линии с сессии и фразы / высказывания статистики, и что включает в себя номера RTF.


<br>
</details>

<details>
<summary>
<b>Часто ли можно разделить аудиофайлы на патроны для использования контейнера speech?</b>
</summary>

Мой текущий план состоит в том, чтобы взять существующий аудио файл и разделить его на 10 секунд куски и отправить их через контейнер. Это приемлемый сценарий?  Есть ли лучший способ обработки больших аудиофайлов с контейнером?

**Ответ:** Просто используйте речь SDK и дать ему файл, он будет делать правильные вещи. Зачем нужно отсылкать файл?


<br>
</details>

<details>
<summary>
<b>Как заставить несколько контейнеров работать на одном и том же узлах?</b>
</summary>

Документ говорит, чтобы разоблачить другой порт, который я делаю, но контейнер LUIS все еще слушает на порту 5000?

**Ответ:** Попробуйте `-p <outside_unique_port>:5000`. Например, `-p 5001:5000`.


<br>
</details>

## <a name="technical-questions"></a>Технические вопросы

<details>
<summary>
<b>Как я могу получить непакетные &lt;AIS для обработки аудио длиной 15 секунд?</b>
</summary>

**Ответ:** `RecognizeOnce()` в интерактивном режиме обрабатывает только до 15 секунд аудио, так как режим предназначен для командовании речи, где высказывания, как ожидается, будут короткими. Если вы `StartContinuousRecognition()` используете для диктовки или разговора, нет 15 второй предел.


<br>
</details>

<details>
<summary>
<b>Каковы рекомендуемые ресурсы, процессор и оперативная память; для 50 одновременных запросов?</b>
</summary>

Сколько одновременных запросов будет 4 основных, 4 ГБ оперативной памяти? Если мы должны служить, например, 50 одновременных запросов, сколько Core и оперативной памяти рекомендуется?

**Ответ:** В режиме реального `en-US`времени, 8 с нашими последними , поэтому мы рекомендуем использовать больше докер контейнеров за 6 одновременных запросов. Он становится сумасшедшим за 16 ядер, и он становится неравномерным доступ памяти (NUMA) узла чувствительны. В следующей таблице описывается минимальное и рекомендуемое распределение ресурсов для каждого контейнера речи.

# <a name="speech-to-text"></a>[Преобразование речи в текст](#tab/stt)

| Контейнер      | Минимальные             | Рекомендуемая         |
|----------------|---------------------|---------------------|
| Преобразование речи в текст | 2 ядра, 2-ГБ памяти | 4 ядра, 4-ГБ памяти |

# <a name="custom-speech-to-text"></a>[Пользовательские речи к тексту](#tab/cstt)

| Контейнер             | Минимальные             | Рекомендуемая         |
|-----------------------|---------------------|---------------------|
| Пользовательские речи к тексту | 2 ядра, 2-ГБ памяти | 4 ядра, 4-ГБ памяти |

# <a name="text-to-speech"></a>[Преобразование текста в речь](#tab/tts)

| Контейнер      | Минимальные             | Рекомендуемая         |
|----------------|---------------------|---------------------|
| Преобразование текста в речь | 1 ядро, 2-ГБ памяти | 2 ядра, 3-ГБ памяти |

# <a name="custom-text-to-speech"></a>[Пользовательский текст к речи](#tab/ctts)

| Контейнер             | Минимальные             | Рекомендуемая         |
|-----------------------|---------------------|---------------------|
| Пользовательский текст к речи | 1 ядро, 2-ГБ памяти | 2 ядра, 3-ГБ памяти |

***

- Каждое ядро должно быть не менее 2,6 ГГц или быстрее.
- Для файлов, регулирование будет в речи SDK, в 2x (первые 5 секунд аудио не задушат).
- Декодер способен делать около 2-3x в режиме реального времени. Для этого общее использование процессора будет близко к двум ядрам для одного распознавания. Вот почему мы не рекомендуем держать более двух активных соединений, в экземпляре контейнера. Крайняя сторона будет поставить около 10 декодеров в 2раза `DS13_V2`реального времени в восемь основных машин, как . Для контейнерной версии 1.3 и позже, есть `DECODER_MAX_COUNT=20`пара, который вы можете попробовать настройки.
- Для микрофона, это будет в 1x реального времени. Общее использование должно быть примерно на одном ядре для одного распознавания.

Рассмотрим общее количество часов аудио у вас есть. Если число большое, для повышения надежности/доступности, мы предлагаем запустить больше экземпляров контейнеров, либо на одной коробке или на нескольких коробках, за балансером нагрузки. Оркестрация может быть сделана с помощью Kubernetes (K8S) и Helm, или с Docker сочинять.

Например, для обработки 1000 часов/24 часов мы попытались настроить 3-4 ВМ с 10 экземплярами/декодерами на ВМ.

<br>
</details>

<details>
<summary>
<b>Поддерживает ли контейнер speech пунктуацию?</b>
</summary>

**Ответ:** У нас есть капитализация (ITN) доступна в контейнере on-prem. Пунктуация зависит от языка и не поддерживается для некоторых языков, включая китайский и японский.

У *do* нас есть неявная и базовая поддержка знаков `off` препинания для существующих контейнеров, но это по умолчанию. Это означает, что вы `.` можете получить персонажа в `。` своем примере, но не персонажа. Для обеспечения этой неявной логики, вот пример того, как это сделать в Python с помощью нашей речи SDK (это было бы аналогично в других языках):

```python
speech_config.set_service_property(
    name='punctuation',
    value='implicit',
    channel=speechsdk.ServicePropertyChannel.UriQueryParameter
)
```

<br>
</details>

<details>
<summary>
<b>Почему я получаю 404 ошибки при попытке POST данных в речевой контейнер?</b>
</summary>

Вот пример HTTP POST:

```http
POST /speech/recognition/conversation/cognitiveservices/v1?language=en-US&format=detailed HTTP/1.1
Accept: application/json;text/xml
Content-Type: audio/wav; codecs=audio/pcm; samplerate=16000
Transfer-Encoding: chunked
User-Agent: PostmanRuntime/7.18.0
Cache-Control: no-cache
Postman-Token: xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
Host: 10.0.75.2:5000
Accept-Encoding: gzip, deflate
Content-Length: 360044
Connection: keep-alive
HTTP/1.1 404 Not Found
Date: Tue, 22 Oct 2019 15:42:56 GMT
Server: Kestrel
Content-Length: 0
```

**Ответ:** Мы не поддерживаем REST API ни в одном из речевых контейнеров, мы поддерживаем WebSockets только через Speech SDK. Всегда ссылайтесь на [query prediction endpoints](speech-container-howto.md#query-the-containers-prediction-endpoint)официальную документацию, см.

<br>
</details>

<details>
<summary>
<b>Почему при использовании службы речевого текста я получаю эту ошибку?</b>
</summary>

```cmd
Error in STT call for file 9136835610040002161_413008000252496:
{
    "reason": "ResultReason.Canceled",
    "error_details": "Due to service inactivity the client buffer size exceeded. Resetting the buffer. SessionId: xxxxx..."
}
```

**Ответ:** Обычно это происходит, когда вы кормите звук быстрее, чем контейнер распознавания речи может принять его. Буферы клиента заполняются, и отмена срабатывает. Вы должны контролировать параллелизм и RTF, на котором вы отправляете аудио.

<br>
</details>

<details>
<summary>
<b>Не могли бы вы объяснить эти ошибки контейнера текста к речи на примерах СЗ?</b>
</summary>

**Ответ:** Если контейнерная версия старше 1,3, то этот код следует использовать:

```cpp
const auto endpoint = "http://localhost:5000/speech/synthesize/cognitiveservices/v1";
auto config = SpeechConfig::FromEndpoint(endpoint);
auto synthesizer = SpeechSynthesizer::FromConfig(config);
auto result = synthesizer->SpeakTextAsync("{{{text1}}}").get();
```

Старые контейнеры не имеют необходимой точки для `FromHost` работы Carbon с API. Если контейнеры используются для версии 1.3, то этот код следует использовать:

```cpp
const auto host = "http://localhost:5000";
auto config = SpeechConfig::FromHost(host);
config->SetSpeechSynthesisVoiceName(
    "Microsoft Server Speech Text to Speech Voice (en-US, AriaRUS)");
auto synthesizer = SpeechSynthesizer::FromConfig(config);
auto result = synthesizer->SpeakTextAsync("{{{text1}}}").get();
```

Ниже приведен пример использования `FromEndpoint` API:

```cpp
const auto endpoint = "http://localhost:5000/cognitiveservices/v1";
auto config = SpeechConfig::FromEndpoint(endpoint);
config->SetSpeechSynthesisVoiceName(
    "Microsoft Server Speech Text to Speech Voice (en-US, AriaRUS)");
auto synthesizer = SpeechSynthesizer::FromConfig(config);
auto result = synthesizer->SpeakTextAsync("{{{text2}}}").get();
```

 Функция `SetSpeechSynthesisVoiceName` называется, потому что контейнеры с обновленным текстовым движком требуют голосового имени.

<br>
</details>

<details>
<summary>
<b>Как я могу использовать v1.7 речи SDK с речевой контейнер?</b>
</summary>

**Ответ:** Есть три конечные точки в контейнере речи для различных обычаев, они определяются как режимы речи - см. ниже:

[!INCLUDE [speech-modes](includes/speech-modes.md)]

Они предназначены для различных целей и используются по-разному.

[Образцы](https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/samples/python/console/speech_sample.py)питона :
- Для одного распознавания (интерактивный режим) с пользовательской конечной точкой (то есть; `SpeechConfig` с параметром конечных `speech_recognize_once_from_file_with_custom_endpoint_parameters()`точек), см.
- Для непрерывного распознавания (режим разговора), и просто изменить, `speech_recognize_continuous_from_file()`чтобы использовать пользовательские конечную точку, как указано выше, см.
- Для включения диктовки в образцах, как выше (только если вы действительно в ней нуждается), сразу после создания, `speech_config`добавить код `speech_config.enable_dictation()`.

В C, чтобы включить диктовку, вызвать функцию. `SpeechConfig.EnableDictation()`

### <a name="fromendpoint-apis"></a>`FromEndpoint`Api
| Язык | Сведения об API |
|----------|:------------|
| C++ | <a href="https://docs.microsoft.com/en-us/cpp/cognitive-services/speech/speechconfig#fromendpoint" target="_blank">`SpeechConfig::FromEndpoint` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| C# | <a href="https://docs.microsoft.com/dotnet/api/microsoft.cognitiveservices.speech.speechconfig.fromendpoint?view=azure-dotnet" target="_blank">`SpeechConfig.FromEndpoint` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Java | <a href="https://docs.microsoft.com/java/api/com.microsoft.cognitiveservices.speech.speechconfig.fromendpoint?view=azure-java-stable" target="_blank">`SpeechConfig.fromendpoint` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Objective-C | <a href="https://docs.microsoft.com/en-us/objectivec/cognitive-services/speech/spxspeechconfiguration#initwithendpoint" target="_blank">`SPXSpeechConfiguration:initWithEndpoint;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Python | <a href="https://docs.microsoft.com/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechconfig?view=azure-python" target="_blank">`SpeechConfig;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| JavaScript | В настоящее время не поддерживается, и это не планируется. |

<br>
</details>

<details>
<summary>
<b>Как я могу использовать v1.8 речи SDK с речевой контейнер?</b>
</summary>

**Ответ:** Появился новый `FromHost` API. Это не заменяет и не изменяет существующие AA. Он просто добавляет альтернативный способ создания речевой конфигурации с помощью пользовательского хоста.

### <a name="fromhost-apis"></a>`FromHost`Api

| Язык | Сведения об API |
|--|:-|
| C# | <a href="https://docs.microsoft.com/dotnet/api/microsoft.cognitiveservices.speech.speechconfig.fromhost?view=azure-dotnet" target="_blank">`SpeechConfig.FromHost` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| C++ | <a href="https://docs.microsoft.com/en-us/cpp/cognitive-services/speech/speechconfig#fromhost" target="_blank">`SpeechConfig::FromHost` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Java | <a href="https://docs.microsoft.com/java/api/com.microsoft.cognitiveservices.speech.speechconfig.fromhost?view=azure-java-stable" target="_blank">`SpeechConfig.fromHost` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Objective-C | <a href="https://docs.microsoft.com/en-us/objectivec/cognitive-services/speech/spxspeechconfiguration#initwithhost" target="_blank">`SPXSpeechConfiguration:initWithHost;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Python | <a href="https://docs.microsoft.com/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechconfig?view=azure-python" target="_blank">`SpeechConfig;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| JavaScript | Сейчас не поддерживается |

> Параметры: хост (обязательный), ключ подписки (необязательно, если вы можете использовать услугу без него).

Формат для `protocol://hostname:port` хоста, где `:port` не является обязательным (см. ниже):
- Если контейнер работает локально, хост-имя `localhost`находится в.
- Если контейнер работает на удаленном сервере, используйте имя хоста или адрес IPv4 этого сервера.

Примеры параметров для речевого текста:
- `ws://localhost:5000`- небезопасное подключение к локальному контейнеру с использованием порта 5000
- `ws://some.host.com:5000`- небезопасное подключение к контейнеру, работая на удаленном сервере

Образцы python сверху, `host` но `endpoint`использовать параметр вместо:

```python
speech_config = speechsdk.SpeechConfig(host="ws://localhost:5000")
```

<br>
</details>

## <a name="next-steps"></a>Следующие шаги

> [!div class="nextstepaction"]
> [Контейнеры cognitive Services](speech-container-howto.md)
