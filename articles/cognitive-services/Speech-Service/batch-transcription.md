---
title: Как использовать транскрипции пакетной службы — речь
titleSuffix: Azure Cognitive Services
description: Пакетное транскрибирование идеально подходит, если вам нужно расшифровать большой объем аудиоматериала в хранилище, например в хранилище BLOB-объектов Azure. Используя выделенный REST API, вы можете указывать аудиофайлы с помощью URI подписанного URL-адреса (SAS) и асинхронно получать транскрипции.
services: cognitive-services
author: PanosPeriorellis
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 07/05/2019
ms.author: panosper
ms.openlocfilehash: 2cccd17ce04b3954a7d0720d9ba25bbe792da3b6
ms.sourcegitcommit: 5aefc96fd34c141275af31874700edbb829436bb
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/04/2019
ms.locfileid: "74806343"
---
# <a name="why-use-batch-transcription"></a>Преимущества пакетного транскрибирования

Пакетное транскрибирование идеально подходит, если вам нужно расшифровать большой объем аудиоматериала в хранилище, например в хранилище BLOB-объектов Azure. Используя выделенный REST API, вы можете указывать аудиофайлы с помощью URI подписанного URL-адреса (SAS) и асинхронно получать транскрипции.

## <a name="prerequisites"></a>Технические условия

### <a name="subscription-key"></a>Ключ подписки

Как и для всех функций службы "Речь", вам необходимо создать ключ подписки на [портале Azure](https://portal.azure.com), следуя [руководству по началу работы](get-started.md). Если вы планируете получать расшифровки из наших базовых моделей, то вам нужно всего лишь создать ключ.

>[!NOTE]
> Для использования записи пакетов требуется стандартная подписка (S0) для службы речи. Ключи бесплатной подписки (F0) не подходят. Дополнительные сведения см. на странице с [ценами и ограничениями](https://azure.microsoft.com/pricing/details/cognitive-services/speech-services/).

### <a name="custom-models"></a>Настраиваемые модели

Чтобы настроить акустическую или языковую модель, следуйте инструкциям в статьях [Создание пользовательской акустической модели](how-to-customize-acoustic-models.md) и [Создание пользовательской языковой модели](how-to-customize-language-model.md). Чтобы использовать созданные модели при пакетном транскрибировании, необходимы идентификаторы моделей. Это не идентификатор конечной точки, указанный в представлении "Сведения о конечной точке", а идентификатор модели, который можно получить, щелкнув пункт "Сведения" для этой модели.

## <a name="the-batch-transcription-api"></a>API пакетного транскрибирования

API пакетного транскрибирования предоставляет асинхронное транскрибирование речи в текст и другие дополнительные возможности. Это REST API, предоставляющий методы для следующих задач:

1. Создание запросов пакетной обработки
1. Состояние запросов
1. Скачивание транскрипций

> [!NOTE]
> API пакетного транскрибирования идеально подходит для колл-центров, где обычно накапливаются тысячи часов аудио. Он упрощает транскрибирование больших объемов аудиозаписей.

### <a name="supported-formats"></a>Поддерживаемые форматы

API пакетного транскрибирования поддерживает следующие форматы:

| Формат | Кодек | Bitrate | Частота выборки |
|--------|-------|---------|-------------|
| WAV | PCM | 16-разрядный | 8 или 16 кГц, моно, стерео |
| MP3 | PCM | 16-разрядный | 8 или 16 кГц, моно, стерео |
| OGG | OPUS | 16-разрядный | 8 или 16 кГц, моно, стерео |

API пакетного транскрибирования разделяет левый и правый каналы во время обработки стереопотоков. Два файла JSON с результатом создаются из одного канала. Временные метки каждой фразы позволяют разработчику создавать упорядоченную окончательную расшифровку. Этот пример запроса включает свойства для фильтрации ненормативной лексики, пунктуации и меток времени на уровне слова.

### <a name="configuration"></a>Настройка

Параметры конфигурации указываются в формате JSON:

```json
{
  "recordingsUrl": "<URL to the Azure blob to transcribe>",
  "models": [{"Id":"<optional acoustic model ID>"},{"Id":"<optional language model ID>"}],
  "locale": "<locale to use, for example en-US>",
  "name": "<user defined name of the transcription batch>",
  "description": "<optional description of the transcription>",
  "properties": {
    "ProfanityFilterMode": "Masked",
    "PunctuationMode": "DictatedAndAutomatic",
    "AddWordLevelTimestamps" : "True",
    "AddSentiment" : "True"
  }
}
```

> [!NOTE]
> API пакетного транскрибирования использует службу REST для запроса расшифровок, их состояния и связанных результатов. Вы можете использовать API для любого языка. В следующем разделе описывается, как используется API.

### <a name="configuration-properties"></a>Свойства конфигурации

Используйте эти необязательные свойства для настройки транскрипции:

| Параметр | Описание |
|-----------|-------------|
| `ProfanityFilterMode` | Указывает, как обрабатывать ненормативную лексику в результатах распознавания. Допустимые значения: `None` — отключает фильтрацию ненормативной лексики, `masked` — заменяет ненормативную лексику звездочками, `removed` — удаляет всю ненормативную лексику из результата, `tags` — добавляет теги, указывающие на ненормативную лексику. Значение по умолчанию — `masked`. |
| `PunctuationMode` | Указывает, как обрабатывать знаки препинания в результатах распознавания. Допустимые значения: `None` — отключает знаки препинания, `dictated` — обозначает явные знаки препинания, `automatic` — передает знаки препинания для определения декодеру, `dictatedandautomatic` — определяет продиктованные знаки препинания или передает их декодеру. |
 | `AddWordLevelTimestamps` | Указывает, следует ли добавлять к выводимым данным метки времени на уровне слова. Допустимые значения: `true` — включает метки времени на уровне слова, `false` — (значение по умолчанию) отключает их. |
 | `AddSentiment` | Указывает, что тональности должен быть добавлен в utterance. Допустимые значения: `true`, которые позволяют тональности на utterance и `false` (значение по умолчанию), чтобы отключить его. |
 | `AddDiarization` | Указывает, что необходимо выполнить анализ диаризатион на входе, который должен быть каналом Mono, содержащим два голоса. Допустимые значения: `true`, которые позволяют диаризатион и `false` (значение по умолчанию) отключить его. Также требуется, чтобы `AddWordLevelTimestamps` было установлено значение true.|

### <a name="storage"></a>Storage

Транскрипция пакетов поддерживает [хранилище BLOB-объектов Azure](https://docs.microsoft.com/azure/storage/blobs/storage-blobs-overview) для чтения звука и записи записанных данных в хранилище.

## <a name="speaker-separation-diarization"></a>Разделение докладчика (Диаризатион)

Диаризатион — это процесс отделения динамиков от звука. Наш пакетный конвейер поддерживает Диаризатион и способен распознать два динамика в записях каналов Mono.

Чтобы запросить обработку запроса на расшифровку звука для диаризатион, необходимо просто добавить соответствующий параметр в HTTP-запросе, как показано ниже.

 ```json
{
  "recordingsUrl": "<URL to the Azure blob to transcribe>",
  "models": [{"Id":"<optional acoustic model ID>"},{"Id":"<optional language model ID>"}],
  "locale": "<locale to us, for example en-US>",
  "name": "<user defined name of the transcription batch>",
  "description": "<optional description of the transcription>",
  "properties": {
    "AddWordLevelTimestamps" : "True",
    "AddDiarization" : "True"
  }
}
```

Метки времени уровня Word также должны быть включены в качестве параметров в приведенном выше запросе. 

Соответствующий звук будет содержать динамики, обозначенные числом (в настоящее время поддерживаются только два голоса, поэтому динамики будут идентифицированы как "динамика 1" и "динамик 2"), а затем выводятся выходные данные для расшифровки.

Также обратите внимание, что Диаризатион недоступен в стерео-записях. Кроме того, все выходные данные JSON будут содержать тег динамика. Если диаризатион не используется, в выходных данных JSON отобразится значение "динамика: null".

> [!NOTE]
> Диаризатион доступен во всех регионах и для всех языков.

## <a name="sentiment"></a>Мнение

Тональности — это новая функция в API-интерфейсе для записи пакетов, которая является важной функцией в домене центра обработки вызовов. Клиенты могут использовать параметры `AddSentiment` для их запросов к

1.  Получение ценной информации о удовлетворенности клиентов
2.  Получение сведений о производительности агентов (команда, принимающая вызовы)
3.  Точное определение точного момента времени, когда вызов потребовался в отрицательном направлении
4.  Точное определение того, что хорошо при отключении отрицательных вызовов
5.  Указание клиентов и их отличий о продукте или службе

Тональности оценивается на каждом сегменте звука, где сегмент звука определен в качестве промежутка времени между началом utterance (смещение) и обнаружением конца байтового потока. Весь текст внутри этого сегмента используется для вычисления тональности. Мы не вычисляем статистические значения тональности для всего вызова или всех речевых функций каждого канала. Эти агрегаты оставлены владельцу домена для дальнейшего применения.

Тональности применяется к лексической форме.

Пример выходных данных JSON выглядит следующим образом:

```json
{
  "AudioFileResults": [
    {
      "AudioFileName": "Channel.0.wav",
      "AudioFileUrl": null,
      "SegmentResults": [
        {
          "RecognitionStatus": "Success",
          "ChannelNumber": null,
          "Offset": 400000,
          "Duration": 13300000,
          "NBest": [
            {
              "Confidence": 0.976174,
              "Lexical": "what's the weather like",
              "ITN": "what's the weather like",
              "MaskedITN": "what's the weather like",
              "Display": "What's the weather like?",
              "Words": null,
              "Sentiment": {
                "Negative": 0.206194,
                "Neutral": 0.793785,
                "Positive": 0.0
              }
            }
          ]
        }
      ]
    }
  ]
}
```
Эта функция использует модель тональности, которая в настоящее время находится в бета-версии.

## <a name="sample-code"></a>Пример кода

Полные примеры доступны в [репозитории примера GitHub](https://aka.ms/csspeech/samples) внутри подкаталога `samples/batch`.

Чтобы использовать настраиваемую акустическую или языковую модель, необходимо добавить в пример кода сведения о подписке, регион службы, URI SAS с указанием на аудиофайл для транскрибирования и идентификаторы моделей.

[!code-csharp[Configuration variables for batch transcription](~/samples-cognitive-services-speech-sdk/samples/batch/csharp/program.cs#batchdefinition)]

Пример кода настроит клиент и отправит запрос на расшифровку. Затем он запросит информацию о состоянии и выведет сведения о ходе выполнения транскрибирования.

[!code-csharp[Code to check batch transcription status](~/samples-cognitive-services-speech-sdk/samples/batch/csharp/program.cs#batchstatus)]

Подробные сведения о предыдущих вызовах см. в [документации по Swagger](https://westus.cris.ai/swagger/ui/index). Полная версия показанного здесь примера находится на [GitHub](https://aka.ms/csspeech/samples) в подкаталоге `samples/batch`.

Обратите внимание на асинхронную настройку для отправки аудио и получения состояния транскрибирования. Клиент, который вы создаете, является клиентом .NET HTTP. Существует метод `PostTranscriptions` для отправки сведений об аудиофайле и метод `GetTranscriptions` для получения результатов. `PostTranscriptions` возвращает дескриптор, который `GetTranscriptions` использует для создания дескриптора, чтобы получить состояние транскрибирования.

Текущий пример кода не определяет какие-либо пользовательские модели. Служба использует базовые модели для расшифровки файлов. Чтобы указать модели, можно передать с тем же методом идентификаторы акустической и языковой моделей.

> [!NOTE]
> Для базового транскрибирования не нужно объявлять идентификаторы базовых моделей. Если вы указали только идентификатор языковой модели (без идентификатора акустической модели), соответствующая акустическая модель выбирается автоматически. Если вы указали только идентификатор акустической модели, соответствующая языковая модель выбирается автоматически.

## <a name="download-the-sample"></a>Скачивание примера приложения

Пример можно скачать в [репозитории с примерами GitHub](https://aka.ms/csspeech/samples) в каталоге `samples/batch`.

> [!NOTE]
> Задания пакетного транскрибирования планируются на условиях максимально возможного выполнения запросов, при этом время запуска задания не прогнозируется. После запуска транскрибирование обрабатывается быстрее, чем аудиоматериал в реальном времени.

## <a name="next-steps"></a>Дальнейшие действия

* [Пробная версия Cognitive Services](https://azure.microsoft.com/try/cognitive-services/)
