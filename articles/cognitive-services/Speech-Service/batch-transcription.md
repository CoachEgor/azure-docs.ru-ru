---
title: Использование пакетного транскрибирования — Службы речи
titlesuffix: Azure Cognitive Services
description: Пакетное транскрибирование идеально подходит, если вам нужно расшифровать большой объем аудиоматериала в хранилище, например в хранилище BLOB-объектов Azure. Используя выделенный REST API, вы можете указывать аудиофайлы с помощью URI подписанного URL-адреса (SAS) и асинхронно получать транскрипции.
services: cognitive-services
author: PanosPeriorellis
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 07/05/2019
ms.author: panosper
ms.openlocfilehash: b71400c3ae3c1cc6737d9194b4d94bf0b9c7efa9
ms.sourcegitcommit: f10ae7078e477531af5b61a7fe64ab0e389830e8
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/05/2019
ms.locfileid: "67606747"
---
# <a name="why-use-batch-transcription"></a>Преимущества пакетного транскрибирования

Пакетное транскрибирование идеально подходит, если вам нужно расшифровать большой объем аудиоматериала в хранилище, например в хранилище BLOB-объектов Azure. Используя выделенный REST API, вы можете указывать аудиофайлы с помощью URI подписанного URL-адреса (SAS) и асинхронно получать транскрипции.

## <a name="prerequisites"></a>Технические условия

### <a name="subscription-key"></a>Ключ подписки

Как и для всех функций службы "Речь", вам необходимо создать ключ подписки на [портале Azure](https://portal.azure.com), следуя [руководству по началу работы](get-started.md). Если вы планируете получать расшифровки из наших базовых моделей, то вам нужно всего лишь создать ключ.

>[!NOTE]
> Для использования пакетного транскрибирования необходима стандартная подписка (S0) для Служб речи. Ключи бесплатной подписки (F0) не подходят. Дополнительные сведения см. на странице с [ценами и ограничениями](https://azure.microsoft.com/pricing/details/cognitive-services/speech-services/).

### <a name="custom-models"></a>Настраиваемые модели

Чтобы настроить акустическую или языковую модель, следуйте инструкциям в статьях [Создание пользовательской акустической модели](how-to-customize-acoustic-models.md) и [Создание пользовательской языковой модели](how-to-customize-language-model.md). Чтобы использовать созданные модели при пакетном транскрибировании, необходимы идентификаторы моделей. Это не идентификатор конечной точки, указанный в представлении "Сведения о конечной точке", а идентификатор модели, который можно получить, щелкнув пункт "Сведения" для этой модели.

## <a name="the-batch-transcription-api"></a>API пакетного транскрибирования

API пакетного транскрибирования предоставляет асинхронное транскрибирование речи в текст и другие дополнительные возможности. Это REST API, предоставляющий методы для следующих задач:

1. Создание запросов пакетной обработки
1. Состояние запросов
1. Скачивание транскрипций

> [!NOTE]
> API пакетного транскрибирования идеально подходит для колл-центров, где обычно накапливаются тысячи часов аудио. Он упрощает транскрибирование больших объемов аудиозаписей.

### <a name="supported-formats"></a>Поддерживаемые форматы

API пакетного транскрибирования поддерживает следующие форматы:

| Формат | Кодек | Bitrate | Частота выборки |
|--------|-------|---------|-------------|
| WAV | PCM | 16-разрядный | 8 или 16 кГц, моно, стерео |
| MP3 | PCM | 16-разрядный | 8 или 16 кГц, моно, стерео |
| OGG | OPUS | 16-разрядный | 8 или 16 кГц, моно, стерео |

API пакетного транскрибирования разделяет левый и правый каналы во время обработки стереопотоков. Два файла JSON с результатом создаются из одного канала. Временные метки каждой фразы позволяют разработчику создавать упорядоченную окончательную расшифровку. Этот пример запроса включает свойства для фильтрации ненормативной лексики, пунктуации и меток времени на уровне слова.

### <a name="configuration"></a>Параметр Configuration

Параметры конфигурации указываются в формате JSON:

```json
{
  "recordingsUrl": "<URL to the Azure blob to transcribe>",
  "models": [{"Id":"<optional acoustic model ID>"},{"Id":"<optional language model ID>"}],
  "locale": "<locale to us, for example en-US>",
  "name": "<user defined name of the transcription batch>",
  "description": "<optional description of the transcription>",
  "properties": {
    "ProfanityFilterMode": "Masked",
    "PunctuationMode": "DictatedAndAutomatic",
    "AddWordLevelTimestamps" : "True",
    "AddSentiment" : "True"
  }
}
```

> [!NOTE]
> API пакетного транскрибирования использует службу REST для запроса расшифровок, их состояния и связанных результатов. Вы можете использовать API для любого языка. В следующем разделе описывается, как используется API.

### <a name="configuration-properties"></a>Свойства конфигурации

Используйте эти дополнительные свойства для настройки расшифровка дикторского текста:

| Параметр | Описание |
|-----------|-------------|
| `ProfanityFilterMode` | Указывает, как обрабатывать ненормативную лексику в результатах распознавания. Допустимые значения: `none` — отключает фильтрацию ненормативной лексики, `masked` — заменяет ненормативную лексику звездочками, `removed` — удаляет всю ненормативную лексику из результата, `tags` — добавляет теги, указывающие на ненормативную лексику. Значение по умолчанию — `masked`. |
| `PunctuationMode` | Указывает, как обрабатывать знаки препинания в результатах распознавания. Допустимые значения: `none` — отключает знаки препинания, `dictated` — обозначает явные знаки препинания, `automatic` — передает знаки препинания для определения декодеру, `dictatedandautomatic` — определяет продиктованные знаки препинания или передает их декодеру. |
 | `AddWordLevelTimestamps` | Указывает, следует ли добавлять к выводимым данным метки времени на уровне слова. Допустимые значения: `true` — включает метки времени на уровне слова, `false` — (значение по умолчанию) отключает их. |
 | `AddSentiment` | Указывает, что utterance добавляются тональности. Принимаются значения `true` позволяющее тональности каждого utterance и `false` (значение по умолчанию) чтобы отключить его. |
 | `AddDiarization` | Указывает, что этот alalysis diarization должны выполняться во входных данных, который должен быть mono канал, содержащий два голоса. Принимаются значения `true` позволяющее diarization и `false` (значение по умолчанию) чтобы отключить его. Это также потребует выполнения `AddWordLevelTimestamps` устанавливается в значение true.|

### <a name="storage"></a>Хранилище

Пакетная служба поддерживает транскрибирования [хранилище BLOB-объектов Azure](https://docs.microsoft.com/azure/storage/blobs/storage-blobs-overview) для чтения, аудио- и транскрипций записи в хранилище.

## <a name="webhooks"></a>Веб-перехватчики

Опрос состояния транскрипция может не обеспечивать максимальную производительность, или предоставить удобства работы пользователей. Чтобы опросить состояние, можно зарегистрировать обратные вызовы, которые клиент увидит после завершения задач расшифровка дикторского текста для долго выполняющихся.

Дополнительные сведения см. в разделе [веб-перехватчики](webhooks.md).

## <a name="speaker-separation-diarization"></a>Разделение говорящего (Diarization)

Diarization — это процесс разделения динамиков в часть аудио. Нашего конвейера пакетной службы поддерживает Diarization и распознавать два для записей с одним каналом.

Для запроса, что ваш запрос аудио транскрипцию обрабатывается для diarization, необходимо просто добавьте соответствующий параметр в запросе HTTP, как показано ниже.

 ```json
{
  "recordingsUrl": "<URL to the Azure blob to transcribe>",
  "models": [{"Id":"<optional acoustic model ID>"},{"Id":"<optional language model ID>"}],
  "locale": "<locale to us, for example en-US>",
  "name": "<user defined name of the transcription batch>",
  "description": "<optional description of the transcription>",
  "properties": {
    "AddWordLevelTimestamps" : "True",
    "AddDiarization" : "True"
  }
}
```

Word уровня отметки времени также отобразятся «включить», так как параметры в приведенном выше запросе указывают.

Соответствующее аудио будет содержать номер динамиков (в настоящее время мы поддерживаем только два голоса, поэтому динамиков будут помечены как "говорящего 1" и «2 динамика") следуют выходные данные расшифровка дикторского текста.

Обратите внимание на то, что Diarization недоступна в стерео записей. Кроме того JSON, все выходные данные будут содержать тег говорящего. Если diarization не используется, он будет отображаться "докладчика: Значение NULL "в JSON выходных данных.

> [!NOTE]
> Diarization доступна во всех регионах, а также для всех языковых стандартах!

## <a name="sentiment"></a>Мнение

Тональность — это новая функция в API пакетной службы расшифровка дикторского текста и очень важна в домене центра вызова. Клиенты могут использовать `AddSentiment` параметров запросов

1.  Получите ценные сведения из уровня удовлетворенности клиентов
2.  Получить информацию о производительности агентов (команда, используя вызовы)
3.  Точно определить точное положение точки во времени, когда вызов заняло оборота в отрицательном направлении
4.  Точно определить, что пошло не удобен в том случае, если включение отрицательное вызовы положительный результат
5.  Определить, как клиенты и что они не нравится о продукт или услуга

Мнению присвоена на сегмент аудио, где создает аудио сегмент представляет собой промежуток времени между началом utterance (смещение) и бездействия обнаружения конца потока байтов. Весь текст внутри данного сегмента используется для вычисления тональности. Мы не вычислить значения совокупной тональности для всего звонка или всей речи каждого канала. Эти агрегаты которых возлагается на владельца домена для дальнейшего применения.

Тональности применяется на лексическую форму.

Пример выходных данных JSON приведено ниже:

```json
{
  "AudioFileResults": [
    {
      "AudioFileName": "Channel.0.wav",
      "AudioFileUrl": null,
      "SegmentResults": [
        {
          "RecognitionStatus": "Success",
          "ChannelNumber": null,
          "Offset": 400000,
          "Duration": 13300000,
          "NBest": [
            {
              "Confidence": 0.976174,
              "Lexical": "what's the weather like",
              "ITN": "what's the weather like",
              "MaskedITN": "what's the weather like",
              "Display": "What's the weather like?",
              "Words": null,
              "Sentiment": {
                "Negative": 0.206194,
                "Neutral": 0.793785,
                "Positive": 0.0
              }
            }
          ]
        }
      ]
    }
  ]
}
```
Компонент использует модель тональности, который сейчас находится на бета-версии.

## <a name="sample-code"></a>Пример кода

Полные примеры доступны в [примера репозитория GitHub](https://aka.ms/csspeech/samples) внутри `samples/batch` подкаталог.

Чтобы использовать настраиваемую акустическую или языковую модель, необходимо добавить в пример кода сведения о подписке, регион службы, URI SAS с указанием на аудиофайл для транскрибирования и идентификаторы моделей.

[!code-csharp[Configuration variables for batch transcription](~/samples-cognitive-services-speech-sdk/samples/batch/csharp/program.cs#batchdefinition)]

Пример кода настроит клиент и отправит запрос на транскрибирование. Затем он запросит информацию о состоянии и выведет сведения о ходе выполнения транскрибирования.

[!code-csharp[Code to check batch transcription status](~/samples-cognitive-services-speech-sdk/samples/batch/csharp/program.cs#batchstatus)]

Подробные сведения о предыдущих вызовах см. в [документации по Swagger](https://westus.cris.ai/swagger/ui/index). Полная версия показанного здесь примера находится на [GitHub](https://aka.ms/csspeech/samples) в подкаталоге `samples/batch`.

Обратите внимание на асинхронную настройку для отправки аудио и получения состояния транскрибирования. Клиент, который вы создаете, является клиентом .NET HTTP. Существует метод `PostTranscriptions` для отправки сведений об аудиофайле и метод `GetTranscriptions` для получения результатов. `PostTranscriptions` возвращает дескриптор, который `GetTranscriptions` использует для создания дескриптора, чтобы получить состояние транскрибирования.

Текущий пример кода не определяет какие-либо пользовательские модели. Служба использует базовые модели для расшифровки файлов. Чтобы указать модели, можно передать с тем же методом идентификаторы акустической и языковой моделей.

> [!NOTE]
> Для базового транскрибирования не нужно объявлять идентификаторы базовых моделей. Если вы указали только идентификатор языковой модели (без идентификатора акустической модели), соответствующая акустическая модель выбирается автоматически. Если вы указали только идентификатор акустической модели, соответствующая языковая модель выбирается автоматически.

## <a name="download-the-sample"></a>Скачивание примера приложения

Пример можно скачать в [репозитории с примерами GitHub](https://aka.ms/csspeech/samples) в каталоге `samples/batch`.

> [!NOTE]
> Задания пакетного транскрибирования планируются на условиях максимально возможного выполнения запросов, при этом время запуска задания не прогнозируется. После запуска транскрибирование обрабатывается быстрее, чем аудиоматериал в реальном времени.

## <a name="next-steps"></a>Дальнейшие действия

* [Пробная версия Cognitive Services](https://azure.microsoft.com/try/cognitive-services/)
