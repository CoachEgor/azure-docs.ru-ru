---
title: Оценка вознаграждения (Персонализатор)
description: Оценка вознаграждения указывает, насколько хорошо подходит пользователю выбор персонализации RewardActionID. Значение оценки вознаграждения определяется бизнес-логикой, основанной на наблюдении за поведением пользователей. Персонализатор обучает свои модели машинного обучения, оценивая вознаграждения.
ms.date: 02/20/2020
ms.topic: conceptual
ms.openlocfilehash: 734e4d0fdcec25884f8535ec61ccd10569fa8890
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "79219371"
---
# <a name="reward-scores-indicate-success-of-personalization"></a>Оценки вознаграждения демонстрируют успешность персонализации

Оценка вознаграждения указывает, насколько хорошо подходит пользователю выбор персонализации [RewardActionID](https://docs.microsoft.com/rest/api/cognitiveservices/personalizer/rank/rank#response). Значение оценки вознаграждения определяется бизнес-логикой, основанной на наблюдении за поведением пользователей.

Персонализатор обучает свои модели машинного обучения, оценивая вознаграждения.

[Узнайте, как](how-to-settings.md#configure-rewards-for-the-feedback-loop) настроить балл вознаграждения по умолчанию на портале Azure для ресурса Personalizer.

## <a name="use-reward-api-to-send-reward-score-to-personalizer"></a>Передача оценки вознаграждения в Персонализатор через API вознаграждения

Вознаграждения отправляются в Персонализатор через [API вознаграждения](https://docs.microsoft.com/rest/api/cognitiveservices/personalizer/events/reward). Как правило, вознаграждением является число от 0 до 1. Отрицательная награда, со значением -1, возможно в определенных сценариях и должна использоваться только в том случае, если у вас есть опыт обучения подкреплению (RL). Персонализатор обучает модель для достижения наивысшей возможной суммы вознаграждения за период времени.

Вознаграждения отправляются после выполнения пользователем действий, что может произойти через несколько дней после этого. В параметре [Reward Wait Time](#reward-wait-time) (Время ожидания вознаграждения) на портале Azure можно указать максимальное количество времени, в течение которого Персонализатор ожидает вознаграждения. По истечении этого времени считается, что вознаграждения нет или применяется настроенное по умолчанию вознаграждение.

Если оценка вознаграждения для события не получена в течение **времени ожидания вознаграждения**, применяется **Default Reward** (Вознаграждение по умолчанию). Обычно **[вознаграждение по умолчанию](how-to-settings.md#configure-reward-settings-for-the-feedback-loop-based-on-use-case)** равно нулю.


## <a name="behaviors-and-data-to-consider-for-rewards"></a>Поведение и данные, которые следует учитывать для вознаграждения

Рассмотрим эти сигналы и поведение для контекста оценки вознаграждения:

* Прямой ввод пользователя для получения предложений, если присутствуют параметры ("Вы имеете в виду X?").
* Продолжительность сеанса.
* Время между сеансами.
* Анализ тональности взаимодействий с пользователем.
* Прямые вопросы и мини-опросы, во время которых бот запрашивает у пользователя отзыв о полезности и точности.
* Ответ на оповещения или задержка такого ответа.

## <a name="composing-reward-scores"></a>Расчет оценок вознаграждения

Оценка вознаграждения должна вычисляться в бизнес-логике. Эта оценка может быть выражена в следующих форматах:

* одно число, отправляемое один раз;
* немедленно отправляемая оценка (например 0,8) и отправляемая позже дополнительная оценка (обычно 0,2).

## <a name="default-rewards"></a>Вознаграждение по умолчанию

Если после вызова события Rank вознаграждение не получено в течение времени, указанного в параметре [Reward Wait Time](#reward-wait-time) (Время ожидания вознаграждения), Персонализатор неявно применяет для этого события **вознаграждение по умолчанию**.

## <a name="building-up-rewards-with-multiple-factors"></a>Вычисление оценок с несколькими факторами

Для эффективной персонализации, вы можете создать награду оценка на основе нескольких факторов.

Например, для персонализации списка видео, можно применить такие правила.

|Действия пользователя|Значение компонента оценки|
|--|--|
|Пользователь щелкнул верхний элемент.|Оценка +0,5|
|Пользователь открыл фактическое содержимое этого элемента.|Оценка +0,3|
|Пользователь просматривал содержимое не менее 5 минут или просмотрел не менее 30% содержимого.|Оценка +0,2|
|||

Итоговую оценку можно отправить в API.

## <a name="calling-the-reward-api-multiple-times"></a>Многократный вызов API вознаграждения

Вы можете вызывать API вознаграждения с одним идентификатором события несколько раз, отправляя разные оценки вознаграждения. Когда Personalizer получает эти награды, он определяет окончательную награду за это событие, агрегируя их, как указано в конфигурации Personalizer.

Значения агрегации:

*  **Во-первых:** Берет первый наградный балл, полученный за событие, и отбрасывает остальное.
* **Sum**: Принимает все наградные баллы, собранные для eventId, и добавляет их вместе.

Все вознаграждения за событие, полученные после истечения **времени ожидания вознаграждения**, отбрасываются и не учитываются в обучении моделей.

Путем суммы награды баллов, ваша окончательная награда может быть за пределами ожидаемого диапазона баллов. Это не приводит к сбою службы.

## <a name="best-practices-for-calculating-reward-score"></a>Советы и рекомендации для вычисления оценки вознаграждения

* **Рассмотрим истинные показатели успешной персонализации**: Легко думать с точки зрения кликов, но хорошая награда основана на том, что вы хотите, чтобы ваши пользователи *для достижения,* а не то, что вы хотите, чтобы люди *сделали*.  Например, вознаграждение на основе числа щелчков может привести к отбору содержимого с заголовками-наживками.

* **Используйте награду оценка того, насколько хорошо персонализации работал**: Персонализация фильм предложение, мы надеемся, приведет к пользователю смотреть фильм и придав ему высокий рейтинг. Но рейтинг фильма часто зависит от многих параметров (качества игры актеров, настроения пользователя), поэтому он не очень точно отражает эффективность *персонализации*. Если пользователь просмотрел несколько первых минут фильма, это может лучше свидетельствовать об эффективности персонализации. Таким образом, неплохим сигналом будет оценка 1 за 5 минут просмотра.

* **Вознаграждение применяется только к RewardActionID**: Personalizer применяет награды, чтобы понять эффективность действия, указанного в RewardActionID. Если вы решите отображать другие действия, и пользователь выполняет их, вознаграждение должно быть равно нулю.

* **Рассмотрим непредвиденные последствия**: Создание функций вознаграждения, которые приводят к ответственным результатам с [этикой и ответственным использованием.](ethics-responsible-use.md)

* **Используйте Incremental Rewards**: Добавление частичных вознаграждений за поведение пользователей меньшего размера помогает Personalizer достичь лучших вознаграждений. Такое частичное вознаграждение сообщает алгоритму о том, что он приближает пользователя к требуемому поведению.
    * Если вы отображаете список кинофильмов, то можно учитывать частичную заинтересованность пользователя, выражающуюся в наведении им указателя мыши на первый элемент списка для получения дополнительных сведений. За такое поведение можно присуждать оценку вознаграждения 0,1.
    * Если пользователь откроет страницу, а затем покинет ее, оценку вознаграждения можно повысить до 0,2.

## <a name="reward-wait-time"></a>Время ожидания результата

Персонализатор будет сопоставлять сведения о вызове Rank с вознаграждениями, отправленными в вызовах Reward, для обучения модели. Они могут поступать в разное время. Персонализатор ожидает в течение определенного времени, начиная с момента вызова Rank, даже если этот вызов Rank выполнялся как неактивное событие и активировался позже.

Если **время ожидания вознаграждения** истекает, когда еще не получено никакой информации о вознаграждении, к такому событию применяется значение вознаграждения по умолчанию в целях обучения. Максимальная длительность ожидания составляет 6 дней.

## <a name="best-practices-for-reward-wait-time"></a>Лучшие практики для времени ожидания вознаграждения

Следуйте этим рекомендациям, чтобы получить оптимальные результаты.

* Время ожидания вознаграждения должно быть максимально коротким, но достаточным для получения отклика пользователя.

* Время ожидания не должно меньшим, чем требуется для получения отклика. Например, если некоторая часть вознаграждения присуждается за 1 минуту просмотра, длительность эксперимента должна быть по меньшей мере вдвое дольше.

## <a name="next-steps"></a>Дальнейшие действия

* [Обучение с подкреплением](concepts-reinforcement-learning.md)
* [Работа с API ранжирования](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Rank/console)
* [Работа с API вознаграждения](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Reward)
