---
title: Оценка вознаграждения (Персонализатор)
titleSuffix: Azure Cognitive Services
description: Оценка вознаграждения указывает, насколько хорошо подходит пользователю выбор персонализации RewardActionID. Значение оценки вознаграждения определяется бизнес-логикой, основанной на наблюдении за поведением пользователей. Персонализатор обучает свои модели машинного обучения, оценивая вознаграждения.
services: cognitive-services
author: edjez
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 06/07/2019
ms.author: edjez
ms.openlocfilehash: 39db28cd7e11d77362a2aefcf4ad8d2748db59c2
ms.sourcegitcommit: dad277fbcfe0ed532b555298c9d6bc01fcaa94e2
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/10/2019
ms.locfileid: "67722510"
---
# <a name="reward-scores-indicate-success-of-personalization"></a>Оценки вознаграждения демонстрируют успешность персонализации

Оценка вознаграждения указывает, насколько хорошо подходит пользователю выбор персонализации [RewardActionID](https://docs.microsoft.com/rest/api/cognitiveservices/personalizer/rank/rank#response). Значение оценки вознаграждения определяется бизнес-логикой, основанной на наблюдении за поведением пользователей.

Персонализатор обучает свои модели машинного обучения, оценивая вознаграждения. 

## <a name="use-reward-api-to-send-reward-score-to-personalizer"></a>Передача оценки вознаграждения в Персонализатор через API вознаграждения

Вознаграждения отправляются в Персонализатор через [API вознаграждения](https://docs.microsoft.com/rest/api/cognitiveservices/personalizer/events/reward). Они выражаются числами в диапазоне от -1 до 1. Персонализатор обучает модель для достижения наивысшей возможной суммы вознаграждения за период времени.

Вознаграждения отправляются после выполнения пользователем действий, что может произойти через несколько дней после этого. В параметре [Reward Wait Time](#reward-wait-time) (Время ожидания вознаграждения) на портале Azure можно указать максимальное количество времени, в течение которого Персонализатор ожидает вознаграждения. По истечении этого времени считается, что вознаграждения нет или применяется настроенное по умолчанию вознаграждение.

Если оценка вознаграждения для события не получена в течение **времени ожидания вознаграждения**, применяется **Default Reward** (Вознаграждение по умолчанию). Обычно **[вознаграждение по умолчанию](how-to-settings.md#configure-reward-settings-for-the-feedback-loop-based-on-use-case)** равно нулю.


## <a name="behaviors-and-data-to-consider-for-rewards"></a>Поведение и данные, которые следует учитывать для вознаграждения

Рассмотрим эти сигналы и поведение для контекста оценки вознаграждения:

* Прямой ввод пользователя для получения предложений, если присутствуют параметры ("Вы имеете в виду X?").
* Продолжительность сеанса.
* Время между сеансами.
* Анализ тональности взаимодействий с пользователем.
* Прямые вопросы и мини-опросы, во время которых бот запрашивает у пользователя отзыв о полезности и точности.
* Ответ на оповещения или задержка такого ответа.

## <a name="composing-reward-scores"></a>Расчет оценок вознаграждения

Оценка вознаграждения должна вычисляться в бизнес-логике. Эта оценка может быть выражена в следующих форматах:

* одно число, отправляемое один раз; 
* немедленно отправляемая оценка (например 0,8) и отправляемая позже дополнительная оценка (обычно 0,2).

## <a name="default-rewards"></a>Вознаграждение по умолчанию

Если после вызова события Rank вознаграждение не получено в течение времени, указанного в параметре [Reward Wait Time](#reward-wait-time) (Время ожидания вознаграждения), Персонализатор неявно применяет для этого события **вознаграждение по умолчанию**.

## <a name="building-up-rewards-with-multiple-factors"></a>Вычисление оценок с несколькими факторами  

Чтобы добиться эффективной персонализации, вы можете вычислять оценку вознаграждения (в диапазоне от -1 до 1) с учетом нескольких факторов. 

Например, для персонализации списка видео, можно применить такие правила.

|Действия пользователя|Значение компонента оценки|
|--|--|
|Пользователь щелкнул верхний элемент.|Оценка +0,5|
|Пользователь открыл фактическое содержимое этого элемента.|Оценка +0,3|
|Пользователь просматривал содержимое не менее 5 минут или просмотрел не менее 30% содержимого.|Оценка +0,2|
|||

Итоговую оценку можно отправить в API.

## <a name="calling-the-reward-api-multiple-times"></a>Многократный вызов API вознаграждения

Вы можете вызывать API вознаграждения с одним идентификатором события несколько раз, отправляя разные оценки вознаграждения. Когда Персонализатор получает такие оценки вознаграждения, он вычисляет на их основе итоговое значение вознаграждения, агрегируя оценки в соответствии с параметрами Персонализатора.

Параметры агрегирования:

*  **Первая**: Принимается первый результат оценки вознаграждения, полученный для события, и отклоняются остальные.
* **Сумма**: Принимаются и суммируются все оценки вознаграждения для одного идентификатора события.

Все вознаграждения за событие, полученные после истечения **времени ожидания вознаграждения**, отбрасываются и не учитываются в обучении моделей.

При суммировании оценок итоговый результат может оказаться меньше –1 или больше +1. Это не приводит к сбою службы.

<!--
@edjez - is the number ignored if it is outside the acceptable range?
-->

## <a name="best-practices-for-calculating-reward-score"></a>Советы и рекомендации для вычисления оценки вознаграждения

* **Определите эффективные показатели успешного выполнения персонализации**. Учет щелчков вести просто, тогда как эффективная оценка вознаграждения должна учитывать *итог* действий пользователей, а не сами *действия*.  Например, вознаграждение на основе числа щелчков может привести к отбору содержимого с заголовками-наживками.

* **Используйте оценку вознаграждения, чтобы понять качество персонализации**. Персонализация предложений кинофильмов должна приводить к тому, что пользователь посмотрит предложенный фильм и поставит ему высокую оценку. Но рейтинг фильма часто зависит от многих параметров (качества игры актеров, настроения пользователя), поэтому он не очень точно отражает эффективность *персонализации*. Если пользователь просмотрел несколько первых минут фильма, это может лучше свидетельствовать об эффективности персонализации. Таким образом, неплохим сигналом будет оценка 1 за 5 минут просмотра.

* **Вознаграждения применяются только к RewardActionID**. Персонализатор применяет вознаграждения, чтобы понять эффективность действия, которое указано в RewardActionID. Если вы решите отображать другие действия, и пользователь выполняет их, вознаграждение должно быть равно нулю.

* **Учитывайте непредвиденные последствия**. Создайте функцию вознаграждения, которая соблюдает высокий уровень [этики и ответственного использования](ethics-responsible-use.md).

* **Используйте вознаграждения с приращением**. Присвоение частичного вознаграждения за неполный результат позволяет Персонализатору получить более точные результаты вознаграждения. Такое частичное вознаграждение сообщает алгоритму о том, что он приближает пользователя к требуемому поведению.
    * Если вы отображаете список кинофильмов, то можно учитывать частичную заинтересованность пользователя, выражающуюся в наведении им указателя мыши на первый элемент списка для получения дополнительных сведений. За такое поведение можно присуждать оценку вознаграждения 0,1. 
    * Если пользователь откроет страницу, а затем покинет ее, оценку вознаграждения можно повысить до 0,2. 

## <a name="reward-wait-time"></a>Время ожидания результата

Персонализатор будет сопоставлять сведения о вызове Rank с вознаграждениями, отправленными в вызовах Reward, для обучения модели. Они могут поступать в разное время. Персонализатор ожидает в течение определенного времени, начиная с момента вызова Rank, даже если этот вызов Rank выполнялся как неактивное событие и активировался позже.

Если **время ожидания вознаграждения** истекает, когда еще не получено никакой информации о вознаграждении, к такому событию применяется значение вознаграждения по умолчанию в целях обучения. Максимальная длительность ожидания составляет 6 дней.

## <a name="best-practices-for-setting-reward-wait-time"></a>Советы и рекомендации для настройки времени ожидания вознаграждения

Следуйте этим рекомендациям, чтобы получить оптимальные результаты.

* Время ожидания вознаграждения должно быть максимально коротким, но достаточным для получения отклика пользователя. 

<!--@Edjez - storage quota? -->

* Время ожидания не должно меньшим, чем требуется для получения отклика. Например, если некоторая часть вознаграждения присуждается за 1 минуту просмотра, длительность эксперимента должна быть по меньшей мере вдвое дольше.

## <a name="next-steps"></a>Следующие шаги

* [Обучение с подкреплением](concepts-reinforcement-learning.md) 
* [Работа с API ранжирования](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Rank/console)
* [Работа с API вознаграждения](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Reward)
