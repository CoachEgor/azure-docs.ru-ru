---
title: Персонализатор. Активное обучение
titleSuffix: Azure Cognitive Services
description: ''
services: cognitive-services
author: edjez
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: overview
ms.date: 05/30/2019
ms.author: edjez
ms.openlocfilehash: d758e8fc7952a414003746d39df9368f3274b08b
ms.sourcegitcommit: cababb51721f6ab6b61dda6d18345514f074fb2e
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/04/2019
ms.locfileid: "66482062"
---
# <a name="active-learning-and-learning-policies"></a>Активное обучение и политики обучения 

Когда приложение вызывает API ранжирования, вы получаете ранжирование содержимое. Бизнес-логика может использовать это ранжирование для определения того, будет ли содержимое отображаться для пользователя. Отображение ранжированного содержимого — это _активное_ событие ранжирования. Когда приложение не отображает ранжированное содержимое — это _неактивное_ событие ранжирования. 

Сведения об активном событии ранжирования возвращаются Персонализатору. Эта информация используется для продолжения обучения модели с использованием текущей политики обучения.

## <a name="active-events"></a>Активные события

Активные события должны всегда отображаться для пользователя, а вызов вознаграждения должен возвращаться для закрытия цикла обучения. 

### <a name="inactive-events"></a>Неактивные события 

Неактивные события не должны изменять базовую модель, так как пользователь не может ничего выбрать из ранжированного содержимого.

## <a name="dont-train-with-inactive-rank-events"></a>Не следует выполнять обучение на основе неактивных событий ранжирования 

Для некоторых приложений вы можете вызывать API ранжирования, не зная, будет ли приложение отображать результаты пользователю. 

Это происходит в следующих случаях:

* Вы можете выполнить предварительное отображение определенных элементов пользовательского интерфейса, которые пользователь может или не может видеть. 
* Приложения могут выполнять прогнозную персонализацию, в рамках которой вызовы ранжирования выполняются с контекстом не непосредственно в реальном времени, и результаты их выполнения могут или не могут использоваться приложением. 

### <a name="disable-active-learning-for-inactive-rank-events-during-rank-call"></a>Отключение активного обучения для неактивных событий ранжирования во время вызова ранжирования

Для отключения автоматического обучения вызовите ранжирование с помощью `learningEnabled = False`.

Обучение для неактивного события неявно активируется при отправке вознаграждения для ранжирования.

## <a name="learning-policies"></a>Политики обучения

Политики обучения определяют некоторые *гиперпараметры* обучения модели. Две модели с одинаковыми данными, обученные на основе разных политик обучения, будут вести себя по-разному.

### <a name="importing-and-exporting-learning-policies"></a>Импорт и экспорт политик обучения

Вы можете импортировать и экспортировать файлы политик обучения на портале Azure. Это позволяет хранить, тестировать, заменять их и архивировать существующие политики в вашей системе управления версиями в качестве артефактов для последующего использования и аудита.

### <a name="learning-policy-settings"></a>Параметры политик обучения

Параметры **политик обучения** не следует изменять. Изменяйте их, только если знаете, как они влияют Персонализатор. В противном случае вы столкнетесь с побочными эффектами, включая неработоспособность моделей Персонализатора.

### <a name="comparing-effectiveness-of-learning-policies"></a>Сравнение эффективности политик обучения

Вы можете сравнивать то, как разные политики обучения применялись бы к ранее использовавшимся данным в журналах Персонализатора, выполняя [автономные оценивания](concepts-offline-evaluation.md).

[Отправьте собственные политики обучения](how-to-offline-evaluation.md) для сравнения с текущей политикой обучения.

### <a name="discovery-of-optimized-learning-policies"></a>Обнаружение оптимизированных политик обучения

Персонализатор можно создать оптимизированную политику обучения при выполнении [автономной оценки](how-to-offline-evaluation.md). Использование оптимизированных политик обучения, характеризующееся с получением лучших вознаграждений при автономной оценке, также даст лучшие результаты при использовании в интерактивном режиме в Персонализаторе.

Вы можете применить созданную оптимизированную политику обучения непосредственно к Персонализатору, чтобы сразу же заменить текущую политику, или сохранить ее для последующей оценки, чтобы в будущем решить, следует ли ее отменить, сохранить или применить позже.