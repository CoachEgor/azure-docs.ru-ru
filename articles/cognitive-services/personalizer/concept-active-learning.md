---
title: Активные и неактивные события — Персонализация
titleSuffix: Azure Cognitive Services
description: В этой статье рассматривается использование активных и неактивных событий, параметров обучения и политик обучения в службе персонализации.
services: cognitive-services
author: diberry
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 05/30/2019
ms.author: diberry
ms.openlocfilehash: 1641a1020193395d7d2ddb9c4893bd7bc89cdcd0
ms.sourcegitcommit: 609d4bdb0467fd0af40e14a86eb40b9d03669ea1
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/06/2019
ms.locfileid: "73681860"
---
# <a name="active-and-inactive-events"></a>Активные и неактивные события

Когда приложение вызывает API ранжирования, вы получаете действие, которое приложение должно отобразить в поле **ревардактионид** .  С этого момента Персонализация принимает вызов наград с тем же идентификатором eventId. Оценка вознаграждений будет использоваться для обучения модели для будущих вызовов ранжирования. Если для eventId не получено получение запроса на вознаграждение, применяется вознаграждение по умолчанию. Вознаграждения по умолчанию задаются в портал Azure.

В некоторых сценариях приложению может потребоваться обратиться к рангу, прежде чем он даже сможет знать, будет ли результат использоваться или отображаться для пользователя. Это может произойти в тех ситуациях, когда, например, страница отрисовки содержимого с повышенным уровнем перезаписывается маркетинговой кампанией. Если результат вызова ранжирования никогда не использовался и пользователь не видел его, не отправляйте соответствующий Звонок на вознаграждение.

Как правило, эти сценарии происходят, когда:

* Выполняется предварительная Визуализация пользовательского интерфейса, который пользователь может или не может видеть. 
* Приложение выполняет прогнозную персонализацию, в которой ранжирующие вызовы выполняются с небольшим контекстом в режиме реального времени, а приложение может или не может использовать выходные данные. 

В этих случаях используйте персонализацию для вызова Rank, запрашивая событие как _неактивное_. Персонализация не ожидает этого события и не будет применять вознаграждение по умолчанию. В дальнейшем в бизнес-логике, если приложение использует информацию из вызова ранжирования, просто _активируйте_ событие. Как только событие будет активно, персонализация предпринимает событие. Если в API-награде нет явного вызова, персонализация применяет вознаграждение по умолчанию.

## <a name="inactive-events"></a>Неактивные события

Чтобы отключить обучение для события, вызовите Rank с помощью `learningEnabled = False`. Для неактивного события обучение неявно активируется, если вы отправляете вознаграждение на событие eventId или вызываете API `activate` для этого eventId.

## <a name="learning-settings"></a>Параметры обучения

Параметры обучения определяют *Параметры* обучения модели. Две модели одних и тех же данных, которые обучены по различным настройкам обучения, будут отличаться.

### <a name="import-and-export-learning-policies"></a>Импорт и экспорт политик обучения

Файлы политики обучения можно импортировать и экспортировать из портал Azure. Используйте этот метод, чтобы сохранить существующие политики, протестировать их, заменить и заархивировать в системе управления исходным кодом в качестве артефактов для последующего обращения и аудита.

### <a name="understand-learning-policy-settings"></a>Общие сведения о параметрах политики обучения

Параметры в политике обучения не предназначены для изменения. Измените параметры только в том случае, если вы понимаете, как они влияют на персонализацию. Без этого набора знаний могут возникнуть проблемы, включая недействительность моделей персонализации.

### <a name="compare-learning-policies"></a>Сравнение политик обучения

Вы можете сравнить, как различные политики обучения выполняются с прошлыми данными в журналах персонализации, выполнив [автономные вычисления](concepts-offline-evaluation.md).

[Загрузите собственные политики обучения](how-to-offline-evaluation.md) , чтобы сравнить их с текущей политикой обучения.

### <a name="optimize-learning-policies"></a>Оптимизация политик обучения

Персонализация может создать оптимизированную политику обучения в [автономной оценке](how-to-offline-evaluation.md). Оптимизированная политика обучения, которая обеспечивает лучшие вознаграждения в автономной оценке, обеспечит лучшие результаты при использовании в режиме "в сети" в службе персонализации.

После оптимизации политики обучения ее можно применить непосредственно к персонализации, чтобы она немедленно заменила текущую политику. Кроме того, можно сохранить оптимизированную политику для дальнейшей оценки, а затем решить, следует ли отбросить, сохранить или применить ее.
