---
title: Активные и неактивные события — Персонализация
titleSuffix: Azure Cognitive Services
description: ''
services: cognitive-services
author: diberry
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 05/30/2019
ms.author: diberry
ms.openlocfilehash: 321f12fef44cae43caf53d78b2908e68f9edd0a8
ms.sourcegitcommit: 38251963cf3b8c9373929e071b50fd9049942b37
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/29/2019
ms.locfileid: "73043908"
---
# <a name="active-and-inactive-events"></a>Активные и неактивные события

Когда приложение вызывает API ранжирования, вы получаете действие, которое приложение должно отобразить в поле Ревардактионид.  С этого момента Персонализация будет ожидать вызова наград с тем же идентификатором eventId. Оценка вознаграждений будет использоваться для обучения модели, которая будет использоваться для будущих вызовов ранжирования. Если для eventId не получен запрос на вознаграждение, будет применено изменили вознаграждение. Вознаграждения по умолчанию устанавливаются на портале Azure.

В некоторых случаях приложению может потребоваться обратиться к рангу, прежде чем даже знать, будет ли результат использоваться или отображаться для пользователя. Это может произойти в тех ситуациях, когда, например, страница просмотра содержимого с повышенным уровнем переписывается с помощью маркетинговой кампании. Если результат вызова ранжирования никогда не использовался и пользователь никогда не увидит его, то было бы неверно обучить его с любым вознаграждением, нулем или иным способом.
Обычно это происходит в следующих случаях:

* Вы можете выполнить предварительное отображение определенных элементов пользовательского интерфейса, которые пользователь может или не может видеть. 
* Приложения могут выполнять прогнозную персонализацию, в рамках которой вызовы ранжирования выполняются с контекстом не непосредственно в реальном времени, и результаты их выполнения могут или не могут использоваться приложением. 

В таких случаях правильный способ использования персонализации — вызов Rank, запрашивающий событие как _неактивное_. Персонализация не будет ждать этого события и не будет применять вознаграждение по умолчанию. В дальнейшем в бизнес-логике, если приложение использует информацию из вызова ранжирования, необходимо только _активировать_ событие. С момента активации события Персонализация ожидает получения события или применения вознаграждений по умолчанию, если явный вызов к API-интерфейсу наград не выполняется.

## <a name="get-inactive-events"></a>Получение неактивных событий

Чтобы отключить обучение для события, вызовите Rank с `learningEnabled = False`.

Обучение для неактивного события неявно активируется, если вы отправляете вознаграждение на событие eventId или вызываете API `activate` для этого eventId.

## <a name="learning-settings"></a>Параметры обучения

Параметры обучения определяют конкретные *Параметры* обучения модели. Две модели одних и тех же данных, обученные по различным параметрам обучения, будут отличаться.

### <a name="import-and-export-learning-policies"></a>Импорт и экспорт политик обучения

Вы можете импортировать и экспортировать файлы политик обучения на портале Azure. Это позволяет хранить, тестировать, заменять их и архивировать существующие политики в вашей системе управления версиями в качестве артефактов для последующего использования и аудита.

### <a name="learning-policy-settings"></a>Параметры политик обучения

Параметры **политик обучения** не следует изменять. Изменяйте их, только если знаете, как они влияют Персонализатор. В противном случае вы столкнетесь с побочными эффектами, включая неработоспособность моделей Персонализатора.

### <a name="comparing-effectiveness-of-learning-policies"></a>Сравнение эффективности политик обучения

Вы можете сравнивать то, как разные политики обучения применялись бы к ранее использовавшимся данным в журналах Персонализатора, выполняя [автономные оценивания](concepts-offline-evaluation.md).

[Отправьте собственные политики обучения](how-to-offline-evaluation.md) для сравнения с текущей политикой обучения.

### <a name="discovery-of-optimized-learning-policies"></a>Обнаружение оптимизированных политик обучения

Персонализатор можно создать оптимизированную политику обучения при выполнении [автономной оценки](how-to-offline-evaluation.md). Использование оптимизированных политик обучения, характеризующееся с получением лучших вознаграждений при автономной оценке, также даст лучшие результаты при использовании в интерактивном режиме в Персонализаторе.

Вы можете применить созданную оптимизированную политику обучения непосредственно к Персонализатору, чтобы сразу же заменить текущую политику, или сохранить ее для последующей оценки, чтобы в будущем решить, следует ли ее отменить, сохранить или применить позже.
