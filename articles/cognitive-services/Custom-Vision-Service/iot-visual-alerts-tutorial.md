---
title: Руководство по работе с примером приложения визуальных оповещений Интернета вещей
titleSuffix: Azure Cognitive Services
description: В этом учебнике показано, как использовать Пользовательское визуальное распознавание с устройством Интернета вещей, чтобы распознавать визуальные состояния в видеотрансляции и сообщать о них.
services: cognitive-services
author: PatrickFarley
manager: nitinme
ms.service: cognitive-services
ms.subservice: custom-vision
ms.topic: tutorial
ms.date: 12/05/2019
ms.author: pafarley
ms.openlocfilehash: 9f3802ada79ee87d1a04634f7caac3b1b4286dce
ms.sourcegitcommit: 5ab4f7a81d04a58f235071240718dfae3f1b370b
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/10/2019
ms.locfileid: "74978038"
---
# <a name="tutorial-use-custom-vision-with-an-iot-device-to-report-visual-states"></a>Руководство по использованию Пользовательского визуального распознавания с устройством Интернета вещей для сообщения о визуальных состояниях

В этом примере приложения показано, как использовать Пользовательское визуальное распознавание, чтобы обучить устройство с камерой обнаруживать визуальные состояния. Этот сценарий обнаружения можно реализовать на устройстве Интернета вещей с помощью модели ONNX, экспортированной из службы "Пользовательское визуальное распознавание".

Визуальное состояние описывает содержимое изображения: пустое помещение или комната с людьми, пустое шоссе или с грузовиком и т. д. На приведенном ниже изображении видно, что приложение обнаруживает, когда перед камерой помещается банан или яблоко.

![Анимация пользовательского интерфейса, распознающего фрукты перед камерой](./media/iot-visual-alerts-tutorial/scoring.gif)

В этом учебнике показано, как выполнять следующие действия.
> [!div class="checklist"]
> * Настроить пример приложения для использования собственных ресурсов Пользовательского визуального распознавания и Центра Интернета вещей.
> * Использовать приложение, чтобы обучить проект "Пользовательское визуальное распознавание".
> * Использовать приложение для оценки новых изображений в режиме реального времени и отправки результатов в Azure.

Если у вас еще нет подписки Azure, [создайте бесплатную учетную запись Azure](https://azure.microsoft.com/free/), прежде чем начинать работу. 

## <a name="prerequisites"></a>Предварительные требования

* [!INCLUDE [create-resources](includes/create-resources.md)]
    > [!IMPORTANT]
    > Этот проект должен быть **компактным** проектом классификации изображений, потому что позже мы будем экспортировать модель в ONNX.
* Вам также потребуется [создать ресурс Центра Интернета вещей](https://ms.portal.azure.com/#create/Microsoft.IotHub) в Azure.
* [Visual Studio 2015 или более поздней версии](https://www.visualstudio.com/downloads/).
* При необходимости устройство Интернета вещей под управлением ОС Windows 10 IoT Базовая версии 17763 или более поздней. Можно также запустить приложение непосредственно на компьютере.
   * Для Raspberry Pi 2 и 3 можно настроить Windows 10 непосредственно в приложении панели мониторинга Интернета вещей. На других устройствах, таких как DrangonBoard, необходимо установить ПО с помощью [метода eMMC](https://docs.microsoft.com/windows/iot-core/tutorials/quickstarter/devicesetup#flashing-with-emmc-for-dragonboard-410c-other-qualcomm-devices). Дополнительные сведения о настройке нового устройства см. в [этой статье](https://docs.microsoft.com/windows/iot-core/tutorials/quickstarter/devicesetup) в документации по Windows IoT.

## <a name="about-the-visual-alerts-app"></a>Сведения о приложении визуальных оповещений

Приложение визуальных оповещений Интернета вещей работает в непрерывном цикле, переключаясь между четырьмя разными состояниями по мере необходимости:

* **No Model** (Нет модели). Нерабочее состояние. Приложение будет постоянно переходить в спящий режим на одну секунду и проверять камеру.
* **Capturing Training Images** (Запись изображений для обучения). В этом состоянии приложение записывает изображение и передает его как изображение для обучения в целевой проект "Пользовательское визуальное распознавание". Затем приложение переходит в спящий режим на 500 мс и повторяет операцию до тех пор, пока не будет записано необходимое количество изображений. Затем оно запускает обучение модели Пользовательского визуального распознавания.
* **Waiting For Trained Model** (Ожидание обученной модели). В этом состоянии приложение вызывает API Пользовательского визуального распознавания каждую секунду, чтобы проверить, содержит ли целевой проект обученную итерацию. При обнаружении такой модели приложение загружает соответствующую модель ONNX в локальный файл и переключается в состояние **Scoring** (Оценка).
* **Scoring** (Оценка). В этом состоянии приложение использует Машинное обучение Windows для оценки одного кадра из камеры с использованием локальной модели ONNX. Полученная классификация изображений отображается на экране и отправляется в виде сообщения в Центр Интернета вещей. Затем приложение переходит в спящий режим на одну секунду, прежде чем оценить новое изображение.

## <a name="understand-the-code-structure"></a>Общие сведения о структуре кода

Ниже приведены файлы, которые реализуют основные функции для этого приложения.

| Файл | ОПИСАНИЕ |
|-------------|-------------|
| [MainPage.xaml](https://github.com/Azure-Samples/Cognitive-Services-Vision-Solution-Templates/blob/master/IoTVisualAlerts/MainPage.xaml) | Этот файл определяет пользовательский интерфейс XAML. Он содержит элемент управления веб-камеры и метки, используемые для обновления состояния.|
| [MainPage.xaml.cs](https://github.com/Azure-Samples/Cognitive-Services-Vision-Solution-Templates/blob/master/IoTVisualAlerts/MainPage.xaml.cs) | Код в этом файле управляет поведением пользовательского интерфейса XAML. В нем также содержится код обработки состояния.|
| [CustomVision\CustomVisionServiceWrapper.cs](https://github.com/Azure-Samples/Cognitive-Services-Vision-Solution-Templates/blob/master/IoTVisualAlerts/CustomVision/CustomVisionServiceWrapper.cs) | Этот класс является программой-оболочкой, которая обрабатывает интеграцию со службой "Пользовательское визуальное распознавание".|
| [CustomVision\CustomVisionONNXModel.cs](https://github.com/Azure-Samples/Cognitive-Services-Vision-Solution-Templates/blob/master/IoTVisualAlerts/CustomVision/CustomVisionONNXModel.cs) | Этот класс представляет собой программу-оболочку, которая обрабатывает интеграцию с Машинным обучением Windows для загрузки модели ONNX и оценки изображений с ее помощью.|
| [IoTHub\IotHubWrapper.cs](https://github.com/Azure-Samples/Cognitive-Services-Vision-Solution-Templates/blob/master/IoTVisualAlerts/IoTHub/IotHubWrapper.cs) | Этот класс представляет собой программу-оболочку, которая обрабатывает интеграцию с Центром Интернета вещей для передачи результатов оценки в Azure.|

## <a name="set-up-the-visual-alerts-app"></a>Настройка приложения визуальных оповещений

Выполните следующие действия, чтобы запустить приложение визуальных оповещений Интернета вещей на компьютере или устройстве Интернета вещей.

1. Клонируйте или скачайте пример [IoTVisualAlerts](https://github.com/Azure-Samples/Cognitive-Services-Vision-Solution-Templates/tree/master/IoTVisualAlerts) на сайте GitHub.
1. Откройте решение _IoTVisualAlerts.sln_ в Visual Studio.
1. Интегрируйте проект "Пользовательское визуальное распознавание":
    1. В скрипте _CustomVision\CustomVisionServiceWrapper.cs_ замените переменную `ApiKey` вашим ключом обучения.
    1. Затем обновите переменную `Endpoint`, указав URL-адрес конечной точки, связанный с ключом.
    1. Обновите переменную `targetCVSProjectGuid`, указав соответствующий идентификатор проекта "Пользовательское визуальное распознавание", который будет использоваться. 
1. Настройте ресурс Центра Интернета вещей:
    1. В скрипте _IoTHub\IotHubWrapper.cs_ обновите переменную `s_connectionString`, указав соответствующую строку подключения для устройства. 
    1. На портале Azure загрузите экземпляр Центра Интернета вещей, щелкните **Устройства интернета вещей** в разделе **Обозреватели**, выберите целевое устройство (или создайте его при необходимости) и найдите строку подключения в разделе **Первичная строка подключения**. Строка будет содержать имя Центра Интернета вещей, идентификатор устройства и ключ общего доступа в следующем формате: `{your iot hub name}.azure-devices.net;DeviceId={your device id};SharedAccessKey={your access key}`.

## <a name="run-the-app"></a>Запуск приложения

Если вы используете приложение на компьютере, выберите **Локальный компьютер** в качестве целевого устройства в Visual Studio и **x64** или **x86** в качестве целевой платформы. Нажмите клавишу F5 для запуска программы. Приложение должно запуститься и отобразить прямую трансляцию с камеры и сообщение о состоянии.

При развертывании на устройстве Интернета вещей с процессором ARM необходимо выбрать **ARM** в качестве целевой платформы и **Удаленный компьютер** в качестве целевого устройства. При появлении запроса укажите IP-адрес устройства (он должен находиться в той же сети, что и ваш компьютер). Вы можете получить IP-адрес из приложения Windows IoT по умолчанию после загрузки устройства и подключения его к сети. Нажмите клавишу F5 для запуска программы.

При первом запуске приложения у него не будет никаких данных о визуальных состояниях. Отобразится сообщение о состоянии, информирующее об отсутствии модели. 

## <a name="capture-training-images"></a>Сохранение изображений для обучения

Чтобы настроить модель, необходимо переключить приложение в состояние **Capturing Training Images** (Запись изображений для обучения). Выполните один из следующих шагов.
* Если вы используете приложение на компьютере, нажмите кнопку в правом верхнем углу пользовательского интерфейса.
* Если вы используете приложение на устройстве Интернета вещей, вызовите метод `EnterLearningMode` на устройстве с помощью Центра Интернета вещей. Его можно вызвать с использованием записи устройства в меню Центра Интернета вещей на портале Azure или с помощью такого средства, как [обозреватель устройств Центра Интернета вещей](https://github.com/Azure/azure-iot-sdk-csharp/tree/master/tools/DeviceExplorer).
 
Когда приложение перейдет в состояние **записи изображений для обучения**, оно будет записывать около двух изображений каждую секунду, пока не достигнет целевого количества изображений. По умолчанию это 30 изображений, но этот параметр можно изменить, передав нужное число в качестве аргумента в метод `EnterLearningMode` Центра Интернета вещей. 

Пока приложение записывает изображения, необходимо предоставить камере типы визуальных состояний, которые требуется обнаруживать (например, пустую комнату, комнату с людьми, пустое рабочее место, стол с игрушечным грузовиком и т. д.).

## <a name="train-the-custom-vision-model"></a>Обучение модели Пользовательского визуального распознавания

После того как приложение закончит запись изображений, оно передаст их, а затем переключится в состояние **Waiting For Trained Model** (Ожидание обученной модели). На этом этапе перейдите на портал [Пользовательского визуального распознавания](https://www.customvision.ai/) и создайте модель на основе новых изображений для обучения. На анимации ниже показан пример такого процесса.

![Анимация: добавление тегов в несколько изображений бананов](./media/iot-visual-alerts-tutorial/labeling.gif)

Чтобы повторить этот процесс для собственного сценария, выполните следующие действия.

1. Войдите на портал службы [Пользовательское визуальное распознавание](http://customvision.ai).
1. Найдите целевой проект, в котором должны быть все изображения для обучения, переданные приложением.
1. Выберите соответствующие изображения для каждого визуального состояния, которое необходимо идентифицировать, и вручную примените к ним тег.
    * Например, если требуется различать пустую комнату и комнату с людьми, рекомендуется пометить пять или более изображений с людьми в качестве нового класса **People** и пометить пять или более изображений без людей в виде тега **Negative**. Это поможет модели различать два состояния.
    * Еще один пример. Если требуется оценить, насколько заполнена полка, вы можете использовать такие теги, как **EmptyShelf**, **PartiallyFullShelf** и **FullShelf**.
1. По завершении нажмите кнопку **Train** (Обучить).
1. После завершения обучения приложение обнаружит, что доступна обученная итерация. Будет запущен процесс экспорта обученной модели в ONNX и ее скачивания на устройство.

## <a name="use-the-trained-model"></a>Использование обученной модели

После того как приложение загрузит обученную модель, оно перейдет в состояние **Scoring** (Оценка) и начнет оценивать изображения с камеры в непрерывном цикле.

Для каждого записанного изображения приложение отобразит подходящий тег на экране. Если визуальное состояние не распознается, отобразится состояние **No Matches** (Нет совпадений). Приложение также отправляет эти сообщения в Центр Интернета вещей, и при обнаружении класса в сообщении будет содержаться метка, оценка достоверности и свойство с именем `detectedClassAlert`, которое клиенты Центра Интернета вещей могут использовать для ускорения маршрутизации сообщений на основе свойств.

Кроме того, в примере используется [библиотека Sense HAT](https://github.com/emmellsoft/RPi.SenseHat). Она помогает определить, когда обучение выполняется на Raspberry Pi с модулем Sense HAT. Модуль можно использовать для отображения выходных данных (например, все индикаторы будут загораться красным при обнаружении класса или могут быть неактивными при отсутствии совпадения).

## <a name="reuse-the-app"></a>Повторное использование приложения

Если вы хотите вернуть приложение в исходное состояние, это можно сделать, нажав кнопку в правом верхнем углу пользовательского интерфейса или вызвав метод `DeleteCurrentModel` в Центре Интернета вещей.

В любой момент можно повторить шаг отправки изображений для обучения, нажав кнопку в правом верхнем углу пользовательского интерфейса или повторно вызвав метод `EnterLearningMode`.

Если вы запускаете приложение на устройстве и вам нужно снова получить IP-адрес (например, для удаленного подключения с помощью [удаленного клиента Windows IoT](https://www.microsoft.com/p/windows-iot-remote-client/9nblggh5mnxz#activetab=pivot:overviewtab)), можно вызвать метод `GetIpAddress` в Центре Интернета вещей.

## <a name="clean-up-resources"></a>Очистка ресурсов

Проект "Пользовательское визуальное распознавание" можно удалить, если его поддержка больше не требуется. На [веб-сайте Пользовательского визуального распознавания](https://customvision.ai) перейдите в раздел **Projects** (Проекты) и щелкните значок корзины под новым проектом.

![Снимок экрана с панелью My New Project (Мой новый проект) со значком корзины](./media/csharp-tutorial/delete_project.png)

## <a name="next-steps"></a>Дополнительная информация

В этом учебнике вы настроили и запустили приложение, которое обнаруживает сведения о визуальном состоянии на устройстве Интернета вещей и отправляет результаты в Центр Интернета вещей. Ознакомьтесь с исходным кодом глубже и выполните одно из предлагаемых изменений ниже.

> [!div class="nextstepaction"]
> [Пример IoTVisualAlerts (GitHub)](https://github.com/Azure-Samples/Cognitive-Services-Vision-Solution-Templates/tree/master/IoTVisualAlerts)

* Добавьте метод Центра Интернета вещей, чтобы переключить приложение непосредственно в состояние **Waiting For Trained Model** (Ожидание обученной модели). Таким образом, модель можно обучить с помощью изображений, которые не записываются самим устройством, а затем отправить новую модель на устройство по команде.
* Чтобы создать панель мониторинга Power BI для визуализации оповещений Центра Интернета вещей, отправленных из примера, следуйте указаниям, приведенным в [этом учебнике](https://docs.microsoft.com/azure/iot-hub/iot-hub-live-data-visualization-in-power-bi).
* Чтобы создать приложение логики, которое реагирует на оповещения Центра Интернета вещей при обнаружении визуальных состояний, ознакомьтесь с [этим учебником](https://docs.microsoft.com/azure/iot-hub/iot-hub-monitoring-notifications-with-azure-logic-apps).
