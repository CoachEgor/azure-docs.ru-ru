---
title: Запись экспериментов с машинным обучением и метрик в журнал
titleSuffix: Azure Machine Learning
description: Отслеживайте эксперименты с машинным обучением Azure и метрики выполнения, чтобы улучшить процесс создания модели. Добавьте функцию ведения журнала в сценарий обучения и просматривайте результаты выполнения, записанные в журнале.  Используйте функции run.log, Run.start_logging или ScriptRunConfig.
services: machine-learning
author: likebupt
ms.author: keli19
ms.reviewer: peterlu
ms.service: machine-learning
ms.subservice: core
ms.workload: data-services
ms.topic: how-to
ms.date: 07/14/2020
ms.custom: seodec18
ms.openlocfilehash: 8a4f58423206a812dd94cc14d32aa52114c147d1
ms.sourcegitcommit: 3543d3b4f6c6f496d22ea5f97d8cd2700ac9a481
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/20/2020
ms.locfileid: "86536366"
---
# <a name="monitor-azure-ml-experiment-runs-and-metrics"></a>Отслеживание выполнения экспериментов с машинным обучением Azure и метрик
[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-basic-enterprise-sku.md)]

Чтобы улучшить процесс создания модели, отслеживайте эксперименты с машинным обучением Azure и метрики выполнения. В этой статье вы узнаете, как добавить код, отвечающий за ведение журнала, в сценарий обучения, как запускать эксперимент, отслеживать выполнение и проверять результаты с помощью службы Машинного обучения Azure.

> [!NOTE]
> Во время машинного обучения Azure в журнал также могут записываться данные из других источников, например при автоматическом запуске машинного обучения, или данные из контейнера Docker, выполняющего задание обучения. Эти журналы не документированы. Если у вас возникли проблемы и вы обратились в службу поддержки Майкрософт, ее специалисты могут воспользоваться этими журналами для устранения неполадок.

> [!TIP]
> Сведения в этом документе предназначены главным образом для специалистов по обработке и анализу данных и разработчиков, желающих отслеживать процесс обучения модели. Если вы являетесь администратором, который заинтересован в наблюдении из Машинного обучения Azure за использованием ресурсов и событиями, такими как квоты, завершенные обучающие запуски или завершенные развертывания моделей, см. раздел [Мониторинг Машинного обучения Azure](monitor-azure-machine-learning.md).

## <a name="available-metrics-to-track"></a>Доступные метрики, которые можно отслеживать

Следующие метрики можно добавить к выполнению во время обучения эксперимента. Чтобы просмотреть подробный список того, что можно отслеживать при выполнении, ознакомьтесь со [справочной документацией по классу Run](https://docs.microsoft.com/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py).

|Тип| Функция Python | Примечания|
|----|:----|:----|
|Скалярные значения |Функция:<br>`run.log(name, value, description='')`<br><br>Пример<br>run.log("accuracy", 0.95) |Запишите числовое или строковое значение в выполнение с заданным именем. Запись метрики в выполнение приводит к тому, что метрика будет сохранена в записи о выполнении в эксперименте.  Одну и ту же метрику можно несколько раз записать в рамках выполнения. Результат будет считаться вектором этой метрики.|
|Списки|Функция:<br>`run.log_list(name, value, description='')`<br><br>Пример<br>run.log_list("accuracies", [0.6, 0.7, 0.87]) | Запишите список значений в выполнение с заданным именем.|
|Строка|Функция:<br>`run.log_row(name, description=None, **kwargs)`<br>Пример<br>run.log_row("Y over X", x=1, y=0.4) | С помощью *log_row* создается метрика с несколькими столбцами, как описано в kwargs. Каждый именованный параметр создает столбец с указанным значением.  *log_row* можно вызвать один раз, чтобы записать произвольный кортеж, или несколько раз в цикле, чтобы создать полную таблицу.|
|Таблица|Функция:<br>`run.log_table(name, value, description='')`<br><br>Пример<br>run.log_table("Y over X", {"x":[1, 2, 3], "y":[0.6, 0.7, 0.89]}) | Запишите объект словаря в выполнение с заданным именем. |
|Изображения|Функция:<br>`run.log_image(name, path=None, plot=None)`<br><br>Пример<br>`run.log_image("ROC", plot=plt)` | Запишите изображение в запись о выполнении. Используйте log_image, чтобы записать файл изображения в формате PNG или график matplotlib в выполнение.  Эти изображения будут видимы, и их можно сравнить в записи о выполнении.|
|Добавление тега к выполнению|Функция:<br>`run.tag(key, value=None)`<br><br>Пример<br>run.tag("selected", "yes") | Добавление тега к выполнению с использованием строкового ключа и дополнительного значения строки.|
|Отправка файла или каталога|Функция:<br>`run.upload_file(name, path_or_stream)`<br> <br> Пример<br>run.upload_file("best_model.pkl", "./model.pkl") | Отправка файла в запись о выполнении. Выполняет автоматическую запись файла в указанном каталоге вывода, который по умолчанию имеет значение "./outputs" для большинства типов выполнения.  Используйте upload_file только в том случае, если необходимо отправить дополнительные файлы или не указан выходной каталог. Мы советуем добавлять `outputs` к имени для того, чтобы файл отправлялся в выходной каталог. Вы можете перечислить все файлы, связанные с этой записью о выполнении, вызвав `run.get_file_names()`.|

> [!NOTE]
> Метрики для скалярных значений, списков, строк и таблиц могут быть типа: число с плавающей точкой, целое число или строка.

## <a name="choose-a-logging-option"></a>Выбор параметра ведения журнала

Если вы хотите отслеживать свой эксперимент, необходимо добавить код, чтобы запустить ведение журнала при отправке выполнения. Ниже приведены способы активации отправки выполнений:
* __Run.start_logging__ добавляет функции регистрации в сценарий обучения и запускает интерактивный сеанс регистрации в определенном эксперименте. **start_logging** создает интерактивное выполнение для использования в таких сценариях, как записные книжки. Все метрики, записанные во время сеанса, добавляются в запись о выполнении эксперимента.
* __ScriptRunConfig__ добавляет функции ведения журнала в сценарий обучения и загружает всю папку сценария с выполнением.  **ScriptRunConfig** является классом для настройки конфигурации выполнений в сценарии. С использованием этого параметра можно добавить код мониторинга, чтобы получать уведомления о завершении или получить визуальное мини-приложение для мониторинга.
* __Ведение журнала конструктора__. Добавьте функции ведения журнала в конвейер конструктора перетаскивания с помощью модуля __Выполнение скриптов Python__. Добавьте код на языке Python, чтобы отслеживать эксперименты в конструкторе. 

## <a name="set-up-the-workspace"></a>Настройка рабочей области
Перед тем, как включать ведение журнала и отправлять эксперимент, необходимо настроить рабочую область.

1. Загрузите рабочую область. Дополнительные сведения о настройке конфигурации рабочей области см. в разделе [Файл конфигурации рабочей области](how-to-configure-environment.md#workspace).

[!notebook-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb?name=load_ws)]


## <a name="option-1-use-start_logging"></a>Вариант 1. Использование start_logging

**start_logging** создает интерактивное выполнение для использования в таких сценариях, как записные книжки. Все метрики, записанные во время сеанса, добавляются в запись о выполнении эксперимента.

В следующем примере простая модель sklearn Ridge обучается локально в локальной записной книжке Jupyter. Дополнительные сведения об отправке экспериментов в различные среды см. в статье [Выбор и использование целевого объекта вычислений для обучения вашей модели в Машинном обучении Azure](https://docs.microsoft.com/azure/machine-learning/how-to-set-up-training-targets).

### <a name="load-the-data"></a>Загрузка данных

В этом примере используется набор данных по диабету — хорошо известный небольшой набор данных, входящий в состав библиотеки scikit-learn. Эта ячейка загружает набор данных и разделяет его на случайные обучающие и проверочные наборы.

[!notebook-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb?name=load_data)]

### <a name="add-tracking"></a>Добавление функции отслеживания
Добавьте функцию отслеживания эксперимента с помощью пакета SDK для службы "Машинное обучение Azure" и отправьте сохраненную модель в запись о выполнении для эксперимента. Следующий код добавляет теги, журналы и отправляет файл модели в выполнение эксперимента.

[!notebook-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb?name=create_experiment)]

Сценарий оканчивается на ```run.complete()```, что помечает выполнение как завершенное.  Обычно эта функция используется в сценариях интерактивной записной книжки.

## <a name="option-2-use-scriptrunconfig"></a>Вариант 2. Использование ScriptRunConfig

[**ScriptRunConfig**](https://docs.microsoft.com/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py) — это класс для настройки конфигурации выполнения сценариев. С использованием этого параметра можно добавить код мониторинга, чтобы получать уведомления о завершении или получить визуальное мини-приложение для мониторинга.

В этом примере расширяется указанная выше базовая модель sklearn Ridge. В нем выполняется простая замена параметров по альфа-значениям модели для записи метрик и обученных моделей в выполнениях в рамках эксперимента. Пример запускается локально в управляемой пользователем среде. 

1. Создайте сценарий обучения `train.py`.

   [!code-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-on-local/train.py)]

2. Сценарий `train.py` ссылается на `mylib.py`, который позволяет получить список альфа-значений для использования в модели ridge.

   [!code-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-on-local/mylib.py)] 

3. Настройте управляемую пользователем локальную среду.

   [!notebook-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-on-local/train-on-local.ipynb?name=user_managed_env)]


4. Отправьте сценарий ```train.py``` для выполнения в среде, управляемой пользователем. Вся папка сценария отправляется для обучения, в том числе файл ```mylib.py```.

   [!notebook-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-on-local/train-on-local.ipynb?name=src)] [!notebook-python[] (~/MachineLearningNotebooks/how-to-use-azureml/training/train-on-local/train-on-local.ipynb?name=run)]

## <a name="option-3-log-designer-experiments"></a>Способ 3. Запись экспериментов в конструкторе

Чтобы добавить логику ведения журнала в эксперименты конструктора, используйте модуль __Выполнение скриптов Python__. С помощью этого рабочего процесса в журнал можно записать любое значение, но особенно полезно записывать метрики из модуля __Анализ модели__ для мониторинга производительности модели при разных запусках.

1. Подключите модуль __Выполнение скриптов Python__ к выходу модуля __Анализ модели__. __Вычисление модели__ может выводить результаты вычисления двух моделей. В следующем примере показано, как регистрировать метрики двух выходных портов на родительском уровне выполнения. 

    ![Подключение модуля "Выполнение скриптов Python" к выходу модуля "Анализ модели"](./media/how-to-track-experiments/designer-logging-pipeline.png)

1. Для записи в журнал средней абсолютной погрешности для обученной модели вставьте указанный ниже код в редактор кода модуля __Выполнение скриптов Python__.

    ```python
    # dataframe1 contains the values from Evaluate Model
    def azureml_main(dataframe1=None, dataframe2=None):
        print(f'Input pandas.DataFrame #1: {dataframe1}')
    
        from azureml.core import Run
    
        run = Run.get_context()
    
        # Log the mean absolute error to the parent run to see the metric in the run details page.
        # Note: 'run.parent.log()' should not be called multiple times because of performance issues.
        # If repeated calls are necessary, cache 'run.parent' as a local variable and call 'log()' on that variable.

        # Log left output port result of Evaluate Model. This also works when evaluate only 1 model.
        run.parent.log(name='Mean_Absolute_Error (left port)', value=dataframe1['Mean_Absolute_Error'][0])

        # Log right output port result of Evaluate Model.
        run.parent.log(name='Mean_Absolute_Error (right port)', value=dataframe1['Mean_Absolute_Error'][1])
    
        return dataframe1,
    ```

1. После завершения выполнения конвейера на странице эксперимента можно увидеть *Mean_Absolute_Error* .

    ![Подключение модуля "Выполнение скриптов Python" к выходу модуля "Анализ модели"](./media/how-to-track-experiments/experiment-page-metrics-across-runs.png)

## <a name="manage-a-run"></a>Управление выполнением

В статье [Запуск, отслеживание и отмена обучающих запусков](how-to-manage-runs.md) описываются специальные рабочие процессы Машинного обучения Azure для управления экспериментами.

## <a name="view-run-details"></a>Просмотр сведений о выполнении

### <a name="view-activequeued-runs-from-the-browser"></a>Просмотр активных и находящихся в очереди запусков с помощью браузера

Целевые объекты вычислений, используемые для обучения моделей, являются общим ресурсом. Таким образом, в определенный момент времени у них может быть несколько активных запусков или запусков в очереди. Чтобы просмотреть в браузере запуски для определенного целевого объекта вычислений, выполните следующее.

1. В [студии Машинного обучения Azure](https://ml.azure.com/) выберите рабочую область, а затем в левой части страницы выберите __Вычисление__.

1. Выберите __Кластеры обучения__, чтобы отобразить список целевых объектов вычислений, используемых для обучения. Выберите кластер.

    ![Выбор кластера обучения](./media/how-to-track-experiments/select-training-compute.png)

1. Выберите __Запуски__. Отобразится список запусков, использующих этот кластер. Чтобы просмотреть сведения о конкретном запуске, нажмите соответствующую ссылку в столбце __Запуск__. Чтобы просмотреть сведения об эксперименте, используйте соответствующую ссылку в столбце __Эксперимент__.

    ![Выбор запусков для кластера обучения](./media/how-to-track-experiments/show-runs-for-compute.png)
    
    > [!TIP]
    > У запуска могут быть дочерние запуски, поэтому у одного задания обучения может быть несколько записей.

После завершения запуска он больше не будет отображаться на этой странице. Чтобы просмотреть сведения о завершенных запусках, перейдите в раздел студии __Эксперименты__, затем выберите эксперимент и запуск. Дополнительные сведения см. в разделе [Запрос метрик выполнения](#queryrunmetrics).

### <a name="monitor-run-with-jupyter-notebook-widget"></a>Мониторинг запуска с помощью мини-приложения Jupyter Notebook
Если для отправки на запуск вы используете метод **ScriptRunConfig**, ход выполнения можно посмотреть с помощью [мини-приложения Jupyter](https://docs.microsoft.com/python/api/azureml-widgets/azureml.widgets?view=azure-ml-py). Подобно представлению запуска, мини-приложение является асинхронным и предоставляет обновления в реальном времени каждые 10–15 секунд до завершения задания.

1. Просмотрите мини-приложения Jupyter во время ожидания завершения выполнения.

   ```python
   from azureml.widgets import RunDetails
   RunDetails(run).show()
   ```

   ![Снимок экрана. Мини-приложение записной книжки Jupyter](./media/how-to-track-experiments/run-details-widget.png)

   Ссылку на это же отображение можно получить и в рабочей области.

   ```python
   print(run.get_portal_url())
   ```

2. **[Для автоматических запусков машинного обучения]** Доступ к диаграммам из предыдущего выполнения. Замените `<<experiment_name>>` именем соответствующего эксперимента.

   ``` 
   from azureml.widgets import RunDetails
   from azureml.core.run import Run

   experiment = Experiment (workspace, <<experiment_name>>)
   run_id = 'autoML_my_runID' #replace with run_ID
   run = Run(experiment, run_id)
   RunDetails(run).show()
   ```

   ![Мини-приложение записной книжки Jupyter для автоматического машинного обучения](./media/how-to-track-experiments/azure-machine-learning-auto-ml-widget.png)


Чтобы посмотреть дополнительные сведения о конвейере, щелкните нужный конвейер в таблице. В студии Машинного обучения Azure отобразится всплывающее окно с диаграммами.

### <a name="get-log-results-upon-completion"></a>Получение результатов записи по завершении

Обучение модели и мониторинг происходят в фоновом режиме, так что вы можете выполнять другие задачи во время ожидания. Также перед выполнением кода можно подождать, пока модель закончит обучение. При использовании **ScriptRunConfig** можно указать ```run.wait_for_completion(show_output = True)```, чтобы узнать, когда обучение модели будет завершено. Флаг ```show_output``` предоставляет подробные выходные данные. 

<a id="queryrunmetrics"></a>

### <a name="query-run-metrics"></a>Запрос метрик выполнения

Метрики обученной модели можно просмотреть с помощью ```run.get_metrics()```. Теперь можно получить все метрики, которые были зарегистрированы в примере выше, чтобы определить оптимальную модель.

<a name="view-the-experiment-in-the-web-portal"></a>
## <a name="view-the-experiment-in-your-workspace-in-azure-machine-learning-studio"></a>Просмотр эксперимента в рабочей области [студии Машинного обучения Azure](https://ml.azure.com).

После завершения эксперимента можно просмотреть записи о выполнении в эксперименте. Доступ к журналу можно получить из [студии Машинного обучения Azure](https://ml.azure.com).

Перейдите на вкладку "Эксперименты" и выберите нужный эксперимент. Вы попадете на панель мониторинга запуска эксперимента, где можно посмотреть метрики и диаграммы для каждого запуска. 

Можно изменить таблицу Run List, чтобы отображалось Последнее, минимальное или максимальное значение журнала для выполнения. Можно выбрать или отменить выбор нескольких запусков в списке Запуск, и выбранные запуски заполнят диаграммы данными. Можно также добавить новые диаграммы или изменить диаграммы, чтобы сравнить зарегистрированные метрики (минимальное, максимальное, Последнее или все значения) в нескольких запусках. Для более эффективного изучения данных можно также максимально увеличить объем диаграмм.

:::image type="content" source="media/how-to-track-experiments/experimentation-tab.gif" alt-text="Сведения о запуске в студии Машинного обучения Azure":::

Вы также можете в деталях просмотреть данные любого запуска, в том числе выходные данные или журналы выполнения, а также скачать моментальный снимок отправленного на запуск эксперимента, чтобы предоставить общий доступ к его папке.

### <a name="viewing-charts-in-run-details"></a>Просмотр диаграмм в сведениях о выполнении

Используя различные способы работы с API ведения журналов, вы можете регистрировать различные типы метрик во время выполнения и просматривать эти метрики в виде графиков в студии Машинного обучения Azure.

|Значение в журнале|Пример кода| Представление на портале|
|----|----|----|
|Массив числовых значений| `run.log_list(name='Fibonacci', value=[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89])`|График для одной переменной|
|Одно повторяющееся числовое значение с тем же именем метрики (например, в цикле for)| `for i in tqdm(range(-10, 10)):    run.log(name='Sigmoid', value=1 / (1 + np.exp(-i))) angle = i / 2.0`| График для одной переменной|
|Повторяющиеся строки с двумя числовыми столбцами|`run.log_row(name='Cosine Wave', angle=angle, cos=np.cos(angle))   sines['angle'].append(angle)      sines['sine'].append(np.sin(angle))`|Графики для двух переменных|
|Таблица с двумя числовыми столбцами|`run.log_table(name='Sine Wave', value=sines)`|Графики для двух переменных|


## <a name="example-notebooks"></a>Примеры записных книжек
Основные понятия, описанные в этой статье, демонстрируют следующие записные книжки:
* [how-to-use-azureml/training/train-within-notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-within-notebook)
* [how-to-use-azureml/training/train-on-local](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local)
* [how-to-use-azureml/track-and-monitor-experiments/logging-api](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/track-and-monitor-experiments/logging-api)

[!INCLUDE [aml-clone-in-azure-notebook](../../includes/aml-clone-for-examples.md)]

## <a name="next-steps"></a>Дальнейшие действия

Попробуйте следующие шаги, чтобы узнать, как использовать пакет SDK службы "Машинное обучение Azure" для Python:

* Пример регистрации оптимальных моделей и их развертывания см. в руководстве [Руководство № 1. Обучение модели классификации изображений с помощью Машинного обучения Azure](tutorial-train-models-with-aml.md).

* Дополнительные сведения о том, как обучать модели PyTorch с помощью службы "Машинное обучение Azure", см. в [этой статье](how-to-train-pytorch.md).
