---
title: Управление общими ловушками модели ML с помощью автоматизированного машинного обучения.
titleSuffix: Azure Machine Learning
description: Определите и устраните общие подводные камни моделей ML с помощью автоматизированных решений машинного обучения Azure Machine Learning.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.reviewer: nibaccam
author: nibaccam
ms.author: nibaccam
ms.date: 03/27/2020
ms.openlocfilehash: e0bc1aa48dfb40ea146fa79fdfd57da841ca1404
ms.sourcegitcommit: e040ab443f10e975954d41def759b1e9d96cdade
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/29/2020
ms.locfileid: "80385536"
---
# <a name="manage-ml-pitfalls-with-automated-machine-learning"></a>Управление ML ловушки с автоматизированным машинным обучением

Чрезмерное упадок и несбалансированные данные являются общими ловушками при построении моделей машинного обучения. По умолчанию автоматизированное машинное обучение Azure Machine Learning предоставляет диаграммы и метрики, помогающие определить эти риски, и реализует лучшие методы, помогающие смягчить их. 

## <a name="identify-over-fitting"></a>Определить чрезмерное соответствие

Чрезмерное упадок в машинном обучении происходит, когда модель слишком хорошо вписывается в обучающие данные, и в результате не может точно предсказать невидимые тестовые данные. Другими словами, модель просто запоминала конкретные закономерности и шум в обучающих данных, но не является достаточно гибкой, чтобы делать прогнозы на реальных данных.

Рассмотрим следующие обученные модели и их соответствующие точности поездов и испытаний.

| Модель | Точность поезда | Точность теста |
|-------|----------------|---------------|
| Объект | 99,9 % | 95% |
| B | 87% | 87% |
| C | 99,9 % | 45 % |

Учитывая модель **А,** существует распространенное заблуждение, что если точность теста на невидимых данных ниже, чем точность обучения, модель переоборудована. Тем не менее, точность теста всегда должна быть меньше, чем точность обучения, и различие для чрезмерной пригодности против надлежащим образом подходят сводится к *тому, насколько* менее точным. 

При сравнении моделей **A** и **B,** модель **А** является лучшей моделью, поскольку она имеет более высокую точность тестирования, и хотя точность теста немного ниже на 95%, это не существенная разница, которая предполагает, что чрезмерное соответствие присутствует. Вы не выбрали бы модель **B** просто потому, что поезд и тест точности ближе друг к другу.

Модель **C** представляет собой явный случай чрезмерной установки; точность обучения очень высока, но точность теста не в любом месте рядом, как высокая. Это различие является субъективным, но происходит от знания вашей проблемы и данных, и какие величины ошибки являются приемлемыми.

## <a name="prevent-over-fitting"></a>Предотвращение чрезмерной установки

В наиболее вопиющих случаях переоборудованная модель предполагает, что комбинации значений объектов, наблюдаемые во время обучения, всегда будут приводить к точно такой же выходной части для цели.

Лучший способ предотвратить чрезмерное соответствие заключается в том, чтобы следовать ML передового опыта, включая:

* Использование дополнительных учебных данных и устранение статистической необъективности
* Предотвращение утечки цели
* Использование меньшего количества функций
* **Регуляризация и оптимизация гиперпарамтора**
* **Ограничения сложности модели**
* **Перекрестная проверка**

В контексте автоматизированного ML, первые три пункта выше, являются **лучшими практиками, которые вы реализуете.** Последние три смелых элемента являются **лучшими практиками автоматизированных снастят ML по** умолчанию для защиты от чрезмерной установки. В настройках, помимо автоматизированного ML, все шесть лучших практик стоит следовать, чтобы избежать чрезмерной установки моделей.

### <a name="best-practices-you-implement"></a>Лучшие практики, которые вы реализуете

Использование **большего количества данных** является самым простым и оптимальным способом предотвращения чрезмерной установки, а в качестве дополнительного бонуса обычно повышает точность. Когда вы используете больше данных, модели становится все труднее запоминать точные шаблоны, и она вынуждена достигать решений, которые являются более гибкими для размещения большего количества условий. Также важно распознать **статистическую погрешность,** чтобы убедиться, что ваши обучающие данные не включают изолированные шаблоны, которые не будут существовать в данных живого прогнозирования. Этот сценарий может быть трудно решить, потому что не может быть чрезмерной установки между поездом и тестовые наборы, но может быть чрезмерной установки настоящее время по сравнению с живыми тестовыми данными.

Целевая утечка является аналогичной проблемой, где вы не можете увидеть чрезмерной установки между поездом / тестовых наборов, а она появляется на время прогнозирования. Утечка цели происходит, когда модель "обманывает" во время обучения, имея доступ к данным, которые она обычно не должна иметь во время прогнозирования. Например, если ваша проблема заключается в том, чтобы предсказать в понедельник, что цена на товар будет в пятницу, но одна из ваших функций случайно включены данные по четвергам, что бы данные модель не будет иметь на время прогнозирования, поскольку он не может видеть в будущем. Целевой утечки легко пропустить, но часто характеризуется аномально высокой точностью для вашей проблемы. Если вы пытаетесь предсказать цену акций и обучил модель с точностью 95%, то, скорее всего, утечек цели где-то в ваших функциях.

Удаление функций также может помочь с чрезмерной установкой, не позволяя модели иметь слишком много полей для запоминания конкретных шаблонов, что приводит к ее более гибкой. Это может быть трудно измерить количественно, но если вы можете удалить функции и сохранить ту же точность, вы, вероятно, сделали модель более гибкой и снизили риск чрезмерной установки.

### <a name="best-practices-automated-ml-implements"></a>Лучшие практики автоматизированных ml-орудия

Регуляризация — это процесс минимизации функции затрат для наказания сложных и чрезмерно оборудованных моделей. Существуют различные типы функций регуляризации, но в целом все они наказывают размер коэффициента модели, дисперсию и сложность. Автоматизированный ML использует L1 (Lasso), L2 (Ridge) и ElasticNet (L1 и L2 одновременно) в различных комбинациях с различными настройками гиперпараметра модели, которые контролируют чрезмерное облегающее. Проще говоря, автоматизированный ML будет варьироваться, насколько модель регулируется и выбрать лучший результат.

Автоматизированный ML также реализует явные ограничения сложности модели для предотвращения чрезмерной установки. В большинстве случаев эта реализация предназначена специально для алгоритмов дерева или леса решений, где отдельное дерево max-depth ограничено, а общее количество деревьев, используемых в методах леса или ансамбля, ограничено.

Перекрестная проверка (CV) — это процесс принятия множества ваших полных обучаемых данных и обучения модели по каждому подмноженому набору. Идея заключается в том, что модель может получить "повезло" и имеют большую точность с одним подмножеством, но с помощью многих подмножеств модель не достигнет этой высокой точности каждый раз. При выполнении резюме вы предоставляете набор данных удержания валидации, указываете свои складки резюме (количество подмножеств), а автоматизированный ML будет тренировать вашу модель и настраивать гиперпараметры, чтобы свести к минимуму ошибку в наборе валидации. Одна складка резюме может быть чрезмерно йоутой, но, используя многие из них, это снижает вероятность того, что окончательная модель чрезмерно подходит. Компромисс заключается в том, что cv приводит к увеличению времени обучения и, таким образом, больше затрат, потому что вместо обучения модели один раз, вы тренируете его один раз для каждого *n* cv подмножества. 

> [!NOTE]
> Перекрестная проверка не включена по умолчанию; он должен быть настроен в автоматизированных настройках ML. Однако после настройки перекрестной проверки и предоставления набора данных проверки процесс автоматизирован для вас. См. 

<a name="imbalance"></a>

## <a name="identify-models-with-imbalanced-data"></a>Определение моделей с несбалансированными данными

Несбалансированные данные обычно встречаются в данных для сценариев классификации машинного обучения и относятся к данным, содержащим непропорциональное соотношение наблюдений в каждом классе. Этот дисбаланс может привести к ложно воспринимаемому положительному эффекту точности модели, поскольку входные данные имеют предубеждение по отношению к одному классу, что приводит к тому, что обученная модель имитирует эту предвзятость. 

Поскольку алгоритмы классификации обычно оцениваются по точности, проверка точности модели является хорошим способом определить, повлияли ли на него несбалансированные данные. Было ли это действительно высокая точность или действительно низкая точность для определенных классов?

Кроме того, автоматизированные запуски ML автоматически генерируют следующие диаграммы, которые могут помочь вам понять правильность классификаций модели и определить модели, потенциально затронутые несбалансированными данными.

Диаграмма| Описание
---|---
[Матрица путаницы](how-to-understand-automated-ml.md#confusion-matrix)| Оценивает правильно классифицированные метки по фактическим метки данных. 
[Точность отзыва](how-to-understand-automated-ml.md#precision-recall-chart)| Оценивает соотношения правильных меток к соотношению найденных экземпляров меток данных 
[Кривые РПЦ](how-to-understand-automated-ml.md#roc)| Оценивает соотношение правильных меток к соотношению ложно-положительных меток.

## <a name="handle-imbalanced-data"></a>Обработка несбалансированных данных 

В рамках своей цели упрощения рабочего процесса машинного обучения автоматизированный ML создал возможности для решения несбалансированных данных, таких как, 

- **Столбец веса**: автоматизированный ML поддерживает взвешенный столбец как ввод, вызывая ряды в данных, которые будут взвешены вверх или вниз, что может сделать класс более или менее "важным".

- Алгоритмы, используемые автоматизированным ML, могут правильно обрабатывать дисбаланс до 20:1, то есть наиболее распространенный класс может иметь в 20 раз больше строк в данных, чем наименее распространенный класс.

Следующие методы являются дополнительными опциями для обработки несбалансированных данных за пределами автоматизированного ML. 

- Перевыборка даже на класс дисбаланс, либо вверх-выборы меньших классов или вниз выборки больших классов. Эти методы требуют опыта для обработки и анализа.

- Используйте метрику производительности, которая лучше справляется с несбалансированными данными. Например, оценка F1 является средневзвешенным значением точности и отзыва. Точность измеряет точность классификатора - низкая точность указывает на большое количество ложных срабатываний--, в то время как отзыв измеряет полноту классификатора - низкий отзыв указывает на большое количество ложных негативов. 

## <a name="next-steps"></a>Дальнейшие действия

Смотрите примеры и узнайте, как строить модели с помощью автоматизированного машинного обучения:

+ Следуйте [учебник: Автоматически обучать регрессионную модель с Azure машинного обучения](tutorial-auto-train-models.md)

+ Настройка настроек для автоматического учебного эксперимента:
  + В студии машинного обучения Azure [используйте эти шаги.](how-to-use-automated-ml-for-ml-models.md)
  + С Python SDK [используйте эти шаги.](how-to-configure-auto-train.md)


