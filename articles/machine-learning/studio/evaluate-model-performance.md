---
title: Оценка работы модели.
titleSuffix: ML Studio (classic) - Azure
description: Узнайте, как оценить производительность модели в Машинное обучение Azure Studio (классическая модель) и о метриках, доступных для этой задачи.
services: machine-learning
ms.service: machine-learning
ms.subservice: studio
ms.topic: conceptual
author: xiaoharper
ms.author: amlstudiodocs
ms.custom: seodec18, previous-author=heatherbshapiro, previous-ms.author=hshapiro
ms.date: 03/20/2017
ms.openlocfilehash: b37844ff93ed1cfb631c2d8da12d0729f61f44ed
ms.sourcegitcommit: 35715a7df8e476286e3fee954818ae1278cef1fc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/08/2019
ms.locfileid: "73837656"
---
# <a name="how-to-evaluate-model-performance-in-azure-machine-learning-studio-classic"></a>Оценка производительности модели в Машинное обучение Azure Studio (классическая модель)

В этой статье показано, как оценить производительность модели в Машинное обучение Azure Studio (классическая модель) и приводится краткое описание метрик, доступных для этой задачи. Доступны три стандартных сценария управляемого обучения: 

* регрессия;
* двоичная классификация; 
* классификация по нескольким классам.



Оценка эффективности модели является одним из основных этапов процесса обработки и анализа данных. Она показывает, насколько успешно обученная модель обрабатывает (прогнозирует) набор данных. 

Машинное обучение Azure Studio (классическая модель) поддерживает оценку модели двумя основными модулями машинного обучения: [Оценка][evaluate-model] модели и [Перекрестная проверка модели][cross-validate-model]. Эти модули позволяют видеть эффективность модели в пересчете на различные показатели, обычно используемые в машинном обучении и статистике.

## <a name="evaluation-vs-cross-validation"></a>Сравнение оценки и перекрестной проверки
Оценка и перекрестная проверка — это стандартные способы для измерения эффективности модели. Оба модуля генерируют показатели оценки, которые вы можете проверить или сравнить с показателями других моделей.

Функция " [Вычисление модели][evaluate-model] " предполагает, что в качестве входных данных используется оцененный DataSet (или два, если вы хотите сравнить производительность двух разных моделей). Поэтому необходимо обучить модель с помощью модуля [обучение модели][train-model] и создать прогнозы для некоторого набора данных с помощью модуля [Оценка модели][score-model] , прежде чем можно будет оценить результаты. Оценка основана на оцененных метках и вероятностях, а также на истинных метках, все из которых выводятся модулем [оценки модели][score-model] .

Кроме того, вы можете использовать перекрестную проверку, чтобы автоматически выполнить ряд операций "обучить-подсчитать-оценить" (10 сборок) для различных подмножеств входных данных. Входные данные делятся на 10 частей: одна резервируется для тестирования, а остальные 9 — для обучения. Этот процесс повторяется 10 раз, затем из показателей оценки выводится средняя величина. Эта процедура позволяет определить, насколько хорошо модель будет обобщаться на новых наборах данных. Модуль [перекрестной проверки модели][cross-validate-model] принимает обученную модель и некоторый помеченный набор данных и выводит результаты оценки каждого из 10 сверток в дополнение к усредненным результатам.

В следующих разделах мы создадим простые модели регрессии и классификации и вычисляем их производительность, используя модули [оценки][evaluate-model] и [перекрестной проверки модели][cross-validate-model] .

## <a name="evaluating-a-regression-model"></a>Оценка модели регрессии
Предположим, что мы хотим спрогнозировать цену автомобиля с помощью таких функций, как измерения, лошадиные мощности, спецификации ядер и т. д. Это типичная задача регрессии, где целевой переменной *price* (Цена) присвоено непрерывное числовое значение. Мы можем разместить модель линейной регрессии, которая, учитывая значения признаков определенного автомобиля, может прогнозировать стоимость этого автомобиля. Эту модель регрессии можно использовать для подсчета того же набора данных, который использовался для обучения. После получения прогнозируемых цен на автомобиль можно оценить производительность модели, просмотрев количество отклонений прогнозов от фактических цен в среднем. Чтобы проиллюстрировать это, мы используем набор данных " *данные о ценах на автомобиль" (RAW)* , доступный в разделе **сохраненные наборы** данных в студии машинного обучения (классический).

### <a name="creating-the-experiment"></a>Создание эксперимента
Добавьте следующие модули в рабочую область в классической версии Машинное обучение Azure Studio:

* данные о ценах на автомобили (необработанные);
* [Линейная регрессия][linear-regression]
* [Обучение модели][train-model]
* [Модель оценки][score-model]
* [Вычисление модели][evaluate-model]

Подключите порты, как показано ниже, на рис. 1 и задайте для столбца метки модуля [обучение модели][train-model] значение *Цена*.

![Оценка модели регрессии](./media/evaluate-model-performance/1.png)

Рисунок 1. Оценка модели регрессии.

### <a name="inspecting-the-evaluation-results"></a>Проверка результатов оценки
После выполнения эксперимента можно щелкнуть порт вывода модуля [Анализ модели][evaluate-model] и выбрать *визуализировать* , чтобы просмотреть результаты оценки. Для моделей регрессии доступны такие метрики оценки: *Mean Absolute Error* (Средняя абсолютная погрешность), *Root Mean Absolute Error* (Среднеквадратическая абсолютная погрешность), *Relative Absolute Error* (Относительная абсолютная погрешность), *Relative Squared Error* (Относительная среднеквадратическая погрешность) и *Coefficient of Determination* (Коэффициент детерминации).

Термин "ошибка" здесь означает разницу между прогнозируемым значением и истинным значением. Абсолютное значение или квадрат этой разницы обычно вычисляется, чтобы зафиксировать абсолютную величину ошибки во всех экземплярах, так как разница между прогнозируемым и истинным значением иногда может быть отрицательным числом. Показатели ошибки измеряют прогнозируемую эффективность модели регрессии с точки зрения среднего отклонения ее прогнозов от истинных значений. Чем ниже значения ошибок, тем более точно модель прогнозирует. Общий показатель ошибок 0 означает, что модель идеально подбирает данные.

Для определения способности модели подбирать данные также часто используется коэффициент детерминации, который также известен как R-квадрат. Его можно интерпретировать как пропорцию отклонений, которые объясняются моделью. В этом случае чем выше пропорция, тем лучше. Значение 1 означает идеальное совпадение.

![Показатели оценки линейной регрессии](./media/evaluate-model-performance/2.png)

Рис. 2. Показатели оценки линейной регрессии.

### <a name="using-cross-validation"></a>Использование перекрестной проверки
Как упоминалось ранее, вы можете выполнять многократное обучение, оценку и вычисления автоматически с помощью модуля [кросс-Validate Model (перекрестная проверка модели][cross-validate-model] ). В этом случае вам потребуется набор данных, обученная модель и модуль [перекрестной проверки модели][cross-validate-model] (см. рисунок ниже). Необходимо задать для столбца метка значение *Цена* в свойствах модуля [Перекрестная проверка модели][cross-validate-model] .

![Перекрестная проверка модели регрессии](./media/evaluate-model-performance/3.png)

Рис. 3. Перекрестная проверка модели регрессии.

После выполнения эксперимента можно просмотреть результаты оценки, щелкнув правый порт вывода модуля [Перекрестная проверка модели][cross-validate-model] . Вы увидите подробное представление показателей для каждой итерации (сборки) и усредненные результаты каждого из показателей (рис. 4).

![Результаты перекрестной проверки модели регрессии](./media/evaluate-model-performance/4.png)

Рис. 4 Результаты перекрестной проверки модели регрессии.

## <a name="evaluating-a-binary-classification-model"></a>Оценка модели двоичной классификации
При использовании двоичной классификации целевая переменная имеет только два возможных результата (например, {0, 1} или {ложь, истина}, {отрицательный, положительный}). Предположим, вы получили набор данных о работниках с некоторыми демографическими переменными и переменными их занятости. Вас просят предсказать уровень их доходов. Результат нужно выразить в виде бинарной переменной со значениями {"<=50 000", ">50 000"}. Иными словами, отрицательный класс представляет работников, которые зарабатывают меньше 50 000 в год, а положительный класс представляет всех остальных работников. Как и в сценарии с регрессией, мы должны обучить модель, посчитать некоторые данные и оценивать результаты. Основное отличие заключается в том, что в классической версии Машинное обучение Azure Studio вычисляются и выводятся следующие метрики. Чтобы проиллюстрировать ситуацию прогноза на уровне дохода, мы будем использовать набор данных для [взрослых](https://archive.ics.uci.edu/ml/datasets/Adult) для создания эксперимента Studio (классический) и оценки производительности модели логистической регрессии из двух классов — широко используемого двоичного классификатора.

### <a name="creating-the-experiment"></a>Создание эксперимента
Добавьте следующие модули в рабочую область в классической версии Машинное обучение Azure Studio:

* набор данных Adult Census Income Binary Classification;
* [Логистическая регрессия двух классов][two-class-logistic-regression]
* [Обучение модели][train-model]
* [Модель оценки][score-model]
* [Вычисление модели][evaluate-model]

Подключите порты, как показано ниже на рис. 5, и задайте для столбца метки модуля [обучение модели][train-model] значение *доход*.

![Оценка модели двоичной классификации](./media/evaluate-model-performance/5.png)

Рис. 5. Оценка модели двоичной классификации.

### <a name="inspecting-the-evaluation-results"></a>Проверка результатов оценки
После выполнения эксперимента можно щелкнуть порт вывода модуля [Анализ модели][evaluate-model] и выбрать *визуализировать* , чтобы просмотреть результаты оценки (рис. 7). Для моделей двоичной классификации доступны такие метрики оценки: *Accuracy* (Правильность), *Precision* (Точность), *Recall* (Полнота), *F1 Score* (Оценка F1) и *AUC*. Кроме того, модуль выводит матрицы неточностей, которые отображают число истинно положительных, ложноотрицательных, ложноположительных и истинно отрицательных результатов, а также кривые *ROC*, *Precision/Recall* (Точность и полнота) и *Lift* (Точность прогноза).

Правильность выражается пропорцией правильно классифицированных экземпляров. Это, как правило, первый показатель, который вы видите во время оценки классификатора. Но если тестовые данные не сбалансированы (большинство экземпляров относятся к одному из классов) или вас больше интересует эффективность на одном из классов, правильность не будет отражать фактическую эффективность классификатора. Предположим, вы тестируете в сценарии классификации уровня дохода, данные, в которых 99 % экземпляров представляют людей, которые зарабатывают меньше или ровно 50 000 $ в год. Можно достичь уровня точности 0,99 путем прогнозирования класса «<=50K» для всех экземпляров. Кажется, что классификатор в целом хорошо справляется с заданием, но в действительности он не смог правильно классифицировать ни одно из лиц с высоким уровнем дохода (1 %).

Поэтому будет целесообразно вычислить дополнительные показатели, которые фиксируют более конкретные аспекты оценки. Прежде чем углубляться в подробности таких показателей, важно понять матрицу неточностей оценки двоичной классификации. Метки классов в обучающем наборе могут принимать только два возможных значения, которые обычно называются положительными или отрицательными. Положительные и отрицательные экземпляры, которые классификатор прогнозирует правильно, называются истинно положительными (ИП) и истинно отрицательными (ИО) результатами соответственно. Точно так же неправильно классифицированные экземпляры называются ложно положительными (ЛП) и ложно отрицательными результатами (ЛО). Матрица путаницы — это просто таблица, показывающая количество экземпляров, которые попадают под каждую из этих четырех категорий. Классическая версия Машинное обучение Azure Studio автоматически определяет, какой из двух классов в наборе данных является положительным классом. Если метки класса являются логическими операторами или целыми числами, то экземпляры с метками «истина» или «1» присваиваются положительному классу. Если метки являются строками, например с набором данных о доходах, то метки сортируются в алфавитном порядке, а первый уровень выбирается как отрицательный класс, а второй — как положительный.

![Матрица неточностей двоичной классификации](./media/evaluate-model-performance/6a.png)

Рис. 6. Матрица неточностей двоичной классификации.

Возвращаясь к проблеме классификации доходов, нужно задать несколько оценочных вопросов, которые помогут определить эффективность используемого классификатора. Естественным вопросом является следующее: "от лиц, которые прогнозируется в модель, > 50 K (TP + FP), сколько было правильно классифицировано (TP)?" На этот вопрос можно ответить, взглянув на показатель **точности** модели, который представляет долю правильно классифицированных положительных результатов: ИП / (ИП + ЛП). Другой распространенный вопрос: «Из всех высокооплачиваемых специалистов с доходом > 50 000 $ (ИП + ЛП) скольких классификатор классифицировал правильно (ИП)?» Это значение представлено показателем **полноты** или процента истинно положительных результатов классификатора: ИП / (ИП + ЛП). Вы могли заметить, что существует очевидный компромисс между точностью и полнотой. Например, обрабатывая относительно сбалансированный набор данных, классификатор, который прогнозирует в основном положительные экземпляры, будет иметь высокий уровень полноты, но довольно низкий уровень точности, так как многие отрицательные экземпляры будут неправильно классифицированы из-за большого количества ложно позитивных результатов. Чтобы увидеть график изменения этих двух показателей, щелкните кривую **ТОЧНОСТЬ/ПОЛНОТА** на странице вывода результатов оценки (верхняя левая часть рисунка 7).

![Результаты оценки двоичной классификации](./media/evaluate-model-performance/7.png)

Рис. 7. Результаты оценки двоичной классификации.

Не менее часто используется показатель **оценки F1**, который учитывает и точность, и полноту. Это среднее гармоническое этих двух метрик и вычисляется следующим образом: F1 = 2 (точность x отозвать)/(точность и отзыв). Показатель F1 — это удобный способ обобщения оценки в одно число. Но все-таки рекомендуется смотреть на точность и полноту вместе, чтобы лучше понять поведение классификатора.

Кроме того, вы можете сравнить доли истинно положительных результатов и ложноположительных результатов, представленных кривой **рабочей характеристики приемника (ROC)** и соответствующим значением **площади под ROC-кривой (AUC)** . Чем ближе эта кривая к верхнему левому углу, тем выше эффективность классификатора. То есть речь идет о максимальном проценте истинно положительных результатов и минимальном проценте ложно положительных результатов. Кривые, близкие к диагонали графика, получаются из классификаторов, которые, как правило, делают прогнозы, близкие к случайному угадыванию.

### <a name="using-cross-validation"></a>Использование перекрестной проверки
Как и в примере регрессии, можно выполнить перекрестную проверку для многократного обучения, оценки и оценки разных подмножеств данных автоматически. Аналогичным образом можно использовать модуль [перекрестной проверки модели][cross-validate-model] , обученную модель логистической регрессии и набор данных. Столбец меток должен иметь значение " *доход* " в свойствах модуля [перекрестной проверки модели][cross-validate-model] . После выполнения эксперимента и щелчка правого порта в модуле [перекрестной проверки модели][cross-validate-model] можно увидеть значения метрик двоичной классификации для каждого сгиба, а также среднее и стандартное отклонение каждого из них. 

![Перекрестная проверка модели двоичной классификации](./media/evaluate-model-performance/8.png)

Рис. 8. Перекрестная проверка модели двоичной классификации.

![Результаты перекрестной проверки модели двоичной классификации](./media/evaluate-model-performance/9.png)

Рис. 9. Результаты перекрестной проверки модели двоичной классификации.

## <a name="evaluating-a-multiclass-classification-model"></a>Оценка модели классификации по нескольким классам
В этом эксперименте мы будем использовать популярный набор данных [IRI](https://archive.ics.uci.edu/ml/datasets/Iris "IRI") , который содержит экземпляры трех различных типов (классов) растения. Существует четыре значения компонентов (чашелистика длина, ширина и длина лепестка/ширина) для каждого экземпляра. В предыдущих экспериментах мы обучили и протестировали модели с помощью тех же наборов данных. Здесь мы будем использовать модуль [Split Data (разделение данных][split] ) для создания двух подмножеств данных, обучения по первому и оценке и оценки во втором. Набор данных IRI общедоступен в [машинное обучение репозитории UCI](https://archive.ics.uci.edu/ml/index.html), и его можно загрузить с помощью модуля [Импорт данных][import-data] .

### <a name="creating-the-experiment"></a>Создание эксперимента
Добавьте следующие модули в рабочую область в классической версии Машинное обучение Azure Studio:

* [Импорт данных][import-data]
* [Лес решений в многоклассовых решениях][multiclass-decision-forest]
* [Разделение данных][split]
* [Обучение модели][train-model]
* [Модель оценки][score-model]
* [Вычисление модели][evaluate-model]

Соедините порты, как показано на рисунке 10.

Задайте для индекса столбца метки в модуле [обучение модели][train-model] значение 5. У этого набора данных нет строки заголовка, но мы знаем, что этикетки находятся в пятом столбце.

Щелкните модуль [Импорт данных][import-data] и задайте для свойства *источник данных* значение веб- *URL через HTTP*, а для параметра *URL-адрес* — значение http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data.

Задайте доли экземпляров, которые будут использоваться для обучения в модуле [Split Data][split] (например, 0,7).

![Оценка классификатора с несколькими классами](./media/evaluate-model-performance/10.png)

Рис. Оценка классификатора с несколькими классами

### <a name="inspecting-the-evaluation-results"></a>Проверка результатов оценки
Запустите эксперимент и щелкните порт вывода [вычислить модель][evaluate-model]. В этом случае результаты оценки представлены в виде матрицы неточностей. В матрице показаны фактические и прогнозируемые экземпляры для всех трех классов.

![Результаты оценки классификации по нескольким классам](./media/evaluate-model-performance/11.png)

Рис. 11. Результаты оценки классификации по нескольким классам.

### <a name="using-cross-validation"></a>Использование перекрестной проверки
Как упоминалось ранее, вы можете выполнять многократное обучение, оценку и вычисления автоматически с помощью модуля [кросс-Validate Model (перекрестная проверка модели][cross-validate-model] ). Вам потребуется набор данных, обученная модель и модуль [перекрестной проверки модели][cross-validate-model] (см. рисунок ниже). Опять же, необходимо задать столбец метки модуля [перекрестной проверки модели][cross-validate-model] (в данном случае — индекс столбца 5). После выполнения эксперимента и щелчка правого порта вывода [модели перекрестной проверки][cross-validate-model]можно просмотреть значения метрик для каждого из сверток, а также среднее и стандартное отклонение. Отображаемые здесь показатели похожи на показатели, о которых шла речь в разделе, посвященном двоичной классификации. Однако в многоклассовой классификации вычисление истинных и отрицательных положительных и ложных положительных и отрицательных результатов выполняется путем подсчета для каждого класса, так как отсутствует общий положительный или отрицательный класс. Например, при расчете точности или полноты класса «Ирис щетинистый» предполагается, что это положительный класс, а все остальные являются отрицательными.

![Перекрестная проверка модели классификации по нескольким классам](./media/evaluate-model-performance/12.png)

Рис. 12. Перекрестная проверка модели классификации по нескольким классам.

![Результаты перекрестной проверки модели классификации по нескольким классам](./media/evaluate-model-performance/13.png)

Рис. 13. Результаты перекрестной проверки модели классификации по нескольким классам.

<!-- Module References -->
[cross-validate-model]: https://msdn.microsoft.com/library/azure/75fb875d-6b86-4d46-8bcc-74261ade5826/
[evaluate-model]: https://msdn.microsoft.com/library/azure/927d65ac-3b50-4694-9903-20f6c1672089/
[linear-regression]: https://msdn.microsoft.com/library/azure/31960a6f-789b-4cf7-88d6-2e1152c0bd1a/
[multiclass-decision-forest]: https://msdn.microsoft.com/library/azure/5e70108d-2e44-45d9-86e8-94f37c68fe86/
[import-data]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/
[score-model]: https://msdn.microsoft.com/library/azure/401b4f92-e724-4d5a-be81-d5b0ff9bdb33/
[split]: https://msdn.microsoft.com/library/azure/70530644-c97a-4ab6-85f7-88bf30a8be5f/
[train-model]: https://msdn.microsoft.com/library/azure/5cc7053e-aa30-450d-96c0-dae4be720977/
[two-class-logistic-regression]: https://msdn.microsoft.com/library/azure/b0fd7660-eeed-43c5-9487-20d9cc79ed5d/
