---
title: Оценка и устранение рисков объективности в моделях машинного обучения
titleSuffix: Azure Machine Learning
description: Сведения об объективности в моделях машинного обучения и о том, как пакет Fairlearn Python может помочь в создании более объективных моделей.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: luquinta
author: luisquintanilla
ms.date: 05/02/2020
ms.openlocfilehash: c21ec0329a7b5716a00262b7422296df3afe208b
ms.sourcegitcommit: bb0afd0df5563cc53f76a642fd8fc709e366568b
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/19/2020
ms.locfileid: "83596363"
---
# <a name="fairness-in-machine-learning-models"></a>Объективность в моделях машинного обучения

Сведения об объективности в машинном обучении и о том, как пакет Fairlearn Python с открытым кодом может помочь в создании более объективных моделей.

## <a name="what-is-fairness-in-machine-learning-systems"></a>Объективность в системах машинного обучения

Системы машинного обучения и искусственного интеллекта могут вести себя не объективно. Одним из способов определения необъективного поведения является его вредное воздействие или влияние на людей. Существует множество типов вредного воздействия, к которым может привести использование системы ИИ. Два распространенных типа вредного воздействия, причинами которого является искусственный интеллект:

- Вредное воздействие распределения. Система ИИ расширяет или ограничивает возможности, ресурсы или сведения. В качестве примеров можно привести наем сотрудников, прием в учебные заведения и кредитование, в которых модель при выборе подходящих кандидатов может отдавать предпочтение определенной группе людей.

- Вредное воздействие качества обслуживания. Система ИИ не работает одинаково для всех групп людей. Например, система распознавания речи может не работать так же хорошо для женщин, как для мужчин.

Чтобы снизить необъективное поведение в системах ИИ, необходимо оценить и устранить эти вредные воздействия.

>[!NOTE]
> Объективность — это социально-техническая задача. Многие аспекты объективности, такие как справедливость и правосудие, не сохраняются в количественных метриках объективности. Кроме того, многие количественные метрики объективности не могут выполняться одновременно. Основная цель — позволить пользователям оценивать различные стратегии устранения рисков, а затем найти компромисс, соответствующий сценарию.

## <a name="fairness-assessment-and-mitigation-with-fairlearn"></a>Оценка и устранение рисков объективности с помощью Fairlearn

Fairlearn — это пакет Python с открытым кодом, позволяющий разработчикам систем машинного обучения оценивать объективность своих систем и устранять наблюдаемые проблемы объективности.

Fairlearn состоит из двух компонентов:

- Панель мониторинга оценки. Мини-приложение записной книжки Jupyter для выполнения оценки влияния прогнозирования модели на различные группы. Оно также позволяет сравнивать несколько моделей, используя метрики объективности и производительности.
- Алгоритм устранения рисков. Набор алгоритмов для снижения необъективности в двоичной классификации и регрессии.

Вместе эти компоненты позволяют специалистам по обработке и анализу данных и руководителям достигать компромисса между объективностью и производительностью, а также определять подходящую стратегию устранения рисков.

## <a name="fairness-assessment"></a>Оценка объективности

В Fairlearn объективность выражается через **групповую справедливость**, которая определяет, какие группы лиц подвергаются риску в случае вредного воздействия.

Соответствующие группы, также известны как подгруппы, определяются с помощью **конфиденциальных функций** или конфиденциальных атрибутов. Конфиденциальные функции передаются в оценщик Fairlearn в виде вектора или матрицы `sensitive_features`. Термин предполагает, что конструктор систем должен быть чувствительным к этим функциям при оценке объективности группы. Следует помнить, что эти функции могут влиять на конфиденциальность из-за личных сведений. Но слово "конфиденциальный" не подразумевает, что эти функции не могут использоваться для создания прогнозов.

На этапе оценки объективность измеряется с помощью метрик различия. **Метрики различий** могут оценивать и сравнивать поведение модели в разных группах как в виде соотношений, так и в виде отличий. Fairlearn поддерживает два класса метрик различий:


- Различия в производительности модели. Эти наборы метрик вычисляют различие (отличие) в значениях выбранной метрики производительности в разных подгруппах. Некоторые примеры:

  - различие в степени правильности;
  - различие в частоте ошибок;
  - различие в точности;
  - различие в полноте;
  - различие в средней абсолютной погрешности (MAE);
  - многое другое.

- Различия в квоте отбора. В этой метрике содержатся отличия в квоте отбора между разными подгруппами. Примером этого может быть различие в квоте одобрения кредита. Квота выбора подразумевает долю точек данных в каждом классе, классифицированном как 1 (в двоичной классификации), или распределение значений прогнозирования (в регрессии).

## <a name="unfairness-mitigation"></a>Устранение рисков необъективности

### <a name="parity-constraints"></a>Ограничения четности

Fairlearn включает различные алгоритмы устранения рисков необъективности. Эти алгоритмы поддерживают набор ограничений для поведения прогнозирования, которое называется **ограничением четности** или критерием. Для ограничений четности требуется, чтобы некоторые аспекты поведения прогнозирования сравнивались между группами, определяемыми конфиденциальными функциями (например, разные состояния гонки). Алгоритмы Fairlearn для устранения рисков используют такие ограничения четности, чтобы уменьшить проблемы объективности.

Fairlearn поддерживает следующие типы ограничений четности:

|Ограничение четности  | Назначение  |Задача машинного обучения  |
|---------|---------|---------|
|Демографическое равенство     |  Устранение вредных воздействий на распределение | Двоичная классификация, регрессия |
|Уравнивание шансов  | Диагностика причин вредных воздействий распределения и качества обслуживания | Двоичная классификация        |
|Потери ограниченной группы     |  Устранение вредного воздействие качества обслуживания | Регрессия |

### <a name="mitigation-algorithms"></a>Алгоритм устранения рисков

Fairlearn предоставляет алгоритмы устранения рисков необъективности при постобработке и сокращении:

- Сокращение. Эти алгоритмы принимают стандартный оценщик Машинного обучения по принципу "черного ящика" (например, модель LightGBM) и создают набор повторно обученных моделей с помощью последовательности повторно взвешенных наборов данных для обучения. Например, кандидаты определенного пола могут быть завышено взвешенными или занижено взвешенными, чтобы переучить модели и уменьшить различия в разных группах по половой принадлежности. Затем пользователи могут выбрать модель, обеспечивающую оптимальный компромисс между правильностью (или другой метрикой производительности) и различием, которая обычно должна основываться на бизнес-правилах и вычислениях затрат.  
- Постобработка. Эти алгоритмы принимают в качестве входных данных существующий классификатор и конфиденциальную функцию. Затем они наследуют преобразование прогнозирования классификатора, чтобы применить указанные ограничения объективности. Главным преимуществом оптимизации порогов является простота и гибкость, так как не требуется переучить модель. 

| Алгоритм | Описание | Задача машинного обучения | Конфиденциальные функции | Поддерживаемые ограничения четности | Тип алгоритма |
| --- | --- | --- | --- | --- | --- |
| `ExponentiatedGradient` | Сведения о справедливой классификации с использованием подхода по принципу "черный ящик" см. в [этой статье](https://arxiv.org/abs/1803.02453) | Двоичная классификация | категориальные; | [Демографическое равенство](#parity-constraints), [уравнивание шансов](#parity-constraints) | Сокращение |
| `GridSearch` | Подход с использованием принципа "черный ящик" описан в статье [Подход сокращения к справедливой классификации](https://arxiv.org/abs/1803.02453)| Двоичная классификация | Двоичные данные | [Демографическое равенство](#parity-constraints), [уравнивание шансов](#parity-constraints) | Сокращение |
| `GridSearch` | Подход с использованием принципа "черный ящик", который реализует вариант поиска в сетке справедливой регрессии с помощью алгоритма для потери ограниченной группы, описывается в [этой статье](https://arxiv.org/abs/1905.12843) | Регрессия | Двоичные данные | [Потери ограниченной группы](#parity-constraints) | Сокращение |
| `ThresholdOptimizer` | Сведения о постобработке на основе алгоритма см. в статье [Равенство возможностей в контролируемом обучении](https://arxiv.org/abs/1610.02413). Этот метод принимает в качестве входных данных существующий классификатор и конфиденциальную функцию и создает монотонное преобразование прогнозирования классификатора, чтобы применить указанные ограничения четности. | Двоичная классификация | категориальные; | [Демографическое равенство](#parity-constraints), [уравнивание шансов](#parity-constraints) | постобработка. |

## <a name="next-steps"></a>Дальнейшие действия

- Для получения сведений об использовании различных компонентов ознакомьтесь с [репозиторием GitHub Fairlearn](https://github.com/fairlearn/fairlearn/) и [примерами записных книжек](https://github.com/fairlearn/fairlearn/tree/master/notebooks).
- Сведения сохранении конфиденциальности данных см. в статье [Сохранение конфиденциальности данных с помощью дифференциальной конфиденциальности и пакета WhiteNoise](concept-differential-privacy.md).