---
title: Ответственное машинное обучение (ML)
titleSuffix: Azure Machine Learning
description: Сведения о концепции ответственного машинного обучения и возможностях его применения в Машинном обучении Azure
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: luquinta
author: luisquintanilla
ms.date: 05/08/2020
ms.openlocfilehash: 3cef3c2179019f6d84de5596e61abaf8d7d3182c
ms.sourcegitcommit: bb0afd0df5563cc53f76a642fd8fc709e366568b
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/19/2020
ms.locfileid: "83595013"
---
# <a name="responsible-machine-learning-ml"></a>Ответственное машинное обучение (ML)

Из этой статьи вы узнаете о том, что представляет собой ответственное машинное обучение и как применить его на практике в Машинном обучении Azure.

На всех этапах разработки и использования систем искусственного интеллекта обязательным условием является доверие. Доверие к платформе, ко всем процессам и моделям. В корпорации Майкрософт ответственный подход к машинному обучению проявляется в следующих ценностях и принципах:

- понимание моделей машинного обучения;
  - анализ и объяснение поведения модели;
  - оценка и устранение необъективности модели;
- защита пользователей и их данных;
  - предотвращение раскрытия данных с применением дифференциальной конфиденциальности;  
- полный контроль процесса машинного обучения;
  - документирование жизненного цикла машинного обучения в таблицах данных.

:::image type="content" source="media/concept-responsible-ml/responsible-ml-pillars.png" alt-text="Основы ответственного машинного обучения":::

Так как искусственный интеллект и автономные системы все глубже проникают в структуру общества, важно своевременно принять меры по прогнозированию и предотвращению нежелательных последствий применения этих технологий.

## <a name="interpret-and-explain-model-behavior"></a>Анализ и объяснение поведения модели

Сложно объяснимые и непрозрачные системы могут создавать проблемы, так как всем заинтересованным лицам, таким как разработчики систем, регулирующие органы, пользователи и руководители, ответственные за принятие решений, будет трудно понять, на чем основываются решения таких систем. Некоторые системы искусственного интеллекта более понятны, чем другие, а иногда приходится выбирать между системой с более высокой точностью или большей понятностью.

Чтобы создавать хорошо объяснимые системы искусственного интеллекта, применяйте пакет с открытым кодом [InterpretML](https://github.com/interpretml/interpret), созданный корпорацией Майкрософт. [InterpretML можно использовать на платформе "Машинное обучение Azure"](how-to-machine-learning-interpretability.md) для [интерпретации и объяснения моделей машинного обучения](how-to-machine-learning-interpretability-aml.md), в том числе [моделей автоматизированного машинного обучения](how-to-machine-learning-interpretability-automl.md).

## <a name="assess-and-mitigate-model-unfairness"></a>Оценка и устранение необъективности модели

По мере того, как системы искусственного интеллекта все больше участвуют в принятии повседневных решений, становится крайне важным непредвзятое поведение таких систем со справедливым распределением результатов.

Необъективность в системах искусственного интеллекта может привести к целому ряду нежелательных последствий:

- ограничение доступа некоторых лиц к возможностям, ресурсам или сведениям;
- усиление смещения и стереотипов.

Многие аспекты объективности невозможно оценить или выразить в метриках. Существуют средства и методики, которые повышают объективность систем искусственного интеллекта при их проектировании и разработке.

Два основных этапа для снижения необъективности в системах ИИ — это оценка и устранение рисков. Мы рекомендуем использовать пакет [FairLearn](https://github.com/fairlearn/fairlearn) с открытым кодом, который умеет оценивать и устранять потенциальную необъективность в системах искусственного интеллекта. Дополнительные сведения о объективности в машинном обучении и пакете FairLearn см. в [этой статье](./concept-fairness-ml.md).

## <a name="prevent-data-exposure-with-differential-privacy"></a>Предотвращение раскрытия данных с применением дифференциальной конфиденциальности

Если для анализа используются некоторые данные, очень важно сохранять их конфиденциальность и приватность на всем протяжении использования. Дифференциальная конфиденциальность — это набор систем и рекомендаций, которые помогают обеспечить безопасность и конфиденциальность данных частных лиц.

В традиционных сценариях необработанные данные хранятся в файлах и базах данных. При анализе данных пользователи обычно используют необработанные данные. Это является проблемой из-за возможного нарушения конфиденциальности личности. Благодаря дифференциальной конфиденциальности можно решить эту проблему, добавив в данные "шум" или случайность, чтобы пользователи не могли определить отдельные точки данных.

Реализация систем с дифференциальной конфиденциальностью — это сложный процесс. [WhiteNoise](https://github.com/opendifferentialprivacy/whitenoise-core) — это проект с открытым кодом, который содержит различные компоненты для создания глобальных систем с дифференциальной конфиденциальностью. Дополнительные сведения о дифференциальной конфиденциальности и проекте WhiteNoise см. в статье [о сохранении конфиденциальности данных благодаря дифференциальной конфиденциальности и WhiteNoise](./concept-differential-privacy.md).

## <a name="document-the-machine-learning-lifecycle-with-datasheets"></a>Документирование жизненного цикла машинного обучения в таблицах данных

Документирование важной информации о процессе машинного обучения является важнейшим условием для принятия взвешенных решений на каждом этапе. Таблицы данных предоставляют способ для документирования ресурсов машинного обучения, которые используются и создаются в рамках жизненного цикла машинного обучения.

Модели обычно воспринимаются как "черные ящики", информации о которых очень мало. Но системы машинного обучения становятся все более распространенными и применяются для принятия многих решений, поэтому использование таблиц данных станет важным шагом по разработке более ответственных систем машинного обучения.

Вот некоторые сведения о модели, которые вы можете документировать в таблицах данных:

- предполагаемое использование;
- архитектура модели;
- использованные для обучения данные;
- использованные для оценки данные;
- метрики производительности модели обучения;
- сведения об объективности.

Следующий пример демонстрирует, как можно применить пакет средств разработки Машинного обучения Azure для реализации [таблиц данных для моделей](https://github.com/microsoft/MLOps/blob/master/pytorch_with_datasheet/model_with_datasheet.ipynb).

## <a name="additional-resources"></a>Дополнительные ресурсы

- Изучите набор рекомендаций [ABOUT ML](https://www.partnershiponai.org/about-ml/) по документированию систем машинного обучения.