---
title: Платформы и средства для проектов обработки и анализа данных — командный процесс обработки и анализа данных
description: Список и описание ресурсов для получения и анализа данных, которые предприятия могут использовать для стандартизации командного процесса обработки и анализа данных.
author: marktab
manager: marktab
editor: marktab
ms.service: machine-learning
ms.subservice: team-data-science-process
ms.topic: article
ms.date: 01/10/2020
ms.author: tdsp
ms.custom: seodec18, previous-author=deguhath, previous-ms.author=deguhath
ms.openlocfilehash: e3297319c67ad2b7c94371356cde49113c7ef737
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "79251613"
---
# <a name="platforms-and-tools-for-data-science-projects"></a>Платформы и средства для проектов обработки и анализа данных

Корпорация Майкрософт предоставляет полный спектр аналитических ресурсов как для облачных, так и для платформ. Развертывание этих ресурсов повысит эффективность и масштабируемость выполнения проектов по обработке и анализу данных. Для команд, выполняющих проекты по обработке и анализу данных, предлагается [командный процесс обработки и анализа данных](overview.md) (TDSP), который обеспечивает отслеживаемость, контроль версий и совместную работу.  Роли и задачи сотрудников в области стандартизации обработки и анализа данных описаны в статье [Team Data Science Process roles and tasks](roles-tasks.md) (Роли и задачи в командном процессе обработки и анализа данных).

Ресурсы аналитики, доступные группам по анализу данных, использующим TDSP, включают:

- виртуальные машины для обработки и анализа данных (Windows и Linux CentOS);
- кластеры Spark в HDInsight;
- Synapse Analytics
- Azure Data Lake;
- кластеры Hive в HDInsight;
- Хранилище файлов Azure
- Услуги S'L Server 2019 R и Python
- Azure Databricks

В этом документе мы кратко опишем эти ресурсы и приведем ссылки на руководства и пошаговые инструкции, опубликованные командами TDSP. Они помогут вам постепенно изучить все ресурсы и успешно применить их для создания интеллектуальных приложений. Дополнительные сведения об этих ресурсах можно найти на страницах соответствующих продуктов. 

## <a name="data-science-virtual-machine-dsvm"></a>Виртуальная машина для обработки и анализа данных

Виртуальная машина для обработки и анализа данных, которую корпорация Майкрософт предоставляет для ОС Windows и Linux, содержит набор популярных средств моделирования и разработки для систем обработки и анализа данных. Среди прочего, она оснащена такими средствами:

- Microsoft R Server Developer Edition 
- дистрибутив Anaconda Python;
- записные книжки Jupyter для Python и R; 
- Visual Studio Community Edition в комплекте со средствами Python и R для Windows (или Eclipse для Linux);
- Power BI Desktop для Windows;
- SQL Server 2016 Developer Edition для Windows (или Postgres для Linux).

Она также включает в себя **ML и ИИ инструменты,** такие как xgboost, mxnet, и Vowpal Wabbit.

Сейчас виртуальная машина для обработки и анализа данных доступна для операционных систем **Windows** и **Linux CentOS**. Выберите нужный размер виртуальной машины для обработки и анализа данных (число ядер и размер памяти), исходя из потребностей проектов обработки и анализа данных, которые вы будете на ней выполнять. 

Для получения дополнительной информации о Windows издание DSVM, см [Microsoft Data Science Виртуальная машина](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-dsvm.dsvm-windows) на Azure Marketplace. Выпуск виртуальной машины для обработки и анализа данных для Linux [описан здесь](https://azure.microsoft.com/marketplace/partners/microsoft-ads/linux-data-science-vm/).

Чтобы узнать, как выполнить некоторые из общих задач науки о данных на DSVM эффективно, увидеть [10 вещей, которые вы можете сделать на данные науки Виртуальная машина](../data-science-virtual-machine/vm-do-ten-things.md)


## <a name="azure-hdinsight-spark-clusters"></a>Кластеры Spark в Azure HDInsight

Apache Spark — это платформа параллельной обработки с открытым кодом, которая поддерживает обработку в памяти, чтобы повысить производительность приложений для анализа больших данных. Подсистема обработки Spark призвана ускорить разработку, повысить удобство использования и реализовать сложную аналитику. Возможности вычисления в памяти позволяют Spark эффективно применять итеративные алгоритмы в машинном обучении и графовых вычислениях. Подсистема Spark также совместима с хранилищем BLOB-объектов Azure (WASB), поэтому сможет легко обрабатывать существующие данные, хранящиеся в Azure.

При создании кластера Spark в HDInsight создание вычислительных ресурсов Azure следует выполнять после установки и настройки Spark. Создание кластера Spark в HDInsight займет около 10 минут. Разместите данные для обработки в хранилище BLOB-объектов Azure. Сведения об использовании хранилища BLOB-объектов Azure совместно с кластером см. в разделе [Использование службы хранилища Azure с кластерами Azure HDInsight](../../hdinsight/hdinsight-hadoop-use-blob-storage.md).

Команда TDSP корпорации Майкрософт опубликовала два полных пошаговых руководства (одно для Python, другое — для Scala) по использованию кластеров Spark в Azure HDInsight для создания решений по обработке и анализу данных. Дополнительные сведения о **кластерах Spark** в Azure HDInsight см. в статье [Общие сведения о Spark в HDInsight](../../hdinsight/spark/apache-spark-overview.md). Чтобы узнать, как с помощью **Python** создать решение по обработке и анализу данных в кластере Spark в Azure HDInsight, см. статью [Общие сведения об обработке и анализе данных с помощью платформы Spark в Azure HDInsight](spark-overview.md). Чтобы узнать, как с помощью **Scala** создать решение по обработке и анализу данных в кластере Spark в Azure HDInsight, см. статью [Обработка и анализ данных с использованием Scala и Spark в Azure](scala-walkthrough.md). 


##  <a name="azure-sql-data-warehouse"></a>Хранилище данных SQL Azure

Хранилище данных SQL Azure позволяет легко масштабировать вычислительные ресурсы за считаные секунды, не выделяя лишних ресурсов и не переплачивая за их использование. Также оно предоставляет уникальную возможность приостановить использование вычислительных ресурсов, обеспечивая более гибкое управление затратами на облачные решения. Возможность развертывать масштабируемые вычислительные ресурсы позволяет перенести все данные в хранилище данных SQL Azure. Затраты на хранение здесь минимальны, а вычисления можно выполнять только для тех сегментов наборов данных, которые нужны вам для анализа. 

Дополнительные сведения о хранилище данных SQL Azure см. на [веб-странице](https://azure.microsoft.com/services/sql-data-warehouse), посвященной этой службе. Чтобы научиться создавать комплексные решения для углубленной аналитики на базе хранилища данных SQL, см. статью [Командный процесс обработки и анализа данных на практике: использование хранилища данных SQL](sqldw-walkthrough.md).


## <a name="azure-data-lake"></a>Azure Data Lake;

Azure Data Lake — это как общекорпоративное хранилище всех типов данных, собранных в одном месте, до введения каких-либо формальных требований или схемы. Такая гибкость позволяет хранить в репозитории данные любого типа, любого размера и структуры, и принимать их с любой скоростью. Организации могут применить Hadoop или углубленную аналитику для поиска закономерностей в репозиториях типа Data Lake. Репозитории типа Data Lake могут также служить бюджетными репозиториями для подготовки данных перед их очисткой и переносом в хранилище данных.

Дополнительные сведения об Azure Data Lake см. в записи блога [Introducing Azure Data Lake](https://azure.microsoft.com/blog/introducing-azure-data-lake/) (Знакомство с Azure Data Lake). Чтобы узнать, как создать комплексное решение для обработки и анализа данных на базе Azure Data Lake, изучите [полное пошаговое руководство по масштабируемому анализу данных с помощью Azure Data Lake](data-lake-walkthrough.md).


## <a name="azure-hdinsight-hive-hadoop-clusters"></a>Кластеры Hive (Hadoop) в Azure HDInsight

Apache Hive — это система хранилища данных для Hadoop, которая позволяет обобщать и анализировать данные, а также обрабатывать запросы с использованием HiveQL (язык запросов, подобный SQL). Hive можно использовать для интерактивного исследования данных или создания многократно используемых заданий пакетного задания обработки.

Hive позволяет создавать структуру для преимущественно неструктурированных данных. Определив такую структуру, вы сможете использовать Hive для отправки запросов к данным в кластере Hadoop. Для этого не нужно изучать Java или MapReduce. HiveQL (язык запросов Hive) позволяет создавать запросы, используя операторы, подобные операторам T-SQL.

Hive позволяет включать пользовательские функции Python в запросы Hive для обработки и анализа данных в записях. Так вы сможете значительно расширить потенциал запросов Hive при анализе данных. В частности, специалисты по обработке и анализу данных смогут создавать масштабируемые функции на языках, с которыми они лучше всего знакомы: HiveQL, который похож на SQL, и Python. 

Дополнительные сведения о кластерах Hive в Azure HDInsight см. в статье [Использование Hive и HiveQL с Hadoop в HDInsight](../../hdinsight/hadoop/hdinsight-use-hive.md). Чтобы научиться создавать масштабируемые комплексные решения для обработки и анализа данных на базе кластеров Hive в Azure HDInsight, см. статью [Командный процесс обработки и анализа данных на практике: использование хранилища данных SQL](hive-walkthrough.md).


## <a name="azure-file-storage"></a>Хранилище файлов Azure 

Хранилище файлов Azure — это служба, которая предоставляет доступ к общим папкам в облаке с использованием стандартного протокола SMB. Поддерживаются версии SMB 2.1 и SMB 3.0. Хранилище файлов Azure позволяет быстро и без дорогостоящей перезаписи выполнить перенос приложений прежних версий, связанных с общими папками. Приложения, работающие на виртуальных машинах Azure, в облачных службах или на локальных клиентах, могут подключать общую папку в облаке так же, как настольное приложение подключает обычную общую папку SMB. Любое количество компонентов приложений может одновременно подключаться и получать доступ к ресурсам хранилища файлов.

Особенно полезной для проектов по обработке и анализу данных будет возможность создать хранилище файлов Azure для совместного использования всеми участниками команды проекта. Все специалисты будут обращаться к одной и той же копии данных, размещенной в хранилище файлов Azure. Также с помощью этого хранилища файлов они смогут совместно использовать наборы функций, созданные во время выполнения проекта. Если проект предусматривает взаимодействие с клиентом, клиент может создать хранилище файлов Azure в своей подписке Azure и разместить в нем данные и компоненты проекта, чтобы предоставить вам доступ к ним. Это позволит клиенту сохранить полный контроль над ресурсами данных, используемыми в проекте. Дополнительные сведения о хранилище файлов Azure см. в статьях [Разработка для службы файлов Azure с помощью .NET](https://azure.microsoft.com/documentation/articles/storage-dotnet-how-to-use-files) и [Использование файлов Azure в Linux](../../storage/files/storage-how-to-use-files-linux.md).


## <a name="sql-server-2019-r-and-python-services"></a>Услуги S'L Server 2019 R и Python

R Services (In-database) предоставляет платформу для разработки и развертывания интеллектуальных приложений, которые могут раскрыть новые идеи. Вы можете использовать полнофункциональный мощный язык R и множество пакетов, созданных сообществом R, чтобы разрабатывать модели и формировать прогнозы на основе данных, хранящихся в SQL Server. Поскольку R Services (In-database) интегрирует язык R с сервером S'L, аналитика хранится близко к данным, что устраняет затраты и риски безопасности, связанные с перемещением данных.

R Services (In-database) поддерживает язык R с открытым исходным кодом с помощью полного набора инструментов и технологий S'L Server. Они обеспечивают высокую производительность, безопасность, надежность и управляемость. Для развертывания решений R вам доступны удобные и знакомые средства. Ваши рабочие приложения могут вызывать среду выполнения R, получать прогнозы и визуальные элементы с использованием Transact-SQL. Кроме того, вы можете использовать библиотеки ScaleR для увеличения масштаба и производительности решений R. Для получения более подробной информации ознакомьтесь с [услугами сервера R.](https://docs.microsoft.com/sql/advanced-analytics/r/sql-server-r-services)

Команда TDSP корпорации Майкрософт опубликовала два полных пошаговых руководства (одно для R-программистов, другое — для разработчиков SQL) по созданию решений для обработки и анализа данных в службах R для SQL Server 2016. Для **R-программистов** предлагается [полное пошаговое руководство по обработке и анализу данных](https://docs.microsoft.com/sql/advanced-analytics/tutorials/walkthrough-data-science-end-to-end-walkthrough). Для **разработчиков SQL** будет полезно руководство по [аналитике в базе данных R для разработчиков SQL](https://docs.microsoft.com/sql/advanced-analytics/tutorials/sqldev-in-database-r-for-sql-developers).


## <a name="appendix-tools-to-set-up-data-science-projects"></a><a name="appendix"></a>Приложение. Средства для настройки проектов по обработке и анализу данных

### <a name="install-git-credential-manager-on-windows"></a>Установка диспетчера учетных данных Git в Windows

Если вы организуете процесс TDSP на **Windows**, вам потребуется **диспетчер учетных данных Git (GCM)** для обмена данными с репозиториями Git. Перед установкой GCM необходимо сначала установить **Chocolaty**. Чтобы установить Chocolaty и GCM, выполните следующие команды в Windows PowerShell **с правами администратора**.  

    iwr https://chocolatey.org/install.ps1 -UseBasicParsing | iex
    choco install git-credential-manager-for-windows -y
    

### <a name="install-git-on-linux-centos-machines"></a>Установка Git на компьютерах Linux (CentOS)

Выполните следующую команду в оболочке bash, чтобы установить Git на компьютере Linux (CentOS):

    sudo yum install git


### <a name="generate-public-ssh-key-on-linux-centos-machines"></a>Создание открытого ключа SSH на компьютерах Linux (CentOS)

Если для выполнения команд Git вы используете компьютер Linux (CentOS), на нем необходимо установить открытый ключ SSH, чтобы службы Azure DevOps Services распознали ваш компьютер. Для этого сначала нужно создать этот открытый ключ SSH, а затем добавить его в список открытых ключей SSH на странице настроек безопасности для Azure DevOps Services. 

1. Чтобы создать ключ SSH, выполните следующие две команды. 

   ```
   ssh-keygen
   cat .ssh/id_rsa.pub
   ```
   
   ![Команды генерации ключа SSH](./media/platforms-and-tools/resources-1-generate_ssh.png)

1. Скопируйте полный текст ключа SSH, включая *ssh-rsa*. 
1. Войдите в Azure DevOps Services. 
1. Нажмите **<\> Ваше имя** в правом верхнем углу страницы и нажмите **безопасности**. 
    
   ![Щелкните свое имя и выберите команду "Безопасность"](./media/platforms-and-tools/resources-2-user-setting.png)

1. Щелкните **Открытые ключи SSH** и нажмите кнопку **+Добавить**. 

   ![Щелкните "Открытые ключи SSH" и нажмите кнопку "+Добавить"](./media/platforms-and-tools/resources-3-add-ssh.png)

1. Вставьте ключ ssh скопированный в текстовое поле и сохраните.


## <a name="next-steps"></a>Дальнейшие действия

Также предоставляются полные пошаговые руководства, которые демонстрируют все этапы процесса для **конкретных сценариев** . Эти этапы с иллюстрациями и краткими описаниями перечислены в [примерах пошаговых руководств](walkthroughs.md). В них показано, как объединить облачные и локальные средства и службы в единый рабочий процесс или конвейер, чтобы создать интеллектуальное приложение. 

Например, как можно выполнить этапы в процессе обучения данных Team [With Azure ML](https://docs.microsoft.com/azure/machine-learning/team-data-science-process/) Data, с помощью студии машинного обучения Azure (классический), см.
