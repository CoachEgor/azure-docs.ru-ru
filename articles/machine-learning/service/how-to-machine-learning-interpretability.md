---
title: Интерпретируемость модели в службе Машинное обучение Azure
titleSuffix: Azure Machine Learning
description: Узнайте, как объяснить, почему модель выполняет прогнозы с помощью пакета SDK для Машинное обучение Azure. Его можно использовать во время обучения и вывода, чтобы понять, как модель выполняет прогнозы.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: mesameki
author: mesameki
ms.reviewer: trbye
ms.date: 10/25/2019
ms.openlocfilehash: 5f1008e8fcbbf7b82a694fd151a9dea9ca7f001e
ms.sourcegitcommit: c22327552d62f88aeaa321189f9b9a631525027c
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/04/2019
ms.locfileid: "73515347"
---
# <a name="model-interpretability-in-azure-machine-learning-service"></a>Интерпретируемость модели в службе Машинное обучение Azure
[!INCLUDE [applies-to-skus](../../../includes/aml-applies-to-basic-enterprise-sku.md)]

## <a name="overview-of-model-interpretability"></a>Общие сведения о интерпретируемости модели

Интерпретируемость очень важна для специалистов по обработке данных и руководителей, отвечающих за принятие бизнес-решений, чтобы обеспечить соответствие политикам компании, отраслевым стандартам и государственным нормам.
+ Специалистам по обработке и анализу данных необходима возможность объяснить свои модели руководителям и заинтересованным лицам, чтобы они могли понять ценность и точность их результатов. 
+ Руководителям, ответственным за принятие бизнес-решений, требуется возможность предоставлять прозрачность пользователям для получения и обслуживания своих доверий.

Включение возможности объяснения модели машинного обучения важно в ходе двух основных этапов разработки модели:
+ На этапе обучения цикла разработки модели машинного обучения. Конструкторы моделей и оценивающие могут использовать результаты интерпретации модели для проверки наличия этих данных и создания отношений доверия с заинтересованными лицами. Они также используют аналитические данные модели для отладки, проверки поведения модели в соответствии с целями, а также для проверки на наличие или несущественных функций.
+ На этапе возникновения проблемы, так как наличие прозрачности в развернутых моделях позволяет руководителям понять, как работает модель, и как ее решения будут рассматриваться и повлиять на людей в реальном времени. 

## <a name="interpretability-with-azure-machine-learning"></a>Интерпретируемость с помощью Машинное обучение Azure

В этой статье вы узнаете, как концепции интерпретации модели реализуются в пакете SDK.

С помощью классов и методов в пакете SDK можно получить:
+ Значения важности функций для необработанных и инженерных функций
+ Интерпретируемость реальных наборов данных в масштабе, во время обучения и вывода.
+ Интерактивные визуализации, помогающие в обнаружении закономерностей в данных и объяснениях во время обучения


В машинном обучении **функции** — это поля данных, используемые для прогнозирования целевой точки данных. Например, для прогнозирования кредитного риска можно использовать поля данных для возраста, размера учетной записи и возраста учетной записи. В этом случае срок хранения, размер учетной записи и возраст учетной записи являются **функциями**. Важность функции сообщает, как каждое поле данных затронуло прогнозы модели. Например, возраст может сильно использоваться в прогнозе, тогда как размер и возраст учетной записи не влияют на точность точности прогнозов. Этот процесс позволяет специалистам по обработке и анализу данных объяснить результирующие прогнозы, чтобы заинтересованные лица могли видеть, какие точки данных наиболее важны в модели.

С помощью этих средств можно глобально описать модели машинного обучения **для всех данных**или **локально на определенной точке данных** , используя современные технологии в удобном для использования и масштабируемом режиме.

Доступ к классам интерпретации предоставляется через несколько пакетов SDK. Узнайте, как [установить пакеты SDK для машинное обучение Azure](https://docs.microsoft.com/python/api/overview/azure/ml/install?view=azure-ml-py).

* `azureml.interpret`, основной пакет, содержащий функциональные возможности, поддерживаемые корпорацией Майкрософт.

* `azureml.contrib.interpret`, предварительная версия и экспериментальные функции, которые можно попробовать.

* `azureml.train.automl.automlexplainer` пакет для интерпретации моделей автоматического машинного обучения.

> [!IMPORTANT]
> Содержимое в пространстве имен `contrib` не полностью поддерживается. По мере того, как экспериментальные функциональные возможности становятся более зрелыми, они постепенно перемещаются в основное пространство имен.

## <a name="how-to-interpret-your-model"></a>Как интерпретировать модель

Вы можете применить классы и методы интерпретации для понимания глобального поведения модели или конкретных прогнозов. Первое из них называется глобальным объяснением, а второе — локальным объяснением.

Методы также можно классифицировать в зависимости от того, является ли метод независимым от модели или зависящим от модели. Некоторые методы предназначены для определенных типов моделей. Например, пояснение к дереву ШАП применимо только к моделям на основе дерева. Некоторые методы обрабатывают модель в виде черного прямоугольника, такого как пояснение к процедуре "имитировать" или ШАП. Пакет `interpret` использует различные подходы на основе наборов данных, типов моделей и вариантов использования.

Выходные данные представляют собой набор сведений о том, как данная модель делает прогноз, например:
* Глобальное/локальное относительное важность функции
* Отношение "глобальный/локальный" и "прогноз"

### <a name="explainers"></a>Пояснения

Этот пакет использует методики интерпретации, разработанные в [интерпретации сообщества](https://github.com/interpretml/interpret-community/), пакет Python с открытым исходным кодом для обучения интерпретируемых моделей и помогая объяснить блаккбокс AI-системы. [Интерпретатор-сообщество](https://github.com/interpretml/interpret-community/) выступает в качестве узла для поддерживаемых объяснений SDK и в настоящее время поддерживает следующие методы интерпретации:

* **Пояснение**к дереву ШАП: пояснение к дереву [ШАП](https://github.com/slundberg/shap), которое посвящено многозначному алгоритму оценки значений времени ШАП, характерному для деревьев и это совокупности деревьев.
* **ШАП глубокое пояснение**: на основе объяснения [ШАП](https://github.com/slundberg/shap), глубокого объяснения — это алгоритм аппроксимации с высокой скоростью для значений ШАП в моделях глубокого обучения, который строится на связи с диплифт, описанным в [документе ШАП НИПС](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions). Модели TensorFlow и модели keras с использованием серверной части TensorFlow поддерживаются (также доступна предварительная поддержка PyTorch).
* **ШАП линейное объяснение**: линейное пояснение [ШАП](https://github.com/slundberg/shap)рассчитывает значения ШАП для линейной модели, при необходимости учитывая корреляцию между функциями.

* **Пояснение ядра ШАП**. в объяснении ядра [ШАП](https://github.com/slundberg/shap)используется специально взвешенная локальная линейная регрессия для оценки значений ШАП для любой модели.
* **Пояснение**к принципу имитации: концепция имитируется на основе представления [глобальных суррогатных моделей](https://christophm.github.io/interpretable-ml-book/global.html) для имитации блаккбокс моделей. Глобальная суррогатная модель — это внутренняя интерпретируемая модель, которая обучена для приблизительных прогнозов модели с черным прямоугольником как можно точнее. Анализу данных может интерпретировать суррогатную модель, чтобы рисовать выводы о модели черного ящика. В качестве суррогатной модели можно использовать одну из следующих интерпретируемых моделей: LightGBM (Лгбмексплаинаблемодел), линейная регрессия (Линеарексплаинаблемодел), модель с метод стохастического градиента (Сгдексплаинаблемодел) и дерево принятия решений ( ДеЦисионтриексплаинаблемодел).


* **Пояснение по важности функции перестановки**: важность функции перестановки — это методика, используемая для объяснения моделей классификации и регрессии, характерных для [бумаги случайных лесов бреиман](https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf) (см. раздел 10). На высоком уровне, как это работает, случайным образом перетасовывание данные по одному компоненту для всего набора данных и вычислению объема изменений в показателях производительности. Чем больше это изменение, тем важна эта функция.

* **Травяное пояснение** (`contrib`). на основе [травяного](https://github.com/marcotcr/lime), травяного объяснения использует для создания локальных суррогатных моделей современные алгоритмы, не зависящие от модели (травяные). В отличие от глобальных суррогатных моделей, он ориентирован на обучение локальных суррогатных моделей для объяснения отдельных прогнозов.
* Диалоговое окно **пояснения к тексту** (`contrib`): в этом пояснении используется иерархическая сеть для получения объяснений модели из текстовых данных для данной текстовой модели с черной рамкой. Он обучает суррогатную модель "+ + +" на заданных прогнозируемых выходах модели черного ящика. После того как вы настроите глобальное обучение по тексту совокупности, он добавляет шаг точной настройки для конкретного документа, чтобы улучшить точность объяснений. При использовании двунаправленного РНН с двумя слоями для внимания предложений и слов. После того как DNN обучена в модели черного ящика и настроена для конкретного документа, пользователь может извлекать важность слова из уровней внимания. Кроме «ТРАВЯных» или «ШАП» для текстовых данных, а также более дорогостоящих в плане обучения времени. Были внесены улучшения, позволяющие пользователю инициализировать сеть с помощью внедрения специализированный Word, чтобы сократить время обучения. Время обучения можно значительно улучшить, выполнив команду "No +" на удаленной виртуальной машине GPU Azure. Реализация класса «со» описывается в разделе [«иерархические сети для классификации документов (Иванов et al., 2016)»](https://www.researchgate.net/publication/305334401_Hierarchical_Attention_Networks_for_Document_Classification).


* **Табличное пояснение**: `TabularExplainer` использует следующую логику для вызова прямых объяснений [ШАП](https://github.com/slundberg/shap) :

    1. Если это модель на основе дерева, примените ШАП `TreeExplainer`, else
    2. Если это модель DNN, примените ШАП `DeepExplainer`, иначе
    3. Если это линейная модель, примените ШАП `LinearExplainer`, иначе
    3. Рассматривайте его как модель с черным ящиком и примените ШАП `KernelExplainer`


`TabularExplainer` также внесли существенные улучшения в функции и производительность по прямым ШАП объяснениям:

* **Формирование сводных данных инициализации**. В случаях, когда скорость объяснения наиболее важна, мы суммируем набор данных инициализации и создадим небольшой набор репрезентативных образцов, который ускоряет как глобальное, так и локальное объяснение.
* **Выборка набора оценочных данных**. Если пользователь передает большой набор образцов оценки, но на самом деле они не требуют оценки, для параметра выборки можно задать значение true, чтобы ускорить глобальное объяснение.

На следующей диаграмме показана текущая структура прямого и мета объяснения.

[Архитектура интерпретации Машинное обучение ![](./media/machine-learning-interpretability-explainability/interpretability-architecture.png)](./media/machine-learning-interpretability-explainability/interpretability-architecture.png#lightbox)


### <a name="models-supported"></a>Поддерживаемые модели

Все модели, обученные в наборах данных в Python `numpy.array`, `pandas.DataFrame`, `iml.datatypes.DenseData`или `scipy.sparse.csr_matrix`, поддерживаются пакетом SDK для `explain` интерпретации.

Функции пояснения принимают в качестве входных данных модели и конвейеры. Если модель предоставлена, то модель должна реализовать функцию прогнозирования `predict` или `predict_proba`, которая соответствует соглашению Scikit. Если указан конвейер (имя скрипта конвейера), функция пояснения предполагает, что выполняющийся скрипт конвейера возвращает прогноз. Мы поддерживаем модели, обученные с помощью платформ глубокого обучения PyTorch, TensorFlow и keras.

### <a name="local-and-remote-compute-target"></a>Локальный и удаленный целевые объекты вычислений

Пакет `explain` предназначен для работы с локальными и удаленными целевыми объектами вычислений. При локальном запуске функции пакета SDK не будут обращаться к службам Azure. Вы можете запустить объяснение удаленно на Машинное обучение Azure вычислить и записать пояснения в Машинное обучение Azure выполнение служб журнала. После записи этих сведений отчеты и визуализации из описания можно легко найти на рабочая область машинного обучения Azure портале для анализа пользователей.


## <a name="next-steps"></a>Дальнейшие действия

См. [инструкции](how-to-machine-learning-interpretability-aml.md) по включению интерпретации для моделей, как локально, так и на машинное обучение Azure удаленных ресурсов вычислений. Дополнительные сценарии см. в [примерах записных книжек](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model/tabular-data) .
