---
title: 'Оценка модели: Ссылка на модуль'
titleSuffix: Azure Machine Learning service
description: Узнайте, как использовать модуль оценки модели в службе машинного обучения Azure, чтобы измерить точность обученной модели.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: xiaoharper
ms.author: zhanxia
ms.date: 05/06/2019
ROBOTS: NOINDEX
ms.openlocfilehash: 40a8247c22da1f7a057e222565ffb2ec4c6b7fb3
ms.sourcegitcommit: 4b9c06dad94dfb3a103feb2ee0da5a6202c910cc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/02/2019
ms.locfileid: "65028745"
---
# <a name="evaluate-model-module"></a>Модуль модели оценки

В этой статье описывается модуль визуального интерфейса (Предварительная версия) для службы машинного обучения Azure.

Этот модуль используется для измерить точность обученной модели. Укажите набор данных, содержащий показатели, созданные на основе модели и **Evaluate Model** модуль вычисляет набор стандартных для отрасли метрик оценки.
  
 Метрики, возвращенный **Evaluate Model** зависят от типа модели, оценка которой выполняется:  
  
-   **Модели классификации**    
-   **Модели регрессии**    



> [!TIP]
> Если вы не знакомы с оценки модели, мы рекомендуем серии видеороликов с аварийного восстановления. Стивен Elston, как часть [курс обучения машины](https://blogs.technet.microsoft.com/machinelearning/2015/09/08/new-edx-course-data-science-machine-learning-essentials/) от EdX. 


Существует три способа использования **Evaluate Model** модуля:

+ Создание оценок на основе обучающих данных, а оценка модели, в зависимости от этих оценок
+ Формирование оценок на модели, но сравнить эти рейтинги для оценок на зарезервированный проверочного набора
+ Сравнить результаты для двух разных, но связанных моделей, используя тот же набор данных

## <a name="use-the-training-data"></a>Использование данных для обучения

Чтобы оценить модель, необходимо подключить набор данных, содержащий набор входных столбцов и оценки.  Если нет других данных, можно использовать в исходном наборе данных.

1. Подключение **Оцененный набор данных** вывод [Score Model](./score-model.md) в качестве входных данных **Evaluate Model**. 
2. Нажмите кнопку **Evaluate Model** модуля и запустите эксперимент для формирования оценок для оценки.

## <a name="use-testing-data"></a>Использование проверочных данных

Распространенный сценарий в машинном обучении — для разделения исходного набора данных на обучающий и проверочный наборы данных, с помощью [разбиения](./split-data.md) модуля, или [секционирование и выборка](./partition-and-sample.md) модуля. 

1. Подключение **Оцененный набор данных** вывод [Score Model](score-model.md) в качестве входных данных **Evaluate Model**. 
2. Соедините выход модуля разделения данных, содержащей проверочные данные с правом входом **Evaluate Model**.
2. Нажмите кнопку **Evaluate Model** модуля и выберите **запустить выбранные** для формирования оценок для оценки.

## <a name="compare-scores-from-two-models"></a>Сравнить результаты двух моделей

Можно также подключиться, второй набор оценок с целью **Evaluate Model**.  Результаты тестирования может быть набор общей оценки, котором известно, что результаты, либо набор результатов из другой модели для тех же данных.

Эта функция полезна в тех случаях, так как вы можете легко сравнить результаты двух различных моделей на тех же данных. Или вы можете сравнить результаты двух различных выполнений по аналогичным данным с разными параметрами.

1. Подключение **Оцененный набор данных** вывод [Score Model](score-model.md) в качестве входных данных **Evaluate Model**. 
2. Соедините выход модуля модели оценки для второй модели справа входом **Evaluate Model**.
3. Щелкните правой кнопкой мыши **Evaluate Model**и выберите **запустить выбранные** для формирования оценок для оценки.

## <a name="results"></a>Результаты

После того, как **Evaluate Model**, щелкните модуль правой кнопкой мыши и выберите **результаты оценки** для просмотра результатов. Вы можете:

+ Сохранение результатов в виде набора данных, для облегчения анализа с другими средствами
+ Создать визуализацию в интерфейсе

При подключении наборов данных в оба входа **Evaluate Model**, то результаты будут содержать метрики для обоих наборов данных или обе модели.
Модели или данные, прикрепленные к левому порту сначала выдается в отчете, следуют метрики для набора данных, или модель на нужный порт.  

Например на следующем рисунке показано сравнение результатов из двух моделей кластеризации, которые были созданы на тех же данных, но с разными параметрами.  

![AML&#95;Comparing2Models](media/module/aml-comparing2models.png "AML_Comparing2Models")  

Поскольку это модели кластеризации, результаты оценки, отличаются от по сравнению с оценки двух моделей регрессии, или по сравнению с двух моделей классификации. Тем не менее общее представление одинаков. 

## <a name="metrics"></a>Метрики

В этом разделе описываются метрики, возвращаемые для определенных типов моделей для использования с **Evaluate Model**:

+ [модели классификации](#bkmk_classification)
+ [модели регрессии](#bkmk_regression)

###  <a name="bkmk_classification"></a> Метрики моделей классификации

При оценке моделей классификации, выводятся следующие метрики. Если сравнить модели, они ранжируются по метрике, выбранная для оценки.  
  
-   **Точность** измеряет степень модель классификации как пропорцию результатов true к общему числу.  
  
-   **Точность** — это доля true результаты по всем положительным результатам.  
  
-   **Отозвать** та доля всех правильных результатов, возвращенных моделью.  
  
-   **F-Оценка** вычисляется как средневзвешенное значение для точности и отзыва в диапазоне от 0 до 1, где оптимальное значение F-оценка-1.  
  
-   **AUC** меры, отображаются площадь под кривой и истинных положительных результатов на y axis и false положительных результатов по оси x. Эта метрика полезно, поскольку она предоставляет одно число, можно сравнивать модели различных типов.  
  
- **Средняя потеря по журналу** — единый показатель, используемый для выражения штраф за неверные результаты. Он рассчитывается как разность двух распределений вероятностей — истинного и в модели.  
  
- **Потеря по журналу обучения** является оценки, представляющий преимущество классификатора случайного прогноза. Потеря по журналу измеряет неопределенности модели путем сравнения значения вероятности, то командлет выводит для известных значений (фактическое подтверждение) в метках. Вы хотите свести к минимуму потеря по журналу для модели в целом.

##  <a name="bkmk_regression"></a> Метрики регрессионных моделей
 
Метрики возвращаются в моделях регрессии предназначены в основном для оценки количества ошибок.  Модель считается в соответствии с данными, также в том случае, если разница между наблюдаемыми и прогнозируемые значения мал. Тем не менее просмотрев шаблон остатки (разница между любой момент прогнозируемые и фактическим значением соответствующего) можно позволит многое узнать о потенциальных смещение в модели.  
  
 Следующие метрики передаются для оценки моделей регрессии. При сравнении моделей, они упорядочены по метрике, выбранная для оценки.  
  
- **Средняя абсолютная погрешность (MAE)** измеряет, насколько прогнозы близки к фактические результаты; таким образом, лучше нижней оценку.  
  
- **Среднеквадратичная погрешность (RMSE)** создает одно значение ошибки в модели. Возведения разницу, метрики не учитывает разницу между чрезмерной прогноза и недостаточное число ресурсов прогноза.  
  
- **Относительная абсолютная погрешность (RAE)** относительный абсолютный разница между ожидаемыми и фактическими значениями; относительный, так как Средняя разность делится на среднее арифметическое.  
  
- **Относительная среднеквадратическая погрешность (RSE)** аналогичным образом нормализует суммарное квадратичное отклонение прогнозируемых значений путем деления на суммарное квадратичное отклонение фактических значений.  
  
- **Ошибка один ноль (MZOE) означает** указывает, была ли прогноз правильный.  Другими словами: `ZeroOneLoss(x,y) = 1` при `x!=y`; в противном случае `0`.
  
- **Коэффициент детерминации**, часто называются R<sup>2</sup>, обозначает совокупную прогнозной модели значение в диапазоне от 0 до 1. Ноль означает, что модель случайного (объясняет nothing); 1 означает, что идеально подходит. Однако следует соблюдать осторожность в интерпретации R<sup>2</sup> значения, как низкие значения могут быть полностью обычный и высокие значения могут быть подозрительная.
  

## <a name="next-steps"></a>Дальнейшие действия

См. в разделе [набор модулей, доступных](module-reference.md) для службы машинного обучения Azure. 