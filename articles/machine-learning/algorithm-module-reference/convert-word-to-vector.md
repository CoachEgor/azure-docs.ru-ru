---
title: Преобразование слов в векторы
titleSuffix: Azure Machine Learning
description: Узнайте, как использовать три предоставленных модели преобразования слов в векторы для извлечения словаря и его соответствующих внедрений слов из корпуса текста.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 05/19/2020
ms.openlocfilehash: e0e796b75690bcacc6be8ef29b8b490c7faa40af
ms.sourcegitcommit: 1f25aa993c38b37472cf8a0359bc6f0bf97b6784
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/26/2020
ms.locfileid: "83853656"
---
# <a name="convert-word-to-vector"></a>Преобразование слов в векторы

В этой статье описывается использование модуля **Преобразование слов в векторы** в конструкторе Машинного обучения Azure (предварительная версия) для применения различных моделей преобразования слов в векторы (предварительно обученных моделей Word2Vec, FastText, Glove) к корпусу текста, который указан в качестве входных данных, и создания словаря с внедренными словами.

В этом модуле используется библиотека Gensim. Дополнительные сведения о Gensim см. на [официальном веб-сайте](https://radimrehurek.com/gensim/apiref.html), где содержатся руководства и описания алгоритмов.

### <a name="more-about-convert-word-to-vector"></a>Подробнее о преобразовании слов в векторы

Вообще говоря, преобразование слова в вектор (или векторизация) является процессом обработки естественного языка (NLP), в котором используются языковые модели или методы для представления слов в векторном пространстве, то есть для представления каждого слова в виде вектора вещественных чисел. Кроме того, слова со схожими значениями могут иметь похожие представления.

Внедрения слов можно использовать в качестве начальных входных данных для последующих задач NLP, таких как классификация текста, анализ тональности и т. д.

Существует множество различных технологий внедрения слов, но в этом модуле реализованы три широко используемых метода, включая две модели интерактивного обучения (Word2Vec и FastText) и одну предварительно обученную модель (glove-wiki-gigaword-100). Модели интерактивного обучения обучаются на основе входных данных, а предварительно обученные модели — в автономном режиме на основе более крупного корпуса текста (например, из Википедии, Google News), обычно содержащего около 100 000 000 000 слов, а внедрения слов остаются неизменными во время векторизации слов. Предварительно обученные модели слов предоставляют такие преимущества, как сокращенное время обучения, более качественно закодированные векторы слов и повышенная общая производительность.

+ Word2Vec является одним из самых популярных методов для изучения внедрения слов с помощью небольшой нейронной сети. Теоретические сведения приводятся в следующем документе, доступном для скачивания в формате PDF: ["Efficient Estimation of Word Representations in Vector Space"](https://arxiv.org/pdf/1301.3781.pdf) (Эффективная оценка представлений слов в векторном пространстве), авторы: Томас Миколов (Tomas Mikolov) и др. Реализация в этом модуле основана на [библиотеке gensim для Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html).

+ Теоретические сведения по FastText приводятся в следующем документе, доступном для скачивания в формате PDF: [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf) (Обогащение векторов слов с помощью информации о подсловах), авторы: Петр Божановски (Piotr Bojanowski) и др. Реализация в этом модуле основана на [библиотеке gensim для FastText](https://radimrehurek.com/gensim/models/fasttext.html).

+ Предварительно обученная модель Glove glove-wiki-gigaword-100 является коллекцией предварительно обученных векторов на основе корпуса текста из Википедии, который содержит 5,6 млрд токенов и словарь из 400 тыс. слов. Доступен документ в формате PDF: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (GloVe. Глобальные векторы для представления слов).

## <a name="how-to-configure-convert-word-to-vector"></a>Настройка преобразования слов в векторы

Для работы с этим модулем требуется набор данных, содержащий столбец текста (предварительно обработанный текст будет лучшим вариантом).

1. Добавьте модуль **Преобразование слов в векторы** в конвейер.

2. В качестве входных данных для модуля укажите набор данных, содержащий один или несколько текстовых столбцов.

3. В качестве **целевого столбца** выберите только один столбец, содержащий текст для обработки.

    Поскольку этот модуль создает словарь на основе текста, содержимое столбцов будет разным, что приводит к формированию разного содержимого словаря, поэтому модуль принимает только один целевой столбец.

4. В качестве значения параметра **Стратегия Word2Vec** выберите нужный вариант: `GloVe pretrained English Model`, `Gensim Word2Vec` или `Gensim FastText`.

5. Если параметру **Стратегия Word2Vec** задано `Gensim Word2Vec` или `Gensim FastText`:

    + **Алгоритм обучения Word2Vec**. Выберите `Skip_gram` или `CBOW`. Их различие описано в исходном [документе](https://arxiv.org/pdf/1301.3781.pdf).

        Методом по умолчанию является `Skip_gram`.

    + **Длина внедрения слов**. Укажите размерность векторов слов. Соответствует параметру `size` в gensim.

        Размер внедрения по умолчанию — 100.

    + **Размер окна контекста**. Укажите максимальное расстояние между прогнозируемым словом и текущим словом. Соответствует параметру `window` в gensim.

        Размер окна по умолчанию — 5.

    + **Количество эпох**. Укажите количество эпох (итераций) по корпусу. Соответствует параметру `iter` в gensim.

        Количество эпох по умолчанию — 5.

6. Для параметра **Максимальный размер словаря** укажите максимальное количество слов в созданном словаре.

    Если количество уникальных слов превышает это значение, удалите редко используемые слова.

    Размер словаря по умолчанию — 10 000.

7. Для параметра **Минимальное количество слов** укажите минимальное количество слов, при котором модуль пропускает все слова с частотой появления ниже этого значения.

    Значение по умолчанию — 5.

8. Отправьте конвейер.

## <a name="examples"></a>Примеры

Выходные данные модуля:

+ **Словарь с внедрениями**. Содержит созданный словарь, а также внедрение каждого слова, одно измерение занимает один столбец.

### <a name="result-examples"></a>Примеры результатов

Чтобы продемонстрировать работу модуля **Преобразование слов в векторы**, в следующем примере мы применим его с параметрами по умолчанию к предварительно обработанному набору данных SP 500 из Википедии, который доступен в Машинном обучении Azure (предварительная версия).

#### <a name="source-dataset"></a>Исходный набор данных

Набор данных содержит столбец категории, а также полный текст, взятый из Википедии. В этой таблице приведено лишь несколько репрезентативных примеров.

|text|
|----------|
|nasdaq 100 component s p 500 component foundation founder location city apple campus 1 infinite loop street infinite loop cupertino california cupertino california location country united states...|
|br nasdaq 100 nasdaq 100 component br s p 500 s p 500 component industry computer software foundation br founder charles geschke br john warnock location adobe systems...|
|s p 500 s p 500 component industry automotive industry automotive predecessor general motors corporation 1908 2009 successor...|
|s p 500 s p 500 component industry conglomerate company conglomerate foundation founder location city fairfield connecticut fairfield connecticut location country usa area...|
|br s p 500 s p 500 component foundation 1903 founder william s harley br arthur davidson harley davidson founder arthur davidson br walter davidson br william a davidson location...|

#### <a name="output-vocabulary-with-embeddings"></a>Выходной словарь с внедрениями

В следующей таблице приводятся выходные данные этого модуля. В качестве входных данных использовался набор данных SP 500 из Википедии. В крайнем левом столбце указывается словарь, его вектор внедрения представлен значениями оставшихся столбцов в той же строке.

|Словарь|Размерность внедрения 0|Размерность внедрения 1|Размерность внедрения 2|Размерность внедрения 3|Размерность внедрения 4|Размерность внедрения 5|...|Размерность внедрения 99|
|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|
|nasdaq|–0,375865|0,609234|0,812797|–0,002236|0,319071|0,591986|...|0,364276
|Компонент|0,081302|0,40001|0,121803|0,108181|0,043651|–0,091452|...|0,636587
|s|–0,34355|–0,037092|–0,012167|0,151542|0,601019|0,084501|...|0,149419
|p|–0,133407|0,073244|0,170396|0,326706|0,213463|–0,700355|...|0,530901
foundation|–0,166819|0,10883|–0,07933|–0,073753|0,262137|0,045725|...|0,27487
founder|–0,297408|0,493067|0,316709|–0,031651|0,455416|–0,284208|...|0,22798
location|–0,375213|0,461229|0,310698|0,213465|0,200092|0,314288|...|0,14228
city|–0,460828|0,505516|–0,074294|–0,00639|0,116545|0,494368|...|–0,2403
apple|0,05779|0,672657|0,597267|–0,898889|0,099901|0,11833|...|0,4636
campus|–0,281835|0,29312|0,106966|–0,031385|0,100777|–0,061452|...|0,05978
infinite|–0,263074|0,245753|0,07058|–0,164666|0,162857|–0,027345|...|–0,0525
loop|–0,391421|0,52366|0,141503|–0,105423|0,084503|–0,018424|...|–0,0521

В этом примере мы использовали заданное по умолчанию значение `Gensim Word2Vec` в качестве **стратегии Word2Vec**, `Skip-gram` в качестве **алгоритма обучения**, **длина внедрения слов** равна 100, поэтому у нас есть 100 столбцов.

## <a name="technical-notes"></a>Технические примечания

В этом разделе содержатся советы и ответы на часто задаваемые вопросы.

+ Отличие модели интерактивного обучения от предварительно обученной модели

    В этом модуле **Преобразование слов в векторы** были доступны три различных стратегии, две модели интерактивного обучения и одна предварительно обученная модель. Модель интерактивного обучения использует входной набор данных в качестве обучающих данных, создает словарь и векторы слова во время обучения, в то время как предварительно обученная модель уже обучена на основе гораздо более объемного корпуса текста, например из Википедии или Twitter, поэтому предварительно обученная модель на самом деле представляет собой набор пар (слово-внедрение).  

    Если в качестве стратегии векторизации слов выбирается предварительно обученная модель Glove, она обобщает словарь из входного набора данных и создает вектор внедрения для каждого слова из предварительно обученной модели без интерактивного обучения. Предварительно обученная модель экономит время на обучение и имеет более высокую производительность, особенно если размер входного набора данных относительно мал.

+ Размер внедрения

    Как правило, для достижения хорошей производительности длина внедрения слов составляет несколько сотен (например, 100, 200, 300), так как небольшой размер внедрения означает небольшое векторное пространство, что может привести к конфликтам внедрения слов.  

    При использовании предварительно обученных моделей длина внедрений слов фиксирована. В этой реализации размер внедрения glove-wiki-gigaword-100 равен 100.


## <a name="next-steps"></a>Дальнейшие действия

Ознакомьтесь с [набором доступных модулей](module-reference.md) в службе Машинного обучения Azure. 

Список специфических ошибок для модулей конструктора (предварительная версия) см. в статье с [кодами ошибок Машинного обучения](designer-error-codes.md).
