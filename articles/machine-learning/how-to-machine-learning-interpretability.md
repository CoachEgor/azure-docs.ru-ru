---
title: Интерпретация моделей в машинном обучении Azure
titleSuffix: Azure Machine Learning
description: Узнайте, как объяснить, почему модель делает прогнозы с помощью SDK для машинного обучения Azure. Его можно использовать во время обучения и выводов, чтобы понять, как ваша модель делает прогнозы.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: mesameki
author: mesameki
ms.reviewer: Luis.Quintanilla
ms.date: 04/02/2020
ms.openlocfilehash: fcb837af85a54102e8c9eafc33249af9dba6b5ce
ms.sourcegitcommit: d597800237783fc384875123ba47aab5671ceb88
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/03/2020
ms.locfileid: "80631426"
---
# <a name="model-interpretability-in-azure-machine-learning"></a>Интерпретация моделей в машинном обучении Azure
[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-basic-enterprise-sku.md)]

## <a name="overview-of-model-interpretability"></a>Обзор интерпретации моделей

Интерпретация имеет решающее значение как для ученых, аудиторов и лиц, принимающих бизнес-решения, для обеспечения соответствия политике компаний, отраслевым стандартам и правительственным нормативным актам:

+ Специалисты по обработке данных должны разъяснять свои модели руководителям и заинтересованным сторонам, чтобы они могли понять ценность и точность своих выводов. Они также требуют интерпретации для отладки своих моделей и принятия обоснованных решений о том, как их улучшить. 

+ Правовым аудиторам требуются инструменты для проверки моделей соблюдения нормативных требований и мониторинга того, как решения моделей влияют на людей. 

+ Лица, принимающие решения, нуждаются в спокойствии, имея возможность обеспечить прозрачность для конечных пользователей. Это позволяет им зарабатывать и поддерживать доверие.


Включение возможности объяснения модели машинного обучения важно на двух основных этапах разработки модели:
+ На этапе обучения, как модель ныеречии и оценщики могут использовать интерпретацию вывода модели для проверки гипотез и укрепления доверия с заинтересованными сторонами. Они также используют сведения о модели для отладки, проверки поведения модели соответствует их целям, а также для проверки несправедливости модели или незначительных функций.

+ На этапе выводов прозрачность вокруг развернутых моделей позволяет руководителям понять, "когда она развернута" и как ее решения относятся к людям в реальной жизни и влияют на них. 

## <a name="interpretability-with-azure-machine-learning"></a>Интерпретация с помощью машинного обучения Azure

Классы интерпретации доступны через несколько пакетов SDK: (Узнайте, как [установить пакеты SDK для машинного обучения Azure)](https://docs.microsoft.com/python/api/overview/azure/ml/install?view=azure-ml-py)

* `azureml.interpret`, основной пакет, содержащий функции, поддерживаемые корпорацией Майкрософт.

* `azureml.contrib.interpret`, Предварительный просмотр, и экспериментальные функциональные возможности, которые вы можете попробовать.

* `azureml.train.automl.automlexplainer`пакет для интерпретации автоматизированных моделей машинного обучения.

`pip install azureml-interpret` Используйте `pip install azureml-interpret-contrib` и для `pip install azureml-interpret-contrib` общего использования, и для использования AutoML, чтобы получить пакеты интерпретации.


> [!IMPORTANT]
> Содержимое `contrib` в пространстве имен не поддерживается полностью. По мере того как экспериментальные функции будут созревать, они постепенно будут перенесены в основное пространство имен.
.



## <a name="how-to-interpret-your-model"></a>Как интерпретировать модель

Используя классы и методы в SDK, вы можете:
+ Объясните прогноз модели, создав значения значений значения объектов для всей модели и/или отдельных точек данных. 
+ Достижение интерпретации моделей на реальных наборах данных в масштабе, во время обучения и выводов.
+ Используйте интерактивную панель визуализации, чтобы обнаружить закономерности в данных и объяснениях во время обучения


В машинном обучении **функции** — это поля данных, используемые для прогнозирования целевой точки данных. Например, для прогнозирования кредитного риска могут использоваться поля данных для возраста, размера учетной записи и возраста учетной записи. В этом случае возраст, размер учетной записи и возраст учетной записи являются **объектами.** Значение функции показывает, как каждое поле данных повлияло на прогнозы модели. Например, возраст может быть в значительной степени использован в прогнозировании, в то время как размер и возраст учетной записи не влияют на значения прогноза значительно. Этот процесс позволяет ученым по обработке данных объяснить полученные прогнозы, чтобы заинтересованные стороны имели видимость того, какие функции являются наиболее важными в модели.

Узнайте об поддерживаемых методах интерпретации, поддерживаемых моделях машинного обучения и поддерживаемых средах выполнения здесь.


## <a name="supported-interpretability-techniques"></a>Поддерживаемые методы интерпретации

 `azureml-interpret`использует методы интерпретации, разработанные в [Interpret-Community](https://github.com/interpretml/interpret-community/), пакет питона с открытым исходным кодом для обучения интерпретируемым моделям и помощи в объяснении систем итогового ящиков. [Interpret-Community](https://github.com/interpretml/interpret-community/) служит в качестве хоста для поддерживаемых sDK объяснений, и в настоящее время поддерживает следующие методы интерпретации:

|Метод интерпретации|Описание|Type|
|--|--|--------------------|
|1. SHAP Дерево Объяснение| [SHAP](https://github.com/slundberg/shap)'S дерево объяснение, которое фокусируется на полиномиальное время быстро SHAP алгоритм оценки значений, характерных для **деревьев и ансамблей деревьев**.|Конкретная модель|
|2. SHAP Глубокий пояснение| Основываясь на объяснении [от SHAP](https://github.com/slundberg/shap), Deep Explainer "является высокоскоростным алгоритмом приближения для значений SHAP в глубоком обучении моделей, который опирается на связь с DeepLIFT описано в [документе SHAP NIPS](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions). **Поддерживаются модели TensorFlow** и **модели Keras** с использованием бэкэнда TensorFlow (есть также предварительная поддержка PyTorch)».|Конкретная модель|
|3. SHAP Линейный объяснитель| [Линейный](https://github.com/slundberg/shap)пояснение SHAP вычисляет значения SHAP для **линейной модели,** дополнительно учитывая межфункциональные корреляции.|Конкретная модель|
|4. Объяснение ядра ядра SHAP| Объяснение ядра [SHAP](https://github.com/slundberg/shap)использует специально взвешенную локальную линейную регрессию для оценки значений SHAP для **любой модели.**|Модель-агностик|
|5. Мимик Объяснение (Глобальный суррогат)| Мими-объяснение основано на идее обучения [глобальных суррогатных моделей](https://christophm.github.io/interpretable-ml-book/global.html) для имитации моделей blackbox. Глобальная суррогатная модель является внутренне интерпретируемой моделью, которая обучается максимально точно мучиться по прогнозам **любой модели черного ящика.** Специалисты по обработке данных могут интерпретировать суррогатную модель, чтобы сделать выводы о модели черного ящика. Вы можете использовать одну из следующих интерпретируемых моделей в качестве суррогатной модели: LightGBM (LGBMExplainableModel), Линейная регрессия (Linear ExplainableModel), Stochastic Gradient Descent объяснимой модели (SGDExplainableModel) и Дерево решений (DecisionTreeExplainableModel).|Модель-агностик|
|6. Объяснение важности характеристик перестановок (PFI)| Значение функции перестановки — это метод, используемый для объяснения моделей классификации и регрессии, который вдохновлен [бумагой Бреймана «Случайные леса»](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) (см. раздел 10). На высоком уровне, как это работает путем случайной перетасовки данных одной функции в то время, для всего набора данных и расчета, сколько метрики производительности интереса изменения. Чем больше изменение, тем важнее компонент. PFI может объяснить общее поведение **любой базовой модели,** но не объясняет отдельные прогнозы. |Модель-агностик|




Помимо методов интерпретации, описанных выше, мы поддерживаем `TabularExplainer`другой [SHAP основе пояснение](https://github.com/slundberg/shap), называется . В зависимости от `TabularExplainer` модели, использует один из поддерживаемых объяснений SHAP:

* TreeExplainer для всех моделей на основе деревьев
* DeepExplainer для моделей DNN
* ЛинейныйОбъяснитель для линейных моделей
* КернелКонслекер для всех других моделей

`TabularExplainer`также сделал значительные функции и повышения производительности по сравнению с прямыми SHAP Объяснения:

* **Суммаризация набора данных инициализации**. В тех случаях, когда скорость объяснения является наиболее важной, мы суммируем набор данных инициализации и создаем небольшой набор репрезентативных выборок, что ускоряет генерацию общих и индивидуальных значений значения значений значения объектов.
* **Выборка набора данных оценки**. Если пользователь проходит в большом наборе образцов оценки, но на самом деле не нужно все из них должны быть оценены, параметр выборки может быть установлен в верной, чтобы ускорить расчет общих объяснений модели.

Следующая диаграмма показывает текущую структуру поддерживаемых объяснений.

[![Архитектура интерпретации машинного обучения](./media/how-to-machine-learning-interpretability/interpretability-architecture.png)](./media/how-to-machine-learning-interpretability/interpretability-architecture.png#lightbox)


## <a name="supported-machine-learning-models"></a>Поддерживаемые модели машинного обучения

Пакет `azureml.interpret` SDK поддерживает модели, обученные следующим формам набора данных:
- `numpy.array`
- `pandas.DataFrame`
- `iml.datatypes.DenseData`
- `scipy.sparse.csr_matrix`

Функции объяснения принимают как модели, так и конвейеры в качестве входных. При предоставлении модели модель должна реализовать `predict` функцию прогнозирования или `predict_proba` соответствующую конвенции Scikit. Если модель не поддерживает это, можно обернуть модель в функцию, `predict` которая `predict_proba` генерирует тот же результат, что и в Scikit, и использовать эту функцию обертки с выбранным объяснением. При предоставлении конвейера функция объяснения предполагает, что запущенный скрипт конвейера возвращает прогноз. Используя этот метод `azureml.interpret` упаковки, можно поддерживать модели, обученные через PyTorch, TensorFlow и Keras deep learning, а также классические модели машинного обучения.

## <a name="local-and-remote-compute-target"></a>Локальная и удаленная вычислительная цель

Пакет `azureml.interpret` предназначен для работы как с локальными, так и с целями дистанционного вычисления. При запуске локально функции SDK не будут связываться с какими-либо службами Azure. 

Вы можете удаленно запускать объяснение на Azure Machine Learning Compute и регистрировать информацию об объяснении в службу истории машинного обучения Azure. После регистрации этой информации отчеты и визуализации из объяснения легко доступны в студии Машинного обучения Azure для анализа пользователей.


## <a name="next-steps"></a>Дальнейшие действия

Ознакомьтесь с [тем, как](how-to-machine-learning-interpretability-aml.md) обеспечить интерпретацию моделей для обучения как локально, так и на удаленных вычислительных ресурсах Azure Machine Learning. Ознакомьтесь с [образцами тетрадей](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model) для дополнительных сценариев.
