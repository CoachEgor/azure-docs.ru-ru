---
title: Интерпретация моделей в машинном обучении Azure
titleSuffix: Azure Machine Learning
description: Узнайте, как объяснить, почему модель делает прогнозы с помощью SDK для машинного обучения Azure. Его можно использовать во время обучения и выводов, чтобы понять, как ваша модель делает прогнозы.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: mesameki
author: mesameki
ms.reviewer: trbye
ms.date: 10/25/2019
ms.openlocfilehash: b68d2a72dc18f683f2203429908a536db1b5124a
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "80063989"
---
# <a name="model-interpretability-in-azure-machine-learning"></a>Интерпретация моделей в машинном обучении Azure
[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-basic-enterprise-sku.md)]

## <a name="overview-of-model-interpretability"></a>Обзор интерпретации моделей

Интерпретация имеет решающее значение для ученых, персональных данных, и лиц, принимающих бизнес-решения, для обеспечения соответствия политике компании, отраслевым стандартам и правительственным нормативным актам:
+ Специалисты по обработке данных должны разъяснять свои модели руководителям и заинтересованным сторонам, чтобы они могли понять ценность и точность своих выводов 
+ Лица, принимающие решения, нуждаются в спокойствии, способной обеспечить прозрачность для конечных пользователей, чтобы завоевать и сохранить свое доверие

Включение возможности объяснения модели машинного обучения важно на двух основных этапах разработки модели:
+ На этапе обучения процесс разработки модели машинного обучения. Разработчики моделей и оценщики могут использовать вывод ы интерпретации модели для проверки гипотез и укрепления доверия с заинтересованными сторонами. Они также используют сведения о модели для отладки, проверки поведения модели соответствует их целям, а также для проверки на наличие необъективных или незначительных функций.
+ На этапе выводов прозрачность вокруг развернутых моделей позволяет руководителям понять, "когда она развернута" и как ее решения относятся к людям в реальной жизни и влияют на них. 

## <a name="interpretability-with-azure-machine-learning"></a>Интерпретация с помощью машинного обучения Azure

В этой статье вы узнаете, как концепции интерпретации моделей реализуются в SDK.

Используя классы и методы в SDK, вы можете получить:
+ Значения значений характеристик как для необработанных, так и для инженерных функций
+ Интерпретация в реальных наборах данных в масштабе, во время обучения и выводов.
+ Интерактивные визуализации, чтобы помочь вам в открытии закономерностей в данных и объяснений во время обучения


В машинном обучении **функции** — это поля данных, используемые для прогнозирования целевой точки данных. Например, для прогнозирования кредитного риска могут использоваться поля данных для возраста, размера учетной записи и возраста учетной записи. В этом случае возраст, размер учетной записи и возраст учетной записи являются **объектами.** Значение функции показывает, как каждое поле данных повлияло на прогнозы модели. Например, возраст может быть сильно использован в прогнозировании, в то время как размер и возраст учетной записи не влияют на точность прогнозирования значительно. Этот процесс позволяет ученым по обработке данных объяснить полученные прогнозы, чтобы заинтересованные стороны имели видимость того, какие точки данных являются наиболее важными в модели.

Используя эти инструменты, вы можете объяснить модели машинного обучения **во всем мире на всех данных**или локально на **определенной точке данных,** используя современные технологии в простой в использовании и масштабируемой моды.

Классы интерпретации доступны через несколько пакетов SDK. Узнайте, как [установить пакеты SDK для машинного обучения Azure.](https://docs.microsoft.com/python/api/overview/azure/ml/install?view=azure-ml-py)

* `azureml.interpret`, основной пакет, содержащий функции, поддерживаемые корпорацией Майкрософт.

* `azureml.contrib.interpret`, Предварительный просмотр, и экспериментальные функциональные возможности, которые вы можете попробовать.

* `azureml.train.automl.automlexplainer`пакет для интерпретации автоматизированных моделей машинного обучения.

> [!IMPORTANT]
> Содержимое `contrib` в пространстве имен не поддерживается полностью. По мере того как экспериментальные функции будут созревать, они постепенно будут перенесены в основное пространство имен.

## <a name="how-to-interpret-your-model"></a>Как интерпретировать модель

Для понимания глобального поведения модели или конкретных прогнозов можно применить классы и методы интерпретации. Первое называется глобальным объяснением, а второе - локальным.

Методы также можно классифицировать в зависимости от того, является ли метод моделью агностиком или конкретной моделью. Некоторые методы нацелены на определенный тип моделей. Например, объяснение дерева SHAP относится только к моделям на основе деревьев. Некоторые методы рассматривают модель как черный ящик, например, мимические пояснения или пояснение ядра SHAP. Пакет `interpret` использует эти различные подходы на основе наборов данных, типов моделей и случаев использования.

Выход представляет собой набор информации о том, как данная модель делает свой прогноз, например:
* Глобальное/локальное относительное значение особенностей
* Глобальные/локальные отношения функций и прогнозирования

### <a name="explainers"></a>Объяснения

В этом пакете используются методы интерпретации, разработанные в [Interpret-Community,](https://github.com/interpretml/interpret-community/)пакет питона с открытым исходным кодом для обучения интерпретируемым моделям и помощи в объяснении систем итогового итогового ящиков. [Interpret-Community](https://github.com/interpretml/interpret-community/) служит в качестве хоста для поддерживаемых sDK объяснений, и в настоящее время поддерживает следующие методы интерпретации:

* **SHAP Дерево Объяснение**: [SHAP](https://github.com/slundberg/shap)'S дерево объяснение, которое фокусируется на полиномиальное время быстро SHAP алгоритм оценки значения, характерного для деревьев и ансамблей деревьев.
* **SHAP Deep Explainer**: Основываясь на объяснении [от SHAP](https://github.com/slundberg/shap), Deep Explainer "является высокоскоростным алгоритмом приближения значений SHAP в моделях глубокого обучения, который основывается на связи с DeepLIFT, описанной в [документе SHAP NIPS](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions). Поддерживаются модели TensorFlow и модели Keras с использованием бэкэнда TensorFlow (есть также предварительная поддержка PyTorch)».
* **SHAP Линейный Объяснение**: Линейный пояснение [SHAP](https://github.com/slundberg/shap)вычисляет значения SHAP для линейной модели, дополнительно учитывая межфункциональные корреляции.

* **Объяснение ядра SHAP**: Объяснение ядра [SHAP](https://github.com/slundberg/shap)использует специально взвешенную локальную линейную регрессию для оценки значений SHAP для любой модели.
* **Mimic Explainer**: Мимик-объяснение основано на идее обучения [глобальных суррогатных моделей](https://christophm.github.io/interpretable-ml-book/global.html) для имитации моделей blackbox. Глобальная суррогатная модель является внутренне интерпретируемой моделью, которая обучается максимально точному приближению к прогнозам модели черного ящика. Специалист по обработке данных может интерпретировать суррогатную модель, чтобы сделать выводы о модели черного ящика. Вы можете использовать одну из следующих интерпретируемых моделей в качестве суррогатной модели: LightGBM (LGBMExplainableModel), Линейная регрессия (Linear ExplainableModel), Stochastic Gradient Descent объяснимая модель (SGDExplainableModel) и Decision Tree ( РешениеДеревоОбъяснитеМодель).


* **Объяснение важности функции перестановки**: Важность функции перестановки — это метод, используемый для объяснения моделей классификации и регрессии, который вдохновлен [бумагой Бреймана «Случайные леса»](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) (см. раздел 10). На высоком уровне, как это работает путем случайной перетасовки данных одной функции в то время, для всего набора данных и расчета, сколько метрики производительности интереса изменения. Чем больше изменение, тем важнее компонент.

* **LIME Explainer** (`contrib`): На основе [LIME](https://github.com/marcotcr/lime), LIME Explainer использует современный локальный интерпретируемый алгоритм модели-агностик (LIME) для создания локальных суррогатных моделей. В отличие от глобальных моделей суррогатных, LIME фокусируется на обучении местных моделей суррогатных объяснить индивидуальные прогнозы.
* **Объяснение текста** HAN`contrib`( ): ОБЪЯСНЕНИЕ текста HAN использует иерархическую сеть внимания для получения объяснений модели из текстовых данных для данной модели текста черного ящика. Он тренирует суррогатную модель HAN на прогнозируемых выводах данной модели черного ящика. После обучения по всему тексту корпуса, он добавляет тонкую настройку шаг для конкретного документа, с тем чтобы повысить точность объяснений. HAN использует двунаправленный RNN с двумя слоями внимания, для предложения и внимания слова. После того, как DNN обучается модели черного ящика и дорабатывается на конкретном документе, пользователь может извлечь значение слова из слоев внимания. Han показан более точным, чем LIME или SHAP для текстовых данных, но более дорогостоящим с точки зрения времени обучения, а также. Улучшения были сделаны, чтобы дать пользователю возможность инициализировать сеть с GloVe слово встраивания, чтобы сократить время обучения. Время обучения можно значительно улучшить, запустив HAN на удаленном вдыхаемом графическом процессоре Azure VM. Реализация HAN описана в [«Иерархических сетях внимания к классификации документов (Yang et al., 2016)».](https://www.researchgate.net/publication/305334401_Hierarchical_Attention_Networks_for_Document_Classification)


* **Табулярный** `TabularExplainer` объяснение : использует следующую логику, чтобы вызвать прямые [sHAP](https://github.com/slundberg/shap) Объяснения:

    1. Если это модель на основе дерева, нанесите SHAP `TreeExplainer`, иначе
    2. Если это модель DNN, применить `DeepExplainer`SHAP , другие
    3. Если это линейная модель, `LinearExplainer`применяйте SHAP, иначе
    3. Относитесь к нему как к модели черного ящика и применяйте SHAP`KernelExplainer`


`TabularExplainer`также сделал значительные функции и повышения производительности по сравнению с прямыми SHAP Объяснения:

* **Суммаризация набора данных инициализации**. В тех случаях, когда скорость объяснения является наиболее важной, мы суммируем набор данных инициализации и создаем небольшой набор репрезентативных выборок, что ускоряет как глобальное, так и локальное объяснение.
* **Выборка набора данных оценки**. Если пользователь проходит в большом наборе образцов оценки, но на самом деле не нужно все из них должны быть оценены, параметр выборки может быть установлен в верной, чтобы ускорить глобальное объяснение.

На следующей диаграмме показана текущая структура прямых и метаобъяснений.

[![Архитектура интерпретации машинного обучения](./media/how-to-machine-learning-interpretability/interpretability-architecture.png)](./media/how-to-machine-learning-interpretability/interpretability-architecture.png#lightbox)


### <a name="models-supported"></a>Модели поддерживаются

Любые модели, обучаемые `numpy.array` `pandas.DataFrame`на `iml.datatypes.DenseData`наборах данных в Python, или `scipy.sparse.csr_matrix` формате, поддерживаются пакетом интерпретаций `explain` SDK.

Функции объяснения принимают как модели, так и конвейеры в качестве входных. При предоставлении модели модель должна реализовать `predict` функцию прогнозирования или `predict_proba` соответствующую конвенции Scikit. Если предоставляется конвейер (имя скрипа конвейера), функция объяснения предполагает, что запущенный скрипт конвейера возвращает прогноз. Мы поддерживаем модели, обучаемые в рамках глубокого обучения PyTorch, TensorFlow и Keras.

### <a name="local-and-remote-compute-target"></a>Локальная и удаленная вычислительная цель

Пакет `explain` предназначен для работы как с локальными, так и с целями дистанционного вычисления. При запуске локально функции SDK не будут связываться с какими-либо службами Azure. Вы можете удаленно запускать объяснение на Azure Machine Learning Compute и регистрировать информацию об объяснении в службах истории машинного обучения Azure. После регистрации этой информации отчеты и визуализации из объяснения легко доступны на рабочем пространстве Azure Machine Learning для анализа пользователей.


## <a name="next-steps"></a>Дальнейшие действия

Ознакомьтесь с [тем, как](how-to-machine-learning-interpretability-aml.md) обеспечить интерпретацию моделей для обучения как локально, так и на удаленных вычислительных ресурсах Azure Machine Learning. Ознакомьтесь с [образцами тетрадей](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model) для дополнительных сценариев.
