---
title: Основные концепции Kubernetes — служба Azure Kubernetes (AKS)
description: Сведения о компонентах Kubernetes для базового кластера и рабочей нагрузки и из связях с функциями службы Azure Kubernetes (AKS)
services: container-service
ms.topic: conceptual
ms.date: 06/03/2019
ms.openlocfilehash: bcf56aa89a42d65fdb7bf03696faad13c64cbc8a
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "79259647"
---
# <a name="kubernetes-core-concepts-for-azure-kubernetes-service-aks"></a>Ключевые концепции Kubernetes для службы Azure Kubernetes (AKS)

По мере продвижения разработки приложений в направлении контейнерного подхода важно организовать и управлять ресурсами. Kubernetes сейчас является ведущей платформой надежного планирования рабочих нагрузок для отказоустойчивых приложений. Служба Azure Kubernetes (AKS) — это управляемая среда Kubernetes, упрощающая развертывание приложений на основе контейнеров и управление ими.

В этой статье представлены основные компоненты инфраструктуры Kubernetes, такие как *плоскость управления,* *узлы*и *пулы узлов.* Кроме того, здесь дается представление о ресурсах рабочих нагрузок, таких как модули *pod*, *развертывания* и *наборы*, а также о возможности группировать ресурсы в *пространства имен*.

## <a name="what-is-kubernetes"></a>Что такое Kubernetes?

Платформа Kubernetes, которая сейчас очень быстро развивается, предназначена для управления контейнерными приложениями и связанными с ними компонентами сети и хранилища. Основное внимание в ней уделяется рабочим нагрузкам приложений, а не базовым компонентам инфраструктуры. В Kubernetes реализуется декларативный подход к развертываниям, подкрепленный продуманным набором API-интерфейсов для операций управления.

Вы можете создавать и запускать переносимые версии современных приложений на базе микрослужб, использующие возможности Kubernetes по оркестрации и управлению доступностью для поддерживаемых компонентов приложения. Kubernetes поддерживает приложения без отслеживания состояния и с отслеживанием состояния, что очень удобно при адаптации приложений на базе микрослужб.

Kubernetes как открытая платформа позволяет создавать приложения на любом языке программирования, для любой операционной системы, с применением любых библиотек и служб сообщений. С Kubernetes можно интегрировать любые средства непрерывной интеграции и непрерывной поставки (CI/CD) для планирования и развертывания выпусков.

Служба Azure Kubernetes (AKS) предоставляет управляемую службу Kubernetes, которая упрощает выполнение важнейших задач развертывания и управления, в том числе координацию обновлений. Самолет управления AKS управляется платформой Azure, и вы платите только за узлы AKS, которые управляют вашими приложениями. AKS построен на открытом исходном коде Azure Kubernetes Service Engine[(aks-двигатель).][aks-engine]

## <a name="kubernetes-cluster-architecture"></a>Архитектура кластера Kubernetes

Кластер Kubernetes разделяется на два компонента:

- Узлы *плоскости управления* обеспечивают основные услуги Kubernetes и оркестровку рабочих нагрузок приложений.
- *узлы* непосредственно выполняют рабочие нагрузки приложения.

![Компоненты управления плоскостью и узлами Kubernetes](media/concepts-clusters-workloads/control-plane-and-nodes.png)

## <a name="control-plane"></a>Уровень управления

При создании кластера AKS автоматически создается и настраивается плоскость управления. Эта плоскость управления предоставляется как управляемый ресурс Azure, отважитый от пользователя. Для плоскости управления нет никаких затрат, только узлы, которые являются частью кластера AKS.

Самолет управления включает в себя следующие компоненты Kubernetes:

- *kube-apiserver* — сервер API-интерфейсов, который предоставляет базовые API-интерфейсы Kubernetes. Этот компонент поддерживает взаимодействие со средствами управления, например с `kubectl` или панелью мониторинга Kubernetes.
- *etcd* — поддерживает состояния кластера Kubernetes и конфигурации. В Kubernetes поддерживается хранилище ключей *etcd* с высоким уровнем доступности.
- *kube-scheduler* — при создании или масштабировании приложения этот планировщик принимает решения о том, какие узлы могут выполнять рабочую нагрузку, и запускает их.
- *kube-controller-manager* — диспетчер контроллеров управляет работой нескольких небольших контроллеров, которые выполняют такие действия, как репликация модулей pod и обработка операций узлов.

AKS предоставляет плоскость управления с одним арендатором, с выделенным сервером API, Scheduler и т.д. Вы определяете количество и размер узлов, а платформа Azure настраивает безопасную связь между плоскостью управления и узлами. Взаимодействие с плоскость управления происходит через AIS `kubectl` Kubernetes, такие как или панель мониторинга Kubernetes.

Этот управляемый самолет управления означает, что вам не нужно настраивать компоненты, такие как высокодоступный магазин *и т.д.,* но это также означает, что вы не можете получить доступ к плоскости управления напрямую. Обновления kubernetes организуются через портал Azure CLI или Azure, который обновляет плоскость управления, а затем узлы. Чтобы устранить возможные проблемы, можно просмотреть журналы диспетчерских самолетов через журналы Azure Monitor.

Если вам нужно настроить плоскость управления определенным образом или вам нужен прямой доступ к ней, вы можете развернуть свой собственный кластер Kubernetes с помощью [aks-двигателя.][aks-engine]

Для связанных с этим рекомендаций см. [Лучшие практики для кластерной безопасности и обновления в AKS.][operator-best-practices-cluster-security]

## <a name="nodes-and-node-pools"></a>Узлы и пулы узлов

Чтобы запускать приложения и вспомогательные службы, вам нужен *узел* Kubernetes. Кластер AKS содержит один или несколько узлов. Они представляют собой виртуальные машины Azure, которые выполняют компоненты узла Kubernetes и среду выполнения контейнера.

- Это `kubelet` агент Kubernetes, который обрабатывает запросы на оркестровку из плоскости управления и планирование запуска запрошенных контейнеров.
- Поддержка виртуальных сетей обеспечивается прокси-сервером *kube-proxy*, который выполняется в каждом узле. Этот прокси-сервер перенаправляет сетевой трафик и управляет IP-адресами для служб и модулей pod.
- *Среда выполнения контейнера* — это компонент, поддерживающий выполнение контейнерных приложений и их взаимодействие с дополнительными ресурсами, такими как виртуальная сеть и хранилище. В AKS Moby используется в качестве времени выполнения контейнера.

![Виртуальная машина Azure и вспомогательные ресурсы для узла Kubernetes](media/concepts-clusters-workloads/aks-node-resource-interactions.png)

Размер виртуальной Машины Azure для узлов определяет количество ядер ЦП, объем памяти, а также тип и размер хранилища (высокопроизводительные твердотельные накопители или обычные жесткие диски), которые будут доступны. Если вы ожидаете, что приложениям потребуется большой объем ресурсов ЦП и (или) памяти и (или) высокая производительность хранилища, учтите это при выборе размера узла. Вы также можете масштабировать количество узлов в кластере AKS в соответствии с текущими потребностями.

В AKS изображение VM для узлов в кластере в настоящее время основано на Ubuntu Linux или Windows Server 2019. Когда вы создаете кластер AKS или масштабируете количество узлов, платформа Azure создает и настраивает необходимое количество виртуальных машин. Для выполнения ручной конфигурации нет ручной конфигурации. Узлы агента выставляются как стандартные виртуальные машины, поэтому любые скидки на использование VM (включая [бронирование Azure)][reservation-discounts]автоматически применяются.

Если вы хотите использовать другую операционную систему узла, контейнерную среду выполнения или пользовательские пакеты, следует развернуть собственный кластер Kubernetes на основе [aks-engine][aks-engine]. Компоненты и параметры конфигурации становятся доступными для вышестоящего обработчика `aks-engine` раньше, чем начинается официальная поддержка в кластерах AKS. Например, если вы хотите использовать время выполнения контейнера, `aks-engine` кроме Moby, можно настроить и развернуть кластер Kubernetes, отвечающий вашим текущим потребностям.

### <a name="resource-reservations"></a>Резервирование ресурсов

Ресурсы узлов используются AKS для того, чтобы функция узла была частью кластера. Это может создать несоответствие между общими ресурсами вашего узла и ресурсами, выражаемыми при использовании в AKS. Это важно отметить при настройке запросов и ограничений для развернутых пользователем стручков.

Чтобы найти ресурсы, высвоболенные узлам:
```kubectl
kubectl describe node [NODE_NAME]

```

Для поддержания производительности и функциональности узлов ресурсы зарезервированы на каждом уде akS. По мере увеличения ресурсов узла резервирование ресурсов увеличивается из-за большего количества развернутых пользователей стручков, нуждающихся в управлении.

>[!NOTE]
> Использование дополнений AKS, таких как Container Insights (OMS), позволит использовать дополнительные ресурсы узлов.

- **Процессор** - зарезервированный процессор зависит от типа узла и конфигурации кластера, которые могут привести к менее раздаточному процессору из-за запуска дополнительных функций

| Ядра процессора на хостах | 1 | 2 | 4 | 8 | 16 | 32|64|
|---|---|---|---|---|---|---|---|
|Кубе-зарезервирован (милликоры)|60|100|140|180|260|420|740|

- **Память** - память, используемая AKS включает в себя сумму двух значений.

1. Кубелет daemon устанавливается на всех узлах агентов Kubernetes для управления созданием и прекращением контейнеров. По умолчанию на AKS, этот daemon имеет следующее правило выселения: *memory.available<750Mi*, что означает, что узло всегда должно иметь по крайней мере 750 Mi allocatable во все времена.  Когда хост находится ниже этого порога доступной памяти, кубелет завершает один из ходовых стручков, чтобы освободить память на хост-машине и защитить ее. Это реактивное действие, как только доступная память уменьшается за порог750Mi.

2. Второе значение — это прогрессивная скорость резервирования памяти для правильного функционирования кубелета (кубе-зарезервировано).
    - 25% от первых 4 ГБ памяти
    - 20% от следующих 4 ГБ памяти (до 8 ГБ)
    - 10% из следующих 8 ГБ памяти (до 16 ГБ)
    - 6% от следующих 112 ГБ памяти (до 128 ГБ)
    - 2% любой памяти выше 128 ГБ

Вышеуказанные правила распределения памяти и процессора используются для поддержания работоспособности узлов агента, включая некоторые стручки системы хостинга, которые имеют решающее значение для работоспособности кластера. Эти правила распределения также приводят к тому, что узел должен сообщать меньше раздавленной памяти и процессора, чем если бы он не был частью кластера Kubernetes. Вышеупомянутое резервирование ресурсов не может быть изменено.

Например, если узел предлагает 7 ГБ, он сообщит о 34% памяти, не разносняемой в верхней части порога жесткого выселения 750Mi.

`(0.25*4) + (0.20*3) = + 1 GB + 0.6GB = 1.6GB / 7GB = 22.86% reserved`

В дополнение к резервациям для самого Kubernetes, базовая ОС узла также резервирует количество процессоров и ресурсов памяти для поддержания функций ОС.

Для связанных с этим рекомендаций см. [Лучшие практики для основных функций планировщика в AKS.][operator-best-practices-scheduler]

### <a name="node-pools"></a>Пулы узлов

Узлы с одинаковой конфигурацией группируются в *пулы узлов*. Кластер Kubernetes содержит один или несколько пулов узлов. При создании кластера AKS вы указываете начальное количество и размер узлов, которые составляют *пул узлов по умолчанию*. В пуле узлов по умолчанию в AKS содержатся базовые виртуальные машины, на которых выполняются узлы агентов.

> [!NOTE]
> Чтобы обеспечить надежное функционирование кластера, необходимо запустить не менее 2 (двух) узлов в пуле узлов по умолчанию.

Когда вы масштабируете или обновляете кластер AKS, выбранные действия применяются именно к пулу узлов по умолчанию. Вы также можете масштабировать или модернизировать определенный пул узлов. При операциях обновления все запущенные контейнеры поочередно переносятся в другие узлы в том же пуле узлов, пока все узлы не будут успешно обновлены.

Для получения дополнительной информации о том, как использовать несколько пулов узлов в AKS, [см. Создать и управлять несколькими пулями узлов для кластера в AKS.][use-multiple-node-pools]

### <a name="node-selectors"></a>Селекторы узлов

В кластере AKS, который содержит несколько пулов узлов, может потребоваться сообщить планировщику Kubernetes, какой пул узлов использовать для данного ресурса. Например, контроллеры входа не должны работать на узлах Windows Server (в настоящее время в предварительном просмотре в AKS). Селекторы узлов позволяют определить различные параметры, такие как ОС узлов, чтобы контролировать, где стручок должен быть запланирован.

Следующий основной пример графики экземплярNGINX на узлах Linux с помощью селектора узла *"beta.kubernetes.io/os": linux*:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: nginx
spec:
  containers:
    - name: myfrontend
      image: nginx:1.15.12
  nodeSelector:
    "beta.kubernetes.io/os": linux
```

Для получения дополнительной информации о том, как контролировать, где стручки запланированы, [см. Лучшие практики для расширенных функций планировщика в AKS][operator-best-practices-advanced-scheduler].

## <a name="pods"></a>Модули pod

Для запуска экземпляров приложения Kubernetes использует *модули pod*. Каждый pod соответствует одному экземпляру приложения. Обычно с каждым контейнером сопоставляется ровно один pod, но в некоторых сложных сценариях pod может содержать несколько контейнеров. Такие pod с несколькими контейнерами назначаются одному узлу и позволяют контейнерам совместно использовать связанные с ними ресурсы.

При создании стручка можно определить *запросы ресурсов,* чтобы запросить определенное количество ресурсов процессора или памяти. В этом случае планировщик Kubernetes будет стараться распределить pod в узел, содержащий достаточное (запрошенное) количество ресурсов. Вы также можете указать максимальное ограничение на используемые ресурсы, чтобы pod не потреблял слишком много вычислительных ресурсов базового узла. Мы рекомендуем включать ограничения ресурсов для всех pod, чтобы планировщик Kubernetes лучше понимал, какие ресурсы потребуются для работы и какие можно использовать.

Дополнительные сведения см. в [обзоре модулей pod Kubernetes ][kubernetes-pods] и [документации по жизненному циклу pod Kubernetes][kubernetes-pod-lifecycle].

Pod представляет собой логический ресурс, а контейнеры выполняют фактические рабочие нагрузки. Модули pod — это временные одноразовые ресурсы. Отдельное планирование pod лишает вас некоторых возможностей Kubernetes, повышающих уровень доступности и избыточность. Лучше всего доверить развертывание pod и управление ими *контроллерам* Kubernetes, например контроллеру развертывания.

## <a name="deployments-and-yaml-manifests"></a>Развертывания и манифесты YAML

*Развертывание* обозначает один или несколько идентичных модулей pod под управлением контроллера развертывания Kubernetes. Развертывание определяет количество создаваемых *реплик* (pod), а планировщик Kubernetes следит за тем, чтобы при возникновении проблем с pod или узлами своевременно создавались дополнительные модули pod в работоспособных узлах.

Вы можете обновить развертывание, изменяя конфигурацию pod, используемого образа контейнера или хранилища данных. Контроллер развертывания будет освобождать и останавливать некоторое число реплик, а также создавать новые реплики с новым определением развертывания. Этот процесс продолжается, пока не будут обновлены все реплики в развертывании.

Для большинства приложений без отслеживания состояний следует использовать именно такую модель развертывания в AKS, а не распределять отдельные модули pod. Kubernetes может отслеживать работоспособность и состояние развертывания, поддерживая выполнение в кластере необходимого числа реплик. При только расписании отдельных стручков стручки не перезапускаются, если они сталкиваются с проблемой, и не переоцениваются на здоровых узлах, если их текущий узлы сталкиваются с проблемой.

Если приложению важно иметь минимальный доступный набор экземпляров, поддерживающий свободу в принятии решений, процесс обновления не должен нарушать такую возможность. *Бюджеты неработоспособности pod* позволяют определить, сколько реплик в развертывании допустимо одновременно отключать на период обновления или изменения узлов. Например, для развертывания с *5-ю* репликами вы можете указать минимальное ограничение в *4* модуля pod, чтобы в любой момент можно было удалять или переназначать только одну реплику. Как и с ограничениями ресурсов для pod, мы рекомендуем всегда указывать бюджет неработоспособности pod для приложений, для которых требуется постоянное присутствие минимального числа реплик.

Развертывания обычно создаются и управляются с помощью `kubectl create` или `kubectl apply`. Чтобы создать развертывание, определите файл манифеста в формате YAML. В следующем примере создается базовое развертывание веб-сервера NGINX. Для этого развертывания настроено создание *3-х* реплик и открытый порт *80* для контейнера. Также определены требуемые и максимальные объемы ресурсов ЦП и памяти.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.2
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 250m
            memory: 64Mi
          limits:
            cpu: 500m
            memory: 256Mi
```

Чтобы создать более сложные приложения, можно включить в манифест в формате YAML дополнительные службы, например подсистемы балансировки нагрузки.

Дополнительные сведения см. в [документации по развертываниям Kubernetes][kubernetes-deployments].

### <a name="package-management-with-helm"></a>Управление пакетами с помощью Helm

Распространенный подход к управлению приложениями в Kubernetes — применение [Helm][helm]. Вы можете создавать новые или использовать существующие общедоступные *диаграммы* Helm, которые содержат упакованную версию кода приложения и YAML-манифесты Kubernetes для развертывания ресурсов. Эти диаграммы Helm можно хранить локально или в удаленном репозитории, например в [репозитории диаграмм Helm в Реестре контейнеров Azure][acr-helm].

Чтобы использовать Helm, в кластере Kubernetes устанавливается серверный компонент *Tiller*. Tiller управляет установкой диаграмм в пределах кластера. Сам клиент Helm можно установить на локальный компьютер или использовать через [Azure Cloud Shell][azure-cloud-shell]. С помощью клиента вы можете найти или создать диаграммы Helm, а затем установить их в кластере Kubernetes.

![Helm включает клиентский компонент и компонент Tiller на стороне сервера, который создает ресурсы внутри кластера Kubernetes.](media/concepts-clusters-workloads/use-helm.png)

Подробнее см. статью [Установка приложения с помощью Helm в службе Azure Kubernetes][aks-helm].

## <a name="statefulsets-and-daemonsets"></a>StatefulSet и DaemonSet

Контроллер развертывания использует планировщик Kubernetes для выполнения указанного количества реплик в любом доступном узле с достаточными ресурсами. Такой подход к использованию развертываний часто обоснован для приложений без отслеживания состояния, но непригоден для приложений, которым нужно поддержание постоянных имен или хранилищ. Для приложений, реплика которых должна существовать в каждом узле или в определенном наборе узлов в кластере, контроллер развертывания не отслеживает распределение реплик между узлами.

В работе с такими приложениями вам помогут два ресурса Kubernetes:

- наборы *StatefulSet*, которые поддерживают состояние приложений за пределами жизненного цикла отдельных модулей pod, например для хранилища;
- наборы *Daemonset*, которые обеспечивают запуск экземпляров на каждом узле с самых ранних этапов начальной загрузки Kubernetes.

### <a name="statefulsets"></a>Наборы StatefulSet

Современные приложения часто работают без отслеживания состояния, но если отслеживание состояния будет обязательным, например для приложений с компонентами баз данных, вы можете применить наборы *StatefulSet*. Наборы StatefulSet действуют так же, как развертывание одного или нескольких идентичных модулей pod и управление ими. Для реплик, включенных в StatefulSet, соблюдаются мягкие и последовательные процессы развертывания, масштабирования, обновления и прерывания. При сохранении StatefulSet (по мере переноса реплик) сохраняется конвенция именования, имена сетей и хранилище.

Вам достаточно определить приложение в формате YAML с помощью `kind: StatefulSet`, и контроллер StatefulSet возьмет на себя развертывание требуемых реплик, а также управление ими. Данные сохраняются в постоянном хранилище, предоставленном в Управляемых дисках Azure или в службе файлов Azure. При использовании StatefulSet базовое постоянное хранилище сохраняется даже после удаления StatefulSet.

Дополнительные сведения см. в [документации по StatefulSet в Kubernetes][kubernetes-statefulsets].

Включенные в StatefulSet реплики назначаются и выполняются в любом доступном узле кластера AKS. Если вам нужно гарантировать, что на каждом узле выполняется по меньшей мере один модуль pod из набора, правильнее использовать наборы DaemonSet.

### <a name="daemonsets"></a>Наборы DaemonSet

Для некоторых задач сбора журналов и (или) мониторинга, возможно, потребуется выполнять определенный модуль pod во всех узлах или в определенном наборе узлов. Объект *DaemonSet* также предназначен для развертывания одного или нескольких идентичных модулей pod, но, в отличие от предыдущего, контроллер DaemonSet гарантирует выполнение экземпляра pod на каждом из указанных узлов.

Контроллер DaemonSet может распределять модули pod в узлы в самом начале процесса загрузки кластера, еще до запуска стандартного планировщика Kubernetes. Эта возможность гарантирует, что модули pod из набора DaemonSet будут запущены раньше, чем модули pod из основного развертывания или набора StatefulSet.

DaemonSet, как и StatefulSet, включается в определение YAML с помощью `kind: DaemonSet`.

Дополнительные сведения см. в [документации по DaemonSet в Kubernetes][kubernetes-daemonset].

> [!NOTE]
> При использовании [надстройки виртуальных узлов](virtual-nodes-cli.md#enable-virtual-nodes-addon)DaemonSets не будет создавать стручки на виртуальном узеле.

## <a name="namespaces"></a>Пространства имен

Ресурсы Kubernetes, такие как модули pod и развертывания, логически группируются в *пространства имен*. Такая группировка позволяет логически разделить кластер AKS и ограничить права на создание, просмотр ресурсов и управление ими. Например, вы можете создать отдельные пространства имен для разных бизнес-подразделений. Пользователи смогут взаимодействовать только с ресурсами из назначенных им пространств имен.

![Пространства имен Kubernetes для логического разделения ресурсов и приложений](media/concepts-clusters-workloads/namespaces.png)

Когда вы создаете кластер AKS, вам доступны следующие пространства имен:

- *default* (по умолчанию) — в этом пространстве имен по умолчанию создаются модули pod и развертывания, для которых не указано пространство имен. В небольших средах вполне допустимо развертывать все приложения в пространстве имен по умолчанию, не создавая дополнительные логические разделы. Если при любом взаимодействии с API Kubernetes, например через `kubectl get pods`, не указано конкретное пространство имен, всегда используется пространство имен по умолчанию.
- *kube-system* — в этом пространстве имен содержатся основные ресурсы, такие как DNS, прокси-сервер и другие сетевые компоненты, а также панели мониторинга Kubernetes. Обычно вам не нужно развертывать приложения в этом пространстве имен.
- *kube-public* — это пространство имен обычно не используется. Оно позволяет сделать ресурс видимым в пределах всего кластера и доступным для просмотра всем пользователям.

Дополнительные сведения см. в [документации по пространствам имен в Kubernetes][kubernetes-namespaces].

## <a name="next-steps"></a>Дальнейшие действия

Из этой статьи вы узнали об основных компонентах Kubernetes и о том, как они применяются к кластерах AKS. Дополнительные сведения о ключевых концепциях Kubernetes и AKS вы получите в следующих статьях:

- [Доступ и идентификация в Kubernetes и AKS][aks-concepts-identity]
- [Безопасность Kubernetes и AKS][aks-concepts-security]
- [Виртуальные сети Kubernetes и AKS][aks-concepts-network]
- [Хранилище Kubernetes и AKS][aks-concepts-storage]
- [Масштабирование Kubernetes и AKS][aks-concepts-scale]

<!-- EXTERNAL LINKS -->
[aks-engine]: https://github.com/Azure/aks-engine
[kubernetes-pods]: https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/
[kubernetes-pod-lifecycle]: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/
[kubernetes-deployments]: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
[kubernetes-statefulsets]: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
[kubernetes-daemonset]: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
[kubernetes-namespaces]: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
[helm]: https://helm.sh/
[azure-cloud-shell]: https://shell.azure.com

<!-- INTERNAL LINKS -->
[aks-concepts-identity]: concepts-identity.md
[aks-concepts-security]: concepts-security.md
[aks-concepts-scale]: concepts-scale.md
[aks-concepts-storage]: concepts-storage.md
[aks-concepts-network]: concepts-network.md
[acr-helm]: ../container-registry/container-registry-helm-repos.md
[aks-helm]: kubernetes-helm.md
[operator-best-practices-cluster-security]: operator-best-practices-cluster-security.md
[operator-best-practices-scheduler]: operator-best-practices-scheduler.md
[use-multiple-node-pools]: use-multiple-node-pools.md
[operator-best-practices-advanced-scheduler]: operator-best-practices-advanced-scheduler.md
[reservation-discounts]:../cost-management-billing/reservations/save-compute-costs-reservations.md
