---
title: Использование зон доступности в службе Azure Kubernetes (AKS)
description: Узнайте, как создать кластер, который распределяет узлы по зонам доступности в службе Azure Kubernetes (AKS)
services: container-service
ms.custom: fasttrack-edit
ms.topic: article
ms.date: 06/24/2019
ms.openlocfilehash: 5693d9e90de9ba68e7b76e0f2bd5b75141dbda71
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "77596816"
---
# <a name="create-an-azure-kubernetes-service-aks-cluster-that-uses-availability-zones"></a>Создание кластера службы Azure Kubernetes (AKS), использующем зоны доступности

Кластер Azure Kubernetes Service (AKS) распределяет ресурсы, такие как узлы и хранилища, по логическим разделам базовой вычислительной инфраструктуры Azure. Эта модель развертывания гарантирует, что узлы будут работать в отдельных доменах обновления и сбоя в одном центре обработки данных Azure. Кластеры AKS, развернутые с помощью этого поведения по умолчанию, обеспечивают высокий уровень доступности для защиты от сбоя оборудования или запланированного события обслуживания.

Чтобы обеспечить более высокий уровень доступности для приложений, кластеры AKS могут быть распределены по зонам доступности. Эти зоны являются физически отдельными центрами обработки данных в пределах данного региона. Когда компоненты кластера распределены по нескольким зонам, кластер AKS может терпеть сбой в одной из этих зон. Ваши приложения и операции управления по-прежнему доступны, даже если у одного центра обработки данных есть проблемы.

В этой статье показано, как создать кластер AKS и распределить компоненты узлов по зонам доступности.

## <a name="before-you-begin"></a>Перед началом

Вам нужна версия Azure CLI 2.0.76 или более поздняя установка и настройка. Чтобы узнать версию, выполните команду  `az --version`. Если вам необходимо выполнить установку или обновление, см. статью  [Установка Azure CLI][install-azure-cli].

## <a name="limitations-and-region-availability"></a>Ограничения и доступность региона

Кластеры AKS в настоящее время могут быть созданы с использованием зон доступности в следующих регионах:

* Центральная часть США
* восточная часть США 2
* Восточная часть США
* Центральная Франция
* Восточная Япония
* Северная Европа
* Юго-Восточная Азия
* южная часть Соединенного Королевства
* Западная Европа
* западная часть США 2

При создании кластера AKS с использованием зон доступности применяются следующие ограничения:

* Зоны доступности можно включить только при создании кластера.
* Настройки зоны доступности не могут быть обновлены после создания кластера. Также нельзя обновлять существующий кластер зоны отсутствия доступности для использования зон доступности.
* Вы не можете отключить зоны доступности для кластера AKS после его создания.
* Выбранный размер узла (VM SKU) должен быть доступен во всех зонах доступности.
* Кластеры с включенными зонами доступности требуют использования балансеров стандартной нагрузки Azure для распределения по зонам.
* Для развертывания балансеров Стандартной нагрузки необходимо использовать версию Kubernetes 1.13.5 или больше.

Кластеры AKS, использовавшие зоны доступности, должны использовать *стандарт* баланса нагрузки Azure SKU, который является значением по умолчанию для типа балансоровых нагрузок. Этот тип балансировочного баланса нагрузки может быть определен только во время создания кластера. Для получения дополнительной информации и ограничений стандартного балансобаланса нагрузки [см.][standard-lb-limitations]

### <a name="azure-disks-limitations"></a>Ограничения дисков Azure

Объемы, использовавающие управляемые дисками Azure, в настоящее время не являются зональными ресурсами. Стручки, перенесенные в другую зону от исходной зоны, не могут прикрепить их предыдущий диск(ы). Рекомендуется запускать рабочие нагрузки без состояния, которые не требуют постоянного хранения, которые могут возникнуть с зональными проблемами.

Если вы должны запустить состояние рабочих нагрузок, используйте пятна и терпимости в стручка спецификации сказать Kubernetes планировщик для создания стручки в той же зоне, как ваши диски. Кроме того, используйте сетевое хранилище, такое как файлы Azure, которые могут прикрепляться к стручкам по мере их расписания между зонами.

## <a name="overview-of-availability-zones-for-aks-clusters"></a>Обзор зон доступности кластеров AKS

Зоны доступности — это предложение с высокой доступностью, которое защищает ваши приложения и данные от сбоев в работе центра обработки данных. Зоны являются уникальными физическими местами в регионе Azure. Каждая зона состоит из одного или нескольких центров обработки данных, оснащенных независимыми системами электроснабжения, охлаждения и сетевого взаимодействия. Чтобы обеспечить отказоустойчивость, во всех включенных регионах используются минимум три отдельные зоны. Физическое разделение зон доступности в пределах региона защищает приложения и данные от сбоев центров обработки данных. Службы, избыточные зоны, реплицируют ваши приложения и данные в зонах доступности для защиты от одноточек сбоя.

Для получения дополнительной [информации][az-overview]см.

Кластеры AKS, развернутые с использованием зон доступности, могут распределять узлы по нескольким зонам в пределах одного региона. Например, кластер в *регионе East US 2* может создавать узлы во всех трех зонах доступности в *Восточной части США 2.* Такое распределение кластерных ресурсов AKS повышает доступность кластеров, поскольку они устойчивы к сбою определенной зоны.

![Распределение узлов AKS в зонах доступности](media/availability-zones/aks-availability-zones.png)

При сбоях в зоне узлы могут быть сбалансированы вручную или с помощью кластерного автоскалатора. Если одна зона становится недоступной, приложения продолжают работать.

## <a name="create-an-aks-cluster-across-availability-zones"></a>Создание кластера AKS в зонах доступности

При создании кластера с помощью команды `--zones` [az aks][az-aks-create] создается параметр, определяющий, в какие зоны развертываются узлы агента. Компоненты плоскости управления AKS для вашего кластера также распределены `--zones` по зонам в наивысшей доступной конфигурации при определении параметра во время создания кластера.

Если при создании кластера AKS компоненты плоскости управления AKS не определяют зоны для пула агентов по умолчанию, компоненты плоскости управления AKS не будут использовать зоны доступности. Можно добавить дополнительные пулы узлов с помощью [узла az aks, добавляя][az-aks-nodepool-add] команду и указывая `--zones` для этих новых узлов, однако компоненты плоскости управления остаются без осведомленности о зоне доступности. Вы не можете изменить осведомленность о зоне для пула узлов или компонентов плоскости управления AKS после их развертывания.

Следующий пример создает кластер AKS под названием *myAKSCluster* в группе ресурсов под названием *myResourceGroup.* Всего создается *3* узла - один агент в зоне *1,* один из *2,* а затем один из *3.* Компоненты плоскости управления AKS также распределены по зонам в наивысшей доступной конфигурации, так как они определены как часть процесса создания кластера.

```azurecli-interactive
az group create --name myResourceGroup --location eastus2

az aks create \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --generate-ssh-keys \
    --vm-set-type VirtualMachineScaleSets \
    --load-balancer-sku standard \
    --node-count 3 \
    --zones 1 2 3
```

Создание кластера AKS занимает несколько минут.

## <a name="verify-node-distribution-across-zones"></a>Проверка распределения узлов по зонам

Когда кластер будет готов, перечислите узлы агента в наборе масштабов, чтобы увидеть, в какой зоне доступности они развернуты.

Во-первых, получить учетные данные кластера AKS, используя команду [az aks get-credentials:][az-aks-get-credentials]

```azurecli-interactive
az aks get-credentials --resource-group myResourceGroup --name myAKSCluster
```

Затем используйте команду [kubectl,][kubectl-describe] чтобы перечислить узлы в кластере. Фильтр на *failure-domain.beta.kubernetes.io/zone* значение, как показано в следующем примере:

```console
kubectl describe nodes | grep -e "Name:" -e "failure-domain.beta.kubernetes.io/zone"
```

Следующий пример показывает три узла, распределенных по указанному региону и зонам доступности, такие как *eastus2-1* для первой зоны доступности и *eastus2-2* для второй зоны доступности:

```console
Name:       aks-nodepool1-28993262-vmss000000
            failure-domain.beta.kubernetes.io/zone=eastus2-1
Name:       aks-nodepool1-28993262-vmss000001
            failure-domain.beta.kubernetes.io/zone=eastus2-2
Name:       aks-nodepool1-28993262-vmss000002
            failure-domain.beta.kubernetes.io/zone=eastus2-3
```

При добавлении дополнительных узлов в пул агентов платформа Azure автоматически распределяет базовые ВМ по указанным зонам доступности.

Обратите внимание, что в новых версиях Kubernetes (1.17.0 и `topology.kubernetes.io/zone` позже), AKS `failure-domain.beta.kubernetes.io/zone`использует новую этикетку в дополнение к унипраженный .

## <a name="verify-pod-distribution-across-zones"></a>Проверка распределения стручков по зонам

Как описано в [известных этикетках, аннотациях и Taints,][kubectl-well_known_labels]Kubernetes использует `failure-domain.beta.kubernetes.io/zone` метку для автоматического распространения стручков в контроллере репликации или сервисе в различных доступных зонах. Для того, чтобы проверить это, вы можете увеличить свой кластер от 3 до 5 узлов, чтобы проверить правильное распространение стручка:

```azurecli-interactive
az aks scale \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --node-count 5
```

Когда операция масштабирования завершается через несколько минут, команда `kubectl describe nodes | grep -e "Name:" -e "failure-domain.beta.kubernetes.io/zone"` должна дать выход, аналогичный этому образцу:

```console
Name:       aks-nodepool1-28993262-vmss000000
            failure-domain.beta.kubernetes.io/zone=eastus2-1
Name:       aks-nodepool1-28993262-vmss000001
            failure-domain.beta.kubernetes.io/zone=eastus2-2
Name:       aks-nodepool1-28993262-vmss000002
            failure-domain.beta.kubernetes.io/zone=eastus2-3
Name:       aks-nodepool1-28993262-vmss000003
            failure-domain.beta.kubernetes.io/zone=eastus2-1
Name:       aks-nodepool1-28993262-vmss000004
            failure-domain.beta.kubernetes.io/zone=eastus2-2
```

Как видите, теперь у нас есть два дополнительных узла в зонах 1 и 2. Можно развернуть приложение, состоящее из трех реплик. Мы будем использовать NGINX в качестве примера:

```console
kubectl run nginx --image=nginx --replicas=3
```

Если вы проверите, что узлы, где работают ваши стручки, вы увидите, что стручки работают на стручках, соответствующих трем различным зонам доступности. Например, с `kubectl describe pod | grep -e "^Name:" -e "^Node:"` командой вы получите выход, похожий на этот:

```console
Name:         nginx-6db489d4b7-ktdwg
Node:         aks-nodepool1-28993262-vmss000000/10.240.0.4
Name:         nginx-6db489d4b7-v7zvj
Node:         aks-nodepool1-28993262-vmss000002/10.240.0.6
Name:         nginx-6db489d4b7-xz6wj
Node:         aks-nodepool1-28993262-vmss000004/10.240.0.8
```

Как видно из предыдущего вывода, первый модуль работает на уде 0, `eastus2-1`который находится в зоне доступности. Второй стручок работает на уде 2, что `eastus2-3`соответствует, а третий `eastus2-2`в уде 4, в . Без какой-либо дополнительной конфигурации Kubernetes правильно распределяет стручки по всем трем зонам доступности.

## <a name="next-steps"></a>Дальнейшие действия

В этой статье подробно описано, как создать кластер AKS, в который используются зоны доступности. Дополнительные соображения по высокодоступным кластерам можно найти в [AKS рекомендации по непрерывности бизнеса и аварийному восстановлению.][best-practices-bc-dr]

<!-- LINKS - internal -->
[install-azure-cli]: /cli/azure/install-azure-cli
[az-feature-register]: /cli/azure/feature#az-feature-register
[az-feature-list]: /cli/azure/feature#az-feature-list
[az-provider-register]: /cli/azure/provider#az-provider-register
[az-aks-create]: /cli/azure/aks#az-aks-create
[az-overview]: ../availability-zones/az-overview.md
[best-practices-bc-dr]: operator-best-practices-multi-region.md
[aks-support-policies]: support-policies.md
[aks-faq]: faq.md
[standard-lb-limitations]: load-balancer-standard.md#limitations
[az-extension-add]: /cli/azure/extension#az-extension-add
[az-extension-update]: /cli/azure/extension#az-extension-update
[az-aks-nodepool-add]: /cli/azure/ext/aks-preview/aks/nodepool#ext-aks-preview-az-aks-nodepool-add
[az-aks-get-credentials]: /cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials

<!-- LINKS - external -->
[kubectl-describe]: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe
[kubectl-well_known_labels]: https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/
