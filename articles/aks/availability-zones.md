---
title: Использование зон доступности в службе Kubernetes Azure (AKS)
description: Узнайте, как создать кластер, который распределяет узлы между зонами доступности в службе Kubernetes Azure (AKS).
services: container-service
ms.custom: fasttrack-edit
ms.topic: article
ms.date: 06/24/2019
ms.openlocfilehash: 5693d9e90de9ba68e7b76e0f2bd5b75141dbda71
ms.sourcegitcommit: 99ac4a0150898ce9d3c6905cbd8b3a5537dd097e
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/25/2020
ms.locfileid: "77596816"
---
# <a name="create-an-azure-kubernetes-service-aks-cluster-that-uses-availability-zones"></a>Создание кластера Azure Kubernetes Service (AKS), использующего зоны доступности

Кластер Azure Kubernetes Service (AKS) распространяет ресурсы, такие как узлы и хранилище, в логические разделы базовой инфраструктуры вычислений Azure. Эта модель развертывания гарантирует, что узлы выполняются между отдельными доменами обновления и сбоя в одном центре обработки данных Azure. Кластеры AKS, развернутые с этим поведением по умолчанию, обеспечивают высокий уровень доступности для защиты от сбоев оборудования или запланированного обслуживания.

Чтобы обеспечить более высокий уровень доступности приложений, кластеры AKS можно распределять между зонами доступности. Эти зоны являются физически отдельными центрами обработки данных в пределах заданного региона. Когда компоненты кластера распределяются по нескольким зонам, кластер AKS может допускать сбой в одной из этих зон. Ваши приложения и операции управления по-прежнему будут доступны, даже если в одном из центров обработки данных возникла проблема.

В этой статье показано, как создать кластер AKS и распределить компоненты узлов между зонами доступности.

## <a name="before-you-begin"></a>Перед началом

Требуется Azure CLI версии 2.0.76 или более поздней. Чтобы узнать версию, выполните команду  `az --version`. Если необходимо установить или обновить, см. раздел [install Azure CLI][install-azure-cli].

## <a name="limitations-and-region-availability"></a>Ограничения и доступность регионов

В настоящее время кластеры AKS можно создавать с помощью зон доступности в следующих регионах:

* Центральная часть США
* восточная часть США 2
* Восток США
* Центральная Франция
* Восточная Япония
* Северная Европа
* Юго-Восточная Азия
* южная часть Соединенного Королевства
* Западная Европа
* западная часть США 2

При создании кластера AKS с помощью зон доступности применяются следующие ограничения.

* Зоны доступности можно включить только при создании кластера.
* Параметры зоны доступности невозможно обновить после создания кластера. Вы также не можете обновить существующий кластер зоны без доступности для использования зон доступности.
* Вы не сможете отключить зоны доступности для кластера AKS после его создания.
* Выбранный размер узла (SKU виртуальной машины) должен быть доступен во всех зонах доступности.
* Для кластеров с включенными зонами доступности необходимо использовать стандартные подсистемы балансировки нагрузки Azure для распределения между зонами.
* Для развертывания стандартных подсистем балансировки нагрузки необходимо использовать Kubernetes версии 1.13.5 или более поздней.

Кластеры AKS, использующие зоны доступности, должны использовать номер SKU " *стандартный* " балансировщика нагрузки Azure, который является значением по умолчанию для типа подсистемы балансировки нагрузки. Этот тип подсистемы балансировки нагрузки может быть определен только во время создания кластера. Дополнительные сведения и ограничения стандартной подсистемы балансировки нагрузки см. в статье [ограничения стандартного номера SKU для балансировщика нагрузки Azure][standard-lb-limitations].

### <a name="azure-disks-limitations"></a>Ограничения для дисков Azure

Тома, использующие управляемые диски Azure, сейчас не зональные ресурсы. Модули Pod, перезапланированные в другой зоне из исходной зоны, не могут повторно присоединить свои предыдущие диски. Рекомендуется запускать рабочие нагрузки без отслеживания состояния, которые не нуждаются в постоянном хранилище, которое может возникать в зональныеных проблемах.

Если необходимо запустить рабочие нагрузки с отслеживанием состояния, используйте таинтс и допуски в спецификациях Pod, чтобы сообщить планировщику Kubernetes о необходимости создания модулей Pod в той же зоне, что и диски. Кроме того, можно использовать сетевое хранилище, например файлы Azure, которые могут подключаться к модулям Pod, так как они планируются между зонами.

## <a name="overview-of-availability-zones-for-aks-clusters"></a>Обзор зон доступности для кластеров AKS

Зоны доступности — это высокодоступное предложение, защищающее приложения и данные от сбоев центров обработки данных. Зоны являются уникальными физическими расположениями в регионе Azure. Каждая зона состоит из одного или нескольких центров обработки данных, оснащенных независимыми системами электроснабжения, охлаждения и сетевого взаимодействия. Чтобы обеспечить отказоустойчивость, во всех включенных регионах используются минимум три отдельные зоны. Физическое разделение зон доступности в пределах региона защищает приложения и данные от сбоев центров обработки данных. Службы, избыточные в пределах зоны, реплицируют приложения и данные между зонами доступности, чтобы защититься от одной точки отказа.

Дополнительные сведения см. [в статье что такое зоны доступности в Azure?][az-overview].

Кластеры AKS, развернутые с помощью зон доступности, могут распределять узлы по нескольким зонам в одном регионе. Например, кластер в регионе " *Восточная часть США 2* " может создавать узлы во всех трех зонах доступности в *восточной части США 2*. Это распределение ресурсов кластера AKS повышает доступность кластера, так как они устойчивы к сбоям определенной зоны.

![Распределение узлов AKS между зонами доступности](media/availability-zones/aks-availability-zones.png)

В случае сбоя зоны узлы можно перераспределить вручную или с помощью автомасштабирования кластера. Если одна зона становится недоступной, приложения продолжают работать.

## <a name="create-an-aks-cluster-across-availability-zones"></a>Создание кластера AKS в разных зонах доступности

При создании кластера с помощью команды [AZ AKS Create][az-aks-create] параметр `--zones` определяет, в какие узлы агента зоны развертываются. Компоненты плоскости управления AKS для кластера также распределяются между зонами в самой высокой доступной конфигурации при определении параметра `--zones` во время создания кластера.

Если вы не определили зоны для пула агентов по умолчанию при создании кластера AKS, компоненты плоскости управления AKS для кластера не будут использовать зоны доступности. Добавить дополнительные пулы узлов можно с помощью команды [AZ AKS нодепул Add][az-aks-nodepool-add] и указать `--zones` для этих новых узлов, однако компоненты плоскости управления остаются без информации о зоне доступности. Невозможно изменить сведения о зоне для пула узлов или компонентов плоскости управления AKS после их развертывания.

В следующем примере создается кластер AKS с именем *myAKSCluster* в группе ресурсов с именем *myResourceGroup*. Создается всего *3* узла — один агент в зоне *1*, один — в *2*, а затем один — в *3*. Компоненты плоскости управления AKS также распределяются между зонами в максимальной доступной конфигурации, так как они определены как часть процесса создания кластера.

```azurecli-interactive
az group create --name myResourceGroup --location eastus2

az aks create \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --generate-ssh-keys \
    --vm-set-type VirtualMachineScaleSets \
    --load-balancer-sku standard \
    --node-count 3 \
    --zones 1 2 3
```

Создание кластера AKS занимает несколько минут.

## <a name="verify-node-distribution-across-zones"></a>Проверка распределения узлов между зонами

Когда кластер будет готов, перечислите узлы агента в масштабируемом наборе, чтобы увидеть, в какой зоне доступности они развернуты.

Сначала получите учетные данные кластера AKS с помощью команды [AZ AKS Get-Credential][az-aks-get-credentials] :

```azurecli-interactive
az aks get-credentials --resource-group myResourceGroup --name myAKSCluster
```

Затем используйте команду [kubectl Опишите][kubectl-describe] , чтобы вывести список узлов в кластере. Выполните фильтрацию по значению *Failure-domain.Beta.kubernetes.IO/Zone* , как показано в следующем примере:

```console
kubectl describe nodes | grep -e "Name:" -e "failure-domain.beta.kubernetes.io/zone"
```

В следующем примере выходных данных показаны три узла, распределенные по заданному региону и зонам доступности, например *eastus2-1* для первой зоны доступности и *eastus2-2* для второй зоны доступности:

```console
Name:       aks-nodepool1-28993262-vmss000000
            failure-domain.beta.kubernetes.io/zone=eastus2-1
Name:       aks-nodepool1-28993262-vmss000001
            failure-domain.beta.kubernetes.io/zone=eastus2-2
Name:       aks-nodepool1-28993262-vmss000002
            failure-domain.beta.kubernetes.io/zone=eastus2-3
```

При добавлении дополнительных узлов в пул агентов Платформа Azure автоматически распределяет базовые виртуальные машины в указанных зонах доступности.

Обратите внимание, что в более новых версиях Kubernetes (1.17.0 и более поздних версий) AKS использует более новую метку `topology.kubernetes.io/zone` в дополнение к устаревшим `failure-domain.beta.kubernetes.io/zone`.

## <a name="verify-pod-distribution-across-zones"></a>Проверка распределения Pod между зонами

Как описано в [хорошо известных метках, Annotations и таинтс][kubectl-well_known_labels], Kubernetes использует метку `failure-domain.beta.kubernetes.io/zone` для автоматического распределения модулей Pod в контроллере или службе репликации в разных доступных зонах. Чтобы протестировать это, можно увеличить масштаб кластера с 3 до 5 узлов, чтобы убедиться в правильности распространения Pod:

```azurecli-interactive
az aks scale \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --node-count 5
```

Когда операция масштабирования завершается через несколько минут, команда `kubectl describe nodes | grep -e "Name:" -e "failure-domain.beta.kubernetes.io/zone"` должна вывести результат, аналогичный приведенному в этом примере:

```console
Name:       aks-nodepool1-28993262-vmss000000
            failure-domain.beta.kubernetes.io/zone=eastus2-1
Name:       aks-nodepool1-28993262-vmss000001
            failure-domain.beta.kubernetes.io/zone=eastus2-2
Name:       aks-nodepool1-28993262-vmss000002
            failure-domain.beta.kubernetes.io/zone=eastus2-3
Name:       aks-nodepool1-28993262-vmss000003
            failure-domain.beta.kubernetes.io/zone=eastus2-1
Name:       aks-nodepool1-28993262-vmss000004
            failure-domain.beta.kubernetes.io/zone=eastus2-2
```

Как видите, у нас теперь есть два дополнительных узла в зонах 1 и 2. Можно развернуть приложение, состоящее из трех реплик. Мы будем использовать NGINX в качестве примера:

```console
kubectl run nginx --image=nginx --replicas=3
```

Если проверить, что узлы, на которых работают модули Pod, будут работать в модулях Pod, соответствующих трем различным зонам доступности. Например, с помощью команды `kubectl describe pod | grep -e "^Name:" -e "^Node:"` вы получите примерно такой результат:

```console
Name:         nginx-6db489d4b7-ktdwg
Node:         aks-nodepool1-28993262-vmss000000/10.240.0.4
Name:         nginx-6db489d4b7-v7zvj
Node:         aks-nodepool1-28993262-vmss000002/10.240.0.6
Name:         nginx-6db489d4b7-xz6wj
Node:         aks-nodepool1-28993262-vmss000004/10.240.0.8
```

Как видно из предыдущих выходных данных, первый модуль выполняется на узле 0, который находится в зоне доступности `eastus2-1`. Второй модуль выполняется на узле 2, который соответствует `eastus2-3`, а третий — в узле 4 в `eastus2-2`. Без какой-либо дополнительной настройки Kubernetes правильно распределяет модули Pod по всем трем зонам доступности.

## <a name="next-steps"></a>Следующие шаги

В этой статье подробно описано, как создать кластер AKS, использующий зоны доступности. Дополнительные сведения о кластерах высокой доступности см. [в статье рекомендации по обеспечению непрерывности бизнес-процессов и аварийному восстановлению в AKS][best-practices-bc-dr].

<!-- LINKS - internal -->
[install-azure-cli]: /cli/azure/install-azure-cli
[az-feature-register]: /cli/azure/feature#az-feature-register
[az-feature-list]: /cli/azure/feature#az-feature-list
[az-provider-register]: /cli/azure/provider#az-provider-register
[az-aks-create]: /cli/azure/aks#az-aks-create
[az-overview]: ../availability-zones/az-overview.md
[best-practices-bc-dr]: operator-best-practices-multi-region.md
[aks-support-policies]: support-policies.md
[aks-faq]: faq.md
[standard-lb-limitations]: load-balancer-standard.md#limitations
[az-extension-add]: /cli/azure/extension#az-extension-add
[az-extension-update]: /cli/azure/extension#az-extension-update
[az-aks-nodepool-add]: /cli/azure/ext/aks-preview/aks/nodepool#ext-aks-preview-az-aks-nodepool-add
[az-aks-get-credentials]: /cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials

<!-- LINKS - external -->
[kubectl-describe]: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe
[kubectl-well_known_labels]: https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/
