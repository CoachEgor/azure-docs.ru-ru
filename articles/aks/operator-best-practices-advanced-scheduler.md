---
title: Рекомендации для оператора — расширенные возможности планировщика в службе Azure Kubernetes (AKS)
description: Ознакомьтесь с рекомендациями для операторов по использованию расширенных возможностей планировщика в службе Azure Kubernetes (AKS), включая отметки и толерантности, селекторы узлов, подобие, подобие модулей pod и анти-подобие
services: container-service
author: mlearned
ms.service: container-service
ms.topic: conceptual
ms.date: 11/26/2018
ms.author: mlearned
ms.openlocfilehash: 4caa4219d2bf7558dbdf71e92e4993722c6e8f6a
ms.sourcegitcommit: 6a42dd4b746f3e6de69f7ad0107cc7ad654e39ae
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/07/2019
ms.locfileid: "67614874"
---
# <a name="best-practices-for-advanced-scheduler-features-in-azure-kubernetes-service-aks"></a>Рекомендации по расширенным возможностям планировщика в службе Azure Kubernetes (AKS)

При управлении кластерами в Cлужбе Azure Kubernetes (AKS) часто возникает необходимость изолировать команды и рабочие нагрузки. Расширенные функции планировщика Kubernetes дают возможность контролировать, в какие узлы распределяются определенные модули pod или как распределить по кластеру приложения с несколькими модулями pod. 

В этой статье с рекомендациями главное внимание уделяется расширенным возможностям планирования Kubernetes для операторов кластера. В этой статье раскрываются следующие темы:

> [!div class="checklist"]
> * Использование отметок и толерантностей для ограничения модулей pod, которые могут назначаться на узлы
> * Назначение модулям pod предпочтений для запуска на определенных узлах с помощью селекторов узлов или подобия узлов
> * Разделение или группирование модулей pod на основе их подобия или анти-подобия

## <a name="provide-dedicated-nodes-using-taints-and-tolerations"></a>Выделение узлов на основе отметок и толерантностей

**Рекомендация.** Ограничьте ресурсоемким приложениям (например, контроллерам входящего трафика) доступ к определенным узлам. Сохраните ресурсы для нуждающихся в них важных рабочих нагрузок, не разрешая назначать другие нагрузки на эти узлы.

При создании кластера AKS можно развернуть узлы с поддержкой GPU или множеством мощных процессоров. Такие узлы часто используются для рабочих нагрузок по обработке больших массивов данных, например для задач машинного обучения или искусственного интеллекта. Так как обычно подобное оборудование — дорогостоящий для развертывания ресурс, имеет смысл ограничить рабочую нагрузку, которую можно назначить на эти узлы. Или же можно выделить определенные узлы в кластере специально для служб входящего трафика, исключив для них другую рабочую нагрузку.

Эта поддержка для различных узлов предоставляется с помощью нескольких пулов узлов. Кластер службы контейнеров AZURE предоставляет один или несколько пулов узлов. Поддержка нескольких пулов узлов в AKS доступна в предварительной версии.

Для ограничения рабочих нагрузок, которые могут выполняться на узлах, в планировщике Kubernetes используются параметры taint (отметка) и toleration (толерантность).

* **Отметка** применяется к узлу, указывая, что на него могут назначаться только определенные модули pod.
* Затем к модулям pod применяется параметр **toleration**, указывающий, что они *допускают* метку узла.

При развертывании модуля pod в кластере AKS модули назначаются только на те узлы, толерантность которых соответствует имеющейся отметке. Например предположим, что поддерживает пул узлов в кластере AKS для узлов с графическим Процессором. Задаем имя, например *gpu*, и значение для назначения. Если задано значение *NoSchedule*, планировщик Kubernetes не может назначить на этот узел модули pod, для которых не задана соответствующая толерантность.

```console
kubectl taint node aks-nodepool1 sku=gpu:NoSchedule
```

Применив отметку к узлам, зададим толерантность в спецификации модулей pod, разрешающую назначение на такие узлы. В следующем примере задаются значения `sku: gpu` и `effect: NoSchedule`, допускающие отметку, примененную к узлу на предыдущем шаге:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: tf-mnist
spec:
  containers:
  - name: tf-mnist
    image: microsoft/samples-tf-mnist-demo:gpu
    resources:
      requests:
        cpu: 0.5
        memory: 2Gi
      limits:
        cpu: 4.0
        memory: 16Gi
  tolerations:
  - key: "sku"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
```

При развертывании этого модуля pod, например с помощью `kubectl apply -f gpu-toleration.yaml`, Kubernetes может успешно назначать данный модуль на узлы с примененной отметкой. Такое логическое ограничение позволяет управлять доступом к ресурсам в пределах кластера.

Если вы применяете отметки, обсудите с владельцами и разработчиками приложений, какие толерантности нужны им для развертывания.

Дополнительные сведения о taints и tolerations, см. в разделе [применение taints и tolerations][k8s-taints-tolerations].

Дополнительные сведения об использовании нескольких пулов узлов в AKS, см. в разделе [создать и управлять несколькими пулы узлов в кластере AKS][use-multiple-node-pools].

### <a name="behavior-of-taints-and-tolerations-in-aks"></a>Поведение taints и tolerations в AKS

При обновлении пул узлов в AKS, taints и tolerations выполните заданному шаблону применяемые на новых узлах.

- **По умолчанию кластеры без поддержки масштабирования виртуальной машины**
  - Предположим, у вас есть кластер с двумя узлами - *node1* и *node2*. При обновлении, дополнительный узел (*node3*) создается.
  - Taints из *node1* применяются к *node3*, затем *node1* удаляется.
  - Создается другой новый узел (с именем *node1*, с момента предыдущего *node1* была удалена) и *node2* taints применяются к новому *node1*. Затем *node2* удаляется.
  - По сути *node1* становится *node3*, и *node2* становится *node1*.

- **Кластеры, использующие виртуальную машину масштабируемых наборов** (сейчас в предварительной версии в AKS)
  - Опять же, давайте предположим, у вас есть кластер с двумя узлами - *node1* и *node2*. Вы обновить пуле узла.
  - Создаются два дополнительных узла, *node3* и *node4*, а taints передаются в соответственно.
  - Исходный *node1* и *node2* удаляются.

При масштабировании пул узлов в AKS taints и tolerations без выполнения посредством конструктора.

## <a name="control-pod-scheduling-using-node-selectors-and-affinity"></a>Управление назначением моделей pod с помощью селекторов узлов и подобия

**Рекомендация.** Управляйте назначением модулей pod на узлы с помощью таких параметров, как node selector (селектор узла), node affinity (подобие узла) и inter-pod affinity (подобие между модулями pod). Эти параметры позволяют планировщику Kubernetes логически ограничивать рабочие нагрузки, например, аппаратно в узле.

Отметки и толерантности используются для жесткой логической изоляции ресурсов: если модуль pod не допускает отметку узла, он не назначается на этот узел. Альтернативный подход — использование селекторов узла. В этом случае вы помечаете узлы, например, чтобы указать наличие локального хранилища SSD или большого объема памяти, а затем задаете селектор узла в спецификации модуля pod. После этого планировщик Kubernetes назначает модули pod на соответствующий узел. В отличие от толерантности, модули pod без соответствующего селектора узла могут назначаться на помеченные узлы. Такое поведение позволяет задействовать неиспользуемые ресурсы на узлах, отдавая приоритет модулям pod, для которых задан соответствующий селектор узла.

Рассмотрим пример узлов с большим объемом памяти. Эти узлы могут отдавать предпочтение модулям pod, запрашивающим большой объем памяти. Но чтобы ресурсы не простаивали, они разрешают запускать и другие модули pod.

```console
kubectl label node aks-nodepool1 hardware:highmem
```

Затем спецификация pod добавляет свойство `nodeSelector`, чтобы задать селектор узла, соответствующий метке на узле:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: tf-mnist
spec:
  containers:
  - name: tf-mnist
    image: microsoft/samples-tf-mnist-demo:gpu
    resources:
      requests:
        cpu: 0.5
        memory: 2Gi
      limits:
        cpu: 4.0
        memory: 16Gi
    nodeSelector:
      hardware: highmem
```

Если вы используете эти параметры планировщика, обсудите с владельцами и разработчиками приложений, как правильно задать спецификации pod.

Дополнительные сведения об использовании селекторы узла см. в разделе [назначение модулей и узлы][k8s-node-selector].

### <a name="node-affinity"></a>Подобие узла

Селектор узла — это базовый способ назначения модулей pod на определенные узлы. Использование параметра *node affinity* (подобие узла) предоставляет дополнительные возможности. С помощью этого параметра можно указать, что происходит в случае, когда модуль pod не соответствует узлу. Вы можете *требовать*, чтобы планировщик Kubernetes подбирал pod в соответствии с помеченным хостом. Или же вы можете *отдавать предпочтение* такому соответствию, но разрешать назначать pod на другой хост, если нет соответствующего.

В следующем примере для подобия узла задано значение *requiredDuringSchedulingIgnoredDuringExecution*. Этот параметр требует, чтобы планировщик Kubernetes использовал узел с соответствующей меткой. Если нет ни одного подходящего узла, модуль pod ожидает назначения. Чтобы разрешить назначить pod на другой узел, нужно установить значение *preferredDuringScheduledIgnoreDuringExecution*:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: tf-mnist
spec:
  containers:
  - name: tf-mnist
    image: microsoft/samples-tf-mnist-demo:gpu
    resources:
      requests:
        cpu: 0.5
        memory: 2Gi
      limits:
        cpu: 4.0
        memory: 16Gi
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: hardware
            operator: In
            values: highmem
```

Часть *IgnoredDuringExecution* указывает, что при смене меток узла не нужно удалять с него pod. Планировщик Kubernetes использует обновленные метки узла только для назначения новых модулей pod, а не для тех, которые уже назначены на этот узел.

Дополнительные сведения см. в разделе [сходства и близости][k8s-affinity].

### <a name="inter-pod-affinity-and-anti-affinity"></a>Подобие между модулями pod и анти-подобие

Последний способ логической изоляции рабочих нагрузок планировщиком Kubernetes — использование параметров inter-pod affinity (подобие между модулями pod) и anti-affinity (анти-подобие). Эти параметры указывают, что либо *не следует* назначать модули pod на узел, для которого уже есть соответствующий pod, либо же что это *следует* делать. По умолчанию планировщик Kubernetes пытается распределить множество модулей pod в наборе реплик по узлам. Вы можете задать это поведение более конкретными правилами.

Хороший пример — веб-приложение, использующее кэш Redis для Azure. Вы можете использовать правила анти-подобия модулей pod, чтобы планировщик Kubernetes распределял реплики по узлам. Может впоследствии использовать правила соответствия, чтобы убедиться в том, что каждый компонент веб-приложения запланировано на том же узле соответствующего кэша. Вот как выглядит распределение модулей pod между узлами:

| **Узел 1** | **Узел 2** | **Узел 3** |
|------------|------------|------------|
| веб-приложение-1   | веб-приложение-2   | веб-приложение-3   |
| кэш-1    | кэш-2    | кэш-3    |

Это пример более сложного развертывания, чем в случае использования селекторов узла или подобия узлов. Такое развертывание позволяет управлять тем, как Kubernetes назначает модули pod на узлы, и логически изолировать ресурсы. Полный пример этого веб-приложения с кэшем Azure Redis пример, см. в разделе [совместное размещение модулей POD на одном узле][k8s-pod-affinity].

## <a name="next-steps"></a>Следующие шаги

Эта статья посвящена расширенным возможностям планировщика Kubernetes. Дополнительную информацию об операциях кластера в AKS см. в рекомендациях на такие темы:

* [Изоляция Мультитенантность и кластера][aks-best-practices-scheduler]
* [Основные возможности планировщик Kubernetes][aks-best-practices-scheduler]
* [Аутентификация и авторизация][aks-best-practices-identity]

<!-- EXTERNAL LINKS -->
[k8s-taints-tolerations]: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
[k8s-node-selector]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
[k8s-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
[k8s-pod-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#always-co-located-in-the-same-node

<!-- INTERNAL LINKS -->
[aks-best-practices-scheduler]: operator-best-practices-scheduler.md
[aks-best-practices-cluster-isolation]: operator-best-practices-cluster-isolation.md
[aks-best-practices-identity]: operator-best-practices-identity.md
[use-multiple-node-pools]: use-multiple-node-pools.md
