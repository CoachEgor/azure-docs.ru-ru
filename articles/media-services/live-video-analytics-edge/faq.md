---
title: Интерактивная аналитика видео на IoT Edge часто задаваемые вопросы — Azure
description: В этом разделе содержатся ответы на IoT Edge часто задаваемые вопросы о службе "Анализ видео".
ms.topic: conceptual
ms.date: 12/01/2020
ms.openlocfilehash: 521cd0e4f5fc8232a000e10520298a979ba1c14c
ms.sourcegitcommit: cc13f3fc9b8d309986409276b48ffb77953f4458
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/14/2020
ms.locfileid: "97401582"
---
# <a name="frequently-asked-questions-faqs"></a>Часто задаваемые вопросы (FAQ)

В этом разделе содержатся ответы на IoT Edge часто задаваемые вопросы о службе "Анализ видео".

## <a name="general"></a>Общие сведения

### <a name="what-are-the-system-variables-that-can-be-used-in-graph-topology-definition"></a>Какие системные переменные можно использовать в определении топологии графа?

|Переменная   |Описание|
|---|---|
|[System.DateTime](/dotnet/framework/data/adonet/sql/linq/system-datetime-methods)|Представляет время в формате UTC, обычно выраженное в виде даты и времени суток (базовое представление Ииииммддсхммссз).|
|System. ПреЦиседатетиме|Представляет экземпляр даты и времени в формате UTC в формате, совместимом с файлом ISO8601, с миллисекундами (базовое представление ГГГГММДДTччммсс недоступен. fffZ).|
|System. Графтопологинаме   |Представляет топологию графа мультимедиа, содержащую схему графа.|
|System. Графинстанценаме|  Представляет экземпляр графа мультимедиа, содержит значения параметров и ссылается на топологию.|

## <a name="configuration-and-deployment"></a>Конфигурация и развертывание

### <a name="can-i-deploy-the-media-edge-module-to-a-windows-10-device"></a>Можно ли развернуть модуль Media ребра на устройстве с Windows 10?

Да. См. статью о [контейнерах Linux в Windows 10](/virtualization/windowscontainers/deploy-containers/linux-containers).

## <a name="capture-from-ip-camera-and-rtsp-settings"></a>Запись из IP-камеры и параметров RTSP

### <a name="do-i-need-to-use-a-special-sdk-on-my-device-to-send-in-a-video-stream"></a>Нужно ли использовать специальный пакет SDK на устройстве для отправки видеопотока?

Нет. Интерактивная аналитика видео на IoT Edge поддерживает запись мультимедиа с помощью протокола потоковой передачи видео RTSP (поддерживается в большинстве IP-камер).

### <a name="can-i-push-media-to-live-video-analytics-on-iot-edge-using-rtmp-or-smooth-like-a-media-services-live-event"></a>Можно ли передавать мультимедиа в Live Video Analytics на IoT Edge с помощью RTMP или Smooth (например, в реальном времени служб мультимедиа)?

* Нет. Интерактивная аналитика видео поддерживает только протокол RTSP для записи видео с IP-камер.
* Должна работать любая камера, поддерживающая потоковую передачу RTSP по протоколу TCP/HTTP. 

### <a name="can-i-reset-or-update-the-rtsp-source-url-on-a-graph-instance"></a>Можно ли сбросить или обновить URL-адрес источника RTSP в экземпляре графа?

Да, если экземпляр графа находится в неактивном состоянии.  

### <a name="is-there-a-rtsp-simulator-available-to-use-during-testing-and-development"></a>Доступен ли симулятор RTSP для использования во время тестирования и разработки?

Да. Существует модуль пограничной [симулятора RTSP](https://github.com/Azure/live-video-analytics/tree/master/utilities/rtspsim-live555) для использования в руководствах по быстрому запуску и учебников для поддержки процесса обучения. Этот модуль является единственным возможным средством и может быть не всегда доступен. Настоятельно рекомендуется не использовать его в течение нескольких часов. Перед планированием рабочего развертывания следует протестировать тестирование с использованием действительного источника RTSP.

### <a name="do-you-support-onvif-discovery-of-ip-cameras-at-the-edge"></a>Поддерживается ли обнаружение IP-камер на пограничном устройстве с помощью ONVIF?

Нет, обнаружение устройств на пограничном устройстве с помощью ONVIF не поддерживается.

## <a name="streaming-and-playback"></a>Потоковая передача и воспроизведение

### <a name="can-assets-recorded-to-ams-from-the-edge-be-played-back-using-media-services-streaming-technologies-like-hls-or-dash"></a>Можно ли воспроизводить активы, записанные в AMS, с помощью таких технологий потоковой передачи мультимедиа, как HLS или ТИРЕ?

Да. Записанные ресурсы можно передавать в поток, как и любые другие ресурсы в службах мультимедиа Azure. Чтобы выполнить потоковую передачу содержимого, необходимо создать конечную точку потоковой передачи и в состоянии выполнения. С помощью стандартного процесса создания указателя потоковой передачи вы получите доступ к манифесту HLS или ТИРЕ для потоковой передачи на любую поддерживаемую платформу проигрывателя. Дополнительные сведения о создании манифестов HLS или ТИРЕ публикации см. в статье [Динамическая упаковка](../latest/dynamic-packaging-overview.md).

### <a name="can-i-use-the-standard-content-protection-and-drm-features-of-media-services-on-an-archived-asset"></a>Можно ли использовать стандартные функции защиты содержимого и DRM для служб мультимедиа в архивном ресурсе?

Да. Все стандартные функции защиты содержимого динамического шифрования и DRM доступны для использования в ресурсах, записанных на графе мультимедиа.

### <a name="what-players-can-i-use-to-view-content-from-the-recorded-assets"></a>Какие игроки можно использовать для просмотра содержимого из записанных ресурсов?

Поддерживаются все стандартные проигрыватели, поддерживающие совместимые Apple HTTP Live Streaming (HLS) версии 3 или 4. Кроме того, поддерживается любой проигрыватель, поддерживающий воспроизведение с поддержкой MPEG-ТИРЕ.

Рекомендуемые игроки для тестирования включают:

* [Проигрыватель мультимедиа Azure](../latest/use-azure-media-player.md)
* [HLS.js](https://hls-js.netlify.app/demo/)
* [Video.js](https://videojs.com/)
* [Dash.js](https://github.com/Dash-Industry-Forum/dash.js/wiki)
* [Проигрыватель Шака](https://github.com/google/shaka-player)
* [ExoPlayer](https://github.com/google/ExoPlayer)
* [Собственный HTTP Live Streaming Apple](https://developer.apple.com/streaming/)
* Пограничная, Chrome или Safari, созданные в видеопроигрывателе HTML5
* Коммерческие игроки, поддерживающие воспроизведение HLS или ТИРЕ

### <a name="what-are-the-limits-on-streaming-a-media-graph-asset"></a>Каковы ограничения на потоковую передачу ресурса графа мультимедиа?

Потоковая передача активного или записанного ресурса из графа мультимедиа использует ту же высокомасштабируемый инфраструктурой и конечную точку потоковой передачи, которую службы мультимедиа поддерживает по запросу и потоковую передачу мультимедиа & развлекательных, OTT и широковещательных клиентов. Это означает, что вы можете быстро и легко включить Azure CDN, Verizon или Akamai для доставки содержимого в аудиторию как небольшое число зрителей или до миллионов в зависимости от вашего сценария.

Содержимое может доставляться с помощью Apple HTTP Live Streaming (HLS) или MPEG-ТИРЕ.

## <a name="design-your-ai-model"></a>Проектирование модели искусственного интеллекта 

### <a name="i-have-multiple-ai-models-wrapped-in-a-docker-container-how-should-i-use-them-with-live-video-analytics"></a>В контейнере DOCKER есть несколько моделей искусственного интеллекта. Как использовать их с помощью функции Live Video Analytics? 

Решения отличаются в зависимости от протокола связи, используемого сервером для обмена данными с функцией Live Video Analytics. Ниже приведены некоторые способы этого.

#### <a name="http-protocol"></a>Протокол HTTP:

* Один контейнер (один Лваекстенсион):  

   На используемом сервере можно использовать один порт, но разные конечные точки для различных моделей искусственного интеллекта. Например, для примера Python можно использовать разные `route` s для каждой модели: 

   ```
   @app.route('/score/face_detection', methods=['POST']) 
   … 
   Your code specific to face detection model… 

   @app.route('/score/vehicle_detection', methods=['POST']) 
   … 
   Your code specific to vehicle detection model 
   … 
   ```

   Затем в развертывании Live Video Analytics при создании экземпляра графов задайте URL-адрес сервера вывода для каждого экземпляра следующим образом: 

   Первый экземпляр: URL-адрес сервера вывода =`http://lvaExtension:44000/score/face_detection`<br/>
   второй экземпляр: URL-адрес сервера вывода =`http://lvaExtension:44000/score/vehicle_detection`  
    > [!NOTE]
    > Кроме того, можно также предоставлять модели искусственного интеллекта на разных портах и вызывать их при создании экземпляров графов.  

* Несколько контейнеров: 

   Каждый контейнер развертывается с другим именем. В настоящее время в документации по аналитическим видео в реальном времени мы показали, как развернуть расширение с именем: **лваекстенсион**. Теперь можно разрабатывать два разных контейнера. Каждый контейнер имеет один и тот же интерфейс HTTP (то есть `/score` Конечная точка). Разверните эти два контейнера с разными именами и убедитесь, что оба они прослушивают **разные порты**. 

   Например, один контейнер с именем `lvaExtension1` прослушивает порт `44000` , другой контейнер с именем `lvaExtension2` прослушивает порт `44001` . 

   В топологии Live Video Analytics создаются два графика с разными URL-адресами вывода, такими как: 

   Первый экземпляр: URL-адрес сервера вывода = `http://lvaExtension1:44001/score`    
   Второй экземпляр: URL-адрес сервера вывода = `http://lvaExtension2:44001/score`
   
#### <a name="grpc-protocol"></a>Протокол GRPC: 

Если используется модуль Live Video Analytics 1,0, то при использовании протокола gRPC единственным способом было бы то, что сервер gRPC получит различные модели AI через разные порты. В [этом примере](https://raw.githubusercontent.com/Azure/live-video-analytics/master/MediaGraph/topologies/grpcExtension/topology.json)имеется один порт 44000, предоставляющий все модели Йоло. Теоретически сервер Йоло gRPC можно было переписывать, чтобы предоставить некоторые модели в 44000, другие в 45000,... 

При использовании модуля Live Video Analytics 2,0 в узел расширения gRPC добавляется новое свойство. Это свойство называется **екстенсионконфигуратион** , которое является необязательной строкой, которая может использоваться как часть контракта gRPC. При наличии нескольких моделей искусственного интеллекта, упакованных в один сервер вывода, вам не нужно предоставлять узел для каждой модели искусственного интеллекта. Вместо этого для экземпляра Graph поставщик расширений (вы) может определить, как выбрать различные модели AI с помощью свойства **екстенсионконфигуратион** и во время выполнения служба Live Video Analytics передаст эту строку серверу, который может использовать его для вызова требуемой модели ИСКУССТВЕНного интеллекта. 

### <a name="i-am-building-a-grpc-server-around-an-ai-model-and-want-to-be-able-to-support-being-used-by-multiple-camerasgraph-instances-how-should-i-build-my-server"></a>Я создаю сервер gRPC на основе модели искусственного интеллекта и хочу поддерживать использование несколькими экземплярами камер и графов. Как создать сервер? 

 Во-первых, убедитесь, что сервер может обрабатывать более одного запроса одновременно. Или убедитесь, что сервер работает в параллельных потоках. 

Например, в одном из [примеров GRPC для Live Video Analytics](https://github.com/Azure/live-video-analytics/blob/master/utilities/video-analysis/notebooks/Yolo/yolov3/yolov3-grpc-icpu-onnx/lvaextension/server/server.py)имеется число параллельных каналов, установленное по умолчанию. См. 

```
server = grpc.server(futures.ThreadPoolExecutor(max_workers=3)) 
```

В описанном выше экземпляре сервера gRPC сервер может одновременно открыть только три канала на камеру (например, для каждого экземпляра топологии графика). Не пытайтесь подключить к серверу более трех экземпляров. Если вы попытаетесь открыть более трех каналов, запросы будут ожидать, пока не будет удален существующий.  

В примерах Python используется выше реализация сервера gRPC. Разработчики могут реализовать собственные серверы или в приведенной выше реализации по умолчанию могут увеличить число рабочих процессов, заданное числом камер, используемых для получения видеоканала. 

Чтобы настроить и использовать несколько камер, разработчики могут создать экземпляр топологии с несколькими графами, где каждый экземпляр указывает на тот же или другой сервер вывода (например, сервер, упомянутый в приведенном выше абзаце). 

### <a name="i-want-to-be-able-to-receive-multiple-frames-from-upstream-before-i-make-an-inferencing-decision-how-can-i-enable-that"></a>Я хочу иметь возможность получать несколько кадров из вышестоящей части, прежде чем принимать решение о принятии решения. Как это можно сделать? 

Текущие [примеры по умолчанию](https://github.com/Azure/live-video-analytics/tree/master/utilities/video-analysis) работают в режиме без отслеживания состояния. Эти образцы не поддерживают состояние предыдущих вызовов и даже те, кто вызывал (то есть экземпляр топологии может вызывать один и тот же сервер вывода, и сервер не сможет определить, кто вызывает и состояние для каждого вызывающего объекта). 

#### <a name="http-protocol"></a>протокол HTTP;

При использовании протокола HTTP: 

Для сохранения состояния каждый вызывающий объект (экземпляр топологии графа) будет вызывать сервер, обращающийся к серверу, с параметром HTTP-запроса, который является уникальным для вызывающего. Например, URL-адрес сервера вывода для  

экземпляр 1-го топологии = `http://lvaExtension:44000/score?id=1`<br/>
экземпляр второй топологии = `http://lvaExtension:44000/score?id=2`

… 

На стороне сервера маршрут оценки будет узнавать, кто вызывает. Если ID = 1, то он может обеспечить отдельное состояние для этого вызывающего объекта (экземпляр топологии графа). Затем можно удержать полученные кадры в буфере (например, массив или словарь с ключом DateTime, а значение — это кадр), после чего можно определить сервер для обработки (Infer) после получения x кадров. 

#### <a name="grpc-protocol"></a>Протокол GRPC 

При использовании протокола gRPC: 

С расширением gRPC каждый сеанс предназначен для одного канала камеры, поэтому нет необходимости указывать идентификатор. Теперь с помощью свойства Екстенсионконфигуратион можно сохранить видеокадры в буфере и определить сервер для обработки (вывод) после получения x кадров. 

### <a name="do-all-processmediastreams-on-a-particular-container-run-the-same-ai-model"></a>Все Процессмедиастреамс в определенном контейнере выполняют одну и ту же модель искусственного интеллекта? 

Нет.  

Вызовы запуска и отмены от конечного пользователя на экземпляре Graph составляют сеанс, или, возможно, происходит отключение или переподключение камеры. Цель состоит в том, чтобы сохранить один сеанс, если камера является потоковым видео. 

* Две камеры, отправляющие видео для обработки, создают два сеанса. 
* Одна камера, которая переходит на диаграмму с двумя узлами Грпцекстенсион, создает два сеанса. 

Каждый сеанс является полным дуплексным подключением между интерактивной видеоаналитикой и сервером gRPC, и каждый сеанс может иметь другую модель или конвейер. 

> [!NOTE]
> В случае отключения или повторного подключения камеры (если камера переходит в автономный режим за период, превышающий допустимые пределы), интерактивная аналитика видео откроет новый сеанс с сервером gRPC. Серверу не требуется отслеживание состояния в этих сеансах. 

В реальном времени Video Analytics также добавлена поддержка нескольких расширений gRPC для одной камеры в экземпляре Graph. Вы сможете использовать эти расширения gRPC для последовательного или параллельного выполнения обработки искусственного интеллекта или даже иметь их сочетание. 

> [!NOTE]
> Параллельное выполнение нескольких расширений повлияет на ресурсы оборудования, и вам придется учитывать это при выборе оборудования, которое будет соответствовать вашим вычислительным потребностям. 

### <a name="what-is-the-max--of-simultaneous-processmediastreams"></a>Каково максимальное число одновременных Процессмедиастреамс? 

Нет ограничений, применяемых в реальном видео-аналитике.  

### <a name="how-should-i-decide-if-my-inferencing-server-should-use-cpu-or-gpu-or-any-other-hardware-accelerator"></a>Как решить, следует ли использовать ЦП или GPU или любой другой ускоритель оборудования? 

Это полностью зависит от того, насколько сложна разработка модели AI и как разработчик хочет использовать ускорители ЦП и оборудования. При разработке модели искусственного интеллекта разработчики могут указать, какие ресурсы должны использоваться моделью для выполнения действий. 

### <a name="how-do-i-store-images-with-bounding-boxes-post-processing"></a>Разделы справки сохранять изображения с ограничивающими прямоугольниками после обработки? 

Сейчас мы предоставляем координаты ограничивающего прямоугольника только в сообщениях вывода. Разработчики могут создать настраиваемый поток МЖПЕГ, который сможет использовать эти сообщения и наложение ограничивающих прямоугольников на видеокадры.  

## <a name="grpc-compatibility"></a>совместимость gRPC 

### <a name="how-will-i-know-what-the-mandatory-fields-for-the-media-stream-descriptor-are"></a>Как определить обязательные поля для дескриптора потока мультимедиа? 

Любому значению поля, которое не предоставляется, будет присвоено значение по умолчанию, [заданное параметром gRPC](https://developers.google.com/protocol-buffers/docs/proto3#default).  

В реальном видео Analytics используется **proto3** версия языка буфера протокола. Все данные буфера протокола, используемые контрактами в реальном времени, доступны в файлах буферных протоколов, [определенных здесь](https://github.com/Azure/live-video-analytics/tree/master/contracts/grpc). 

### <a name="how-should-i-ensure-that-i-am-using-the-latest-protocol-buffer-files"></a>Как убедиться в том, что я использую последние файлы буфера протокола? 

Последние файлы буфера протокола можно [получить здесь](https://github.com/Azure/live-video-analytics/tree/master/contracts/grpc). При обновлении файлов контракта они будут отображаться в этом расположении. Хотя немедленный план обновления файлов протоколов отсутствует, найдите имя пакета в верхней части файлов, чтобы узнать версию. Должно быть: 

```
microsoft.azure.media.live_video_analytics.extensibility.grpc.v1 
```

Все обновления этих файлов увеличивают значение "v-value" в конце имени. 

> [!NOTE]
> Так как в реальном видео Analytics используется proto3 версия языка, поля являются необязательными, и это делает их обратной и прямой совместимость. 

### <a name="what-grpc-features-are-available-for-me-to-use-with-live-video-analytics-which-features-are-mandatory-and-which-ones-are-optional"></a>Какие функции gRPC доступны для использования в службе Live Video Analytics? Какие функции являются обязательными и какие из них являются необязательными? 

Можно использовать любые функции gRPC на стороне сервера, если выполнен контракт protobuf. 

## <a name="monitoring-and-metrics"></a>Мониторинг и метрики

### <a name="can-i-monitor-the-media-graph-on-the-edge-using-event-grid"></a>Можно ли отслеживать граф мультимедиа на пограничном устройстве с помощью сетки событий?

Да. Вы можете использовать метрики Prometheus и публиковать их в сетке событий. 

### <a name="can-i-use-azure-monitor-to-view-the-health-metrics-and-performance-of-my-media-graphs-in-the-cloud-or-on-the-edge"></a>Можно ли использовать Azure Monitor для просмотра работоспособности, метрик и производительности графов мультимедиа в облаке или на границе?

Да. Поддерживается. Дополнительные сведения [об использовании метрик Azure Monitor](https://docs.microsoft.com/azure/azure-monitor/platform/data-platform-metrics).

### <a name="are-there-any-tools-to-make-it-easier-to-monitor-the-media-services-iot-edge-module"></a>Существуют ли инструменты, облегчающие мониторинг модуля IoT Edge служб мультимедиа?

Visual Studio Code поддерживает расширение "средства Azure IoT", которое позволяет легко отслеживать конечные точки модуля Лваедже. С помощью этого средства можно быстро начать наблюдение за встроенной конечной точкой центра Интернета вещей для событий и просмотреть сообщения о выводе, направляемые с пограничным устройством в облако. 

Кроме того, это расширение можно использовать для редактирования модуля двойника для модуля Лваедже, чтобы изменить параметры графа мультимедиа.

Дополнительные сведения см. в статье [мониторинг и ведение журнала](monitoring-logging.md) .

## <a name="billing-and-availability"></a>Выставление счетов и доступность

### <a name="how-is-live-video-analytics-on-iot-edge-billed"></a>Как выставляются счета за использование службы "Анализ видео в реальном времени" IoT Edge?

Дополнительные сведения см. на [странице с ценами](https://azure.microsoft.com/pricing/details/media-services/) .

## <a name="next-steps"></a>Дальнейшие действия

[Краткое руководство. Начало работы. Общие сведения об Аналитике видеотрансляции в IoT Edge](get-started-detect-motion-emit-events-quickstart.md)
