---
title: Синхронизация данных из Базы данных SQL Azure для пограничных вычислений с помощью Фабрики данных Azure | Документация Майкрософт
description: Сведения о синхронизации данных между Базой данных SQL Azure для пограничных вычислений и хранилищем BLOB-объектов Azure
keywords: база данных sql для пограничных вычислений,синхронизация данных из базы данных sql для пограничных вычислений, фабрика данных для базы данных sql для пограничных вычислений
services: sql-database-edge
ms.service: sql-database-edge
ms.topic: tutorial
author: SQLSourabh
ms.author: sourabha
ms.reviewer: sstein
ms.date: 11/04/2019
ms.openlocfilehash: 0e75da9516303bb4250b6847a4b381d07b3d7dad
ms.sourcegitcommit: c22327552d62f88aeaa321189f9b9a631525027c
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/04/2019
ms.locfileid: "73509208"
---
# <a name="tutorial-sync-data-from-sql-database-edge-to-azure-blob-storage-using-azure-data-factory"></a>Руководство по Синхронизация данных из Базы данных SQL для пограничных вычислений с хранилищем BLOB-объектов Azure с помощью Фабрики данных Azure

В этом руководстве вы используете Фабрику данных Azure для добавочной синхронизации данных из таблицы в экземпляре Базы данных SQL Azure для пограничных вычислений с хранилищем BLOB-объектов Azure.

## <a name="before-you-begin"></a>Перед началом работы

Если вы еще не создали базу данных или таблицу в Базе данных SQL Azure для пограничных вычислений, создайте ее одним из указанных ниже способов.

* С помощью [SQL Server Management Studio](/sql/ssms/download-sql-server-management-studio-ssms/) или [Azure Data Studio](/sql/azure-data-studio/download/) подключитесь к Базе данных SQL для пограничных вычислений и выполните скрипт SQL, чтобы создать базу данных и таблицу.
* Создайте базу данных SQL и таблицу с помощью [SQLCMD](/sql/tools/sqlcmd-utility/), подключившись к модулю Базы данных SQL для пограничных вычислений напрямую. Дополнительные сведения см. в статье о [подключении к ядру СУБД с помощью sqlcmd](/sql/ssms/scripting/sqlcmd-connect-to-the-database-engine/).
* С помощью программы SQLPackage.exe разверните файл DACPAC в контейнере Базы данных SQL для пограничных вычислений. Этот процесс можно автоматизировать, указав универсальный код ресурса (URI) файла SQLPackage в конфигурации требуемых свойств модулей или используя клиентскую программу SqlPackage.exe напрямую для развертывания файла DACPAC в Базе данных SQL для пограничных вычислений.

    Сведения о скачивании программы sqlpackage см. в статье [Скачивание и установка sqlpackage](/sql/tools/sqlpackage-download/). Ниже приведены примеры команд для SqlPackage.exe. Дополнительные сведения см. в документации по sqlpackage.

    **Создание файла DACPAC**:

    ```cmd
    sqlpackage /Action:Extract /SourceConnectionString:"Data Source=<Server_Name>,<port>;Initial Catalog=<DB_name>;User ID=<user>;Password=<password>" /TargetFile:<dacpac_file_name> 
    ```

    **Применение файла DACPAC**:

    ```cmd
    sqlpackage /Action:Publish /Sourcefile:<dacpac_file_name> /TargetServerName:<Server_Name>,<port> /TargetDatabaseName:<DB_Name> /TargetUser:<user> /TargetPassword:<password>
    ```

## <a name="create-a-sql-table-and-procedure-to-store-and-update-the-watermark-levels"></a>Создание таблицы SQL и процедуры для хранения и обновления служебных знаков

Таблица служебных знаков используется для хранения последней метки времени, данные до которой уже синхронизированы с хранилищем Azure. Таблица служебных знаков обновляется после каждой синхронизации с помощью хранимой процедуры Transact-SQL (T-SQL). 

Выполните следующие команды в экземпляре Базы данных SQL для пограничных вычислений.

```sql
    Create table [dbo].[watermarktable]
    (
    TableName varchar(255),
    WatermarkValue datetime,
    )
    GO

    CREATE PROCEDURE usp_write_watermark @timestamp datetime, @TableName varchar(50)  
    AS  
    BEGIN
    UPDATE [dbo].[watermarktable]
    SET [WatermarkValue] = @timestamp WHERE [TableName] = @TableName
    END
    Go
```

## <a name="create-a-data-factory-workflow"></a>Создание рабочего процесса Фабрики данных

В этом разделе вы создадите конвейер Фабрики данных Azure для синхронизации данных из таблицы в Базе данных SQL Azure для пограничных вычислений с хранилищем BLOB-объектов Azure.

### <a name="create-data-factory-using-the-data-factory-ui"></a>Создание Фабрики данных с помощью пользовательского интерфейса службы "Фабрика данных"

Создайте Фабрику данных, следуя инструкциям в [этом руководстве](../data-factory/quickstart-create-data-factory-portal.md#create-a-data-factory).

### <a name="create-a-data-factory-pipeline"></a>Создание конвейера Фабрики данных

1. На странице **Начало работы** в пользовательском интерфейсе Фабрики данных выберите плитку **Создать конвейер**.

    ![Фабрика данных — создание конвейера](media/tutorial-sync-data-factory/data-factory-get-started.png)

2. На странице **Общие** в окне **Свойства** для конвейера введите имя **PeriodicSync**.

3. Добавьте первое действие **поиска**, которое получает старое значение служебного знака. На панели элементов **Действия** разверните раздел **Общие** и перетащите действие **Поиск** в область конструктора конвейеров. Измените имя действия на *OldWatermark*.

    ![поиск предыдущего служебного знака](media/tutorial-sync-data-factory/create-old-watermark-lookup.png)

4. Перейдите на вкладку **Параметры** и щелкните **+ Создать** в области **Исходный набор данных**. На этом шаге вы создадите набор данных для представления данных в таблице watermarktable. В этой таблице содержится старый нижний предел, который использовался в предыдущей операции копирования.

5. В окне **Новый набор данных** выберите **Azure SQL Server** и нажмите кнопку **Продолжить**.  

6. В окне **Установка свойств** для набора данных введите *WatermarkDataset* в поле имени.

7. В поле **Связанная служба**, выберите**Создать**, а затем выполните указанные ниже действия.

    1. В поле **Имя** введите *SQLDBEdgeLinkedService*.

    2. В поле **Имя сервера** введите сведения о сервере Базы данных SQL для пограничных вычислений.

    3. Выберите **имя базы данных** в раскрывающемся списке.

    4. Введите **имя пользователя** и **пароль**.

    5. Чтобы проверить подключение к экземпляру Базы данных SQL для пограничных вычислений, нажмите кнопку **Проверить подключение**.

    6. Нажмите кнопку **Создать**.

    ![создание связанной службы](media/tutorial-sync-data-factory/create-linked-service.png)

    7. Нажмите кнопку **ОК**.

8. На вкладке **Параметры** щелкните **Изменить**.

9. На вкладке **Подключение** выберите *[dbo].[watermarktable]* для **Таблицы**. Если вы хотите просмотреть данные в таблице, нажмите кнопку **Просмотр данных**.

10. Перейдите в редактор конвейеров, выбрав вкладку конвейера вверху или имя конвейера в представлении в виде дерева слева. В окне свойств для действия **Поиск** убедитесь в том, что в поле **Исходный набор данных** выбран вариант **WatermarkDataset**.

11. На панели элементов **Действия** разверните раздел **Общие** и перетащите еще одно действие **Поиск** в область конструктора конвейеров, а затем назовите его **NewWatermark** на вкладке **Общие** в окне свойств. Это действие поиска получает новое значение нижнего предела из таблицы, где хранятся исходные данные для копирования в приемник.

12. В окне свойств второго действия **Поиск** перейдите на вкладку **Параметры** и нажмите кнопку **Создать**, чтобы создать набор данных, который будет указывать на исходную таблицу с новым значением служебного знака.

13. В окне **Новый набор данных** выберите экземпляр Базы данных SQL для пограничных вычислений и нажмите кнопку **Продолжить**.

    1. В окне **Настройки свойств** введите **SourceDataset** для **Имя**. В качестве связанной службы выберите *SQLDBEdgeLinkedService*.

    2. Выберите ***таблицу, которую нужно синхронизировать***. Можно также указать запрос для этого набора данных, как будет описано далее в этом руководстве. Этот запрос будет более приоритетным, чем указанная на этом шаге таблица.

    3. Нажмите кнопку **ОК**.

14. Перейдите в редактор конвейеров, выбрав вкладку конвейера вверху или имя конвейера в представлении в виде дерева слева. В окне свойств для действия **Поиск**, убедитесь, что в поле **Исходный набор данных** выбран вариант **SourceDataset**.

15. В поле **Использовать запрос** выберите **Запрос** и введите приведенный ниже запрос, изменив в нем имя таблицы. Он выбирает максимальное значение метки времени из таблицы. Убедитесь, что установлен флажок **First row only** (Только первая строка).

    ```sql
    select MAX(timestamp) as NewWatermarkvalue from [TableName]
    ```

    ![Выбор запроса](media/tutorial-sync-data-factory/select-query-data-factory.png)

16. На панели элементов **Действия** разверните раздел **Перемещение и преобразование**, перетащите действие **Копирование** с панели, а затем присвойте ему имя **IncrementalCopy**.

17. Соедините оба действия **поиска** с действием **копирования**, перетащив **зеленую кнопку** от действий **поиска** к действию **копирования**. Когда цвет границы для действия копирования изменится на синий, отпустите кнопку мыши.

18. Выберите действие **Копирование** и проверьте его свойства в окне **Свойства**.

19. Откройте вкладку **Источник** в окне **Свойства** и выполните здесь следующие действия.

    1. Выберите **SourceDataset** в поле **Source Dataset** (Исходный набор данных).

    2. Выберите **Запрос** в поле **Использовать запрос**.

    3. Введите SQL-запрос в поле **Запрос**. Пример запроса ниже

    4. SQL-запрос:

    ```sql
    select * from TemperatureSensor where timestamp > '@{activity('OldWaterMark').output.firstRow.WatermarkValue}' and timestamp <= '@{activity('NewWaterMark').output.firstRow.NewWatermarkvalue}'
    ```

20. Перейдите на вкладку **Приемник** и нажмите кнопку **+ Создать** в поле **Целевой набор данных**.

21. В этом руководстве в качестве целевого хранилища данных используется **хранилище BLOB-объектов Azure**. Выберите **Хранилище BLOB-объектов Azure** и нажмите кнопку **Продолжить** в окне **Новый набор данных**.

22. В окне **Выбор формата** выберите тип формата данных, а затем нажмите кнопку **Продолжить**.

23. В окне **Установка свойств** введите имя **SinkDataset**. В разделе "Связанная служба" выберите **+ Создать**. На этом шаге вы создадите подключение (связанную службу) для **хранилища BLOB-объектов Azure**.

24. В окне **Новая связанная служба (хранилище BLOB-объектов Azure)** выполните указанные ниже действия.

    1. Введите имя *AzureStorageLinkedService*.

    2. В списке **Имя учетной записи хранения** выберите учетную запись хранения Azure с вашей подпиской Azure.

    3. Проверьте **подключение** и нажмите кнопку **Готово**.

25. Убедитесь, что окно **Установка свойств** *AzureStorageLinkedService* выбрано в списке **Связанная служба**. Затем щелкните **Создать** и нажмите кнопку **ОК**.

26. На вкладке **Приемник** выберите **Изменить**.

27. Перейдите на вкладку **Подключение** в *SinkDataset* и выполните указанные ниже действия.

    1. В поле **Путь к файлу** введите *asdedatasync/incrementalcopy*. Здесь **adftutorial** обозначает имя контейнера больших двоичных объектов, а **incrementalcopy** — имя папки в нем. Создайте контейнер (если его еще нет) или присвойте ему имя имеющегося контейнера. Фабрика данных Azure автоматически создает целевую папку *incrementalcopy*, если она еще не существует. Можно также нажать кнопку **Обзор** рядом с полем **Путь к файлу**, чтобы перейти к нужной папке в контейнере больших двоичных объектов.

    2. Для части **Файл** в поле **Путь к файлу** выберите **Добавить динамическое содержимое [Alt+P]** , а затем *введите @CONCAT('Incremental-', pipeline().RunId, '.txt')* в открывшемся окне. Затем нажмите кнопку **Готово**. Это выражение динамически создает имя файла. Каждый запуск конвейера имеет уникальный идентификатор. Действие копирования использует этот идентификатор запуска при создании имени файла.

28. Перейдите в редактор **конвейеров**, выбрав вкладку конвейера вверху или имя конвейера в представлении в виде дерева слева.

29. На панели элементов **Действия** разверните элемент **Общие**, а затем перетащите действие **Хранимая процедура** с панели элементов **Действия** в область конструктора конвейера. **Соедините** результаты действия **Копирование**, обозначенные зеленым цветом, с действием **Хранимая процедура**.

30. Выберите **действие хранимой процедуры** в конструкторе конвейеров и измените его имя на *SPtoUpdateWatermarkActivity*.

31. Перейдите на вкладку **Учетная запись SQL** и выберите *SQLDBEdgeLinkedService* в списке **Связанная служба**.

32. Перейдите на вкладку **Хранимая процедура** и выполните здесь следующие действия:

    1. В качестве **имени хранимой процедуры** выберите *[dbo].[usp_write_watermark]* .

    2. Чтобы указать значения для параметров хранимой процедуры, выберите "Импорт параметров" и введите следующие значения.

    |ИМЯ|type|Значение|
    |-----|----|-----|
    |LastModifiedtime|Дата и время|@{activity('NewWaterMark').output.firstRow.NewWatermarkvalue}|
    |TableName|Строка,|@{activity('OldWaterMark').output.firstRow.TableName}|

33. Чтобы проверить настройки конвейера, нажмите кнопку **Проверить** на панели инструментов. Убедитесь, что проверка завершается без ошибок. Чтобы закрыть окно **отчета о проверке конвейера**, нажмите кнопку **>>** .

34. Опубликуйте сущности (связанные службы, наборы данных и конвейеры) в службе фабрики данных Azure, щелкнув **Опубликовать все**. Дождитесь сообщения об успешном завершении публикации.

## <a name="trigger-a-pipeline-on-schedule"></a>Активация конвейера по расписанию

1. На панели инструментов конвейера выберите **Добавить триггер**, а затем выберите **Создать или изменить** и щелкните **+ Создать**.

2. Присвойте триггеру имя *HourlySync*, выберите **тип** "Расписание" и задайте для параметра **Повторение** значение "Каждый час".

3. Нажмите кнопку **ОК**.

4. Выберите **Опубликовать все**.

5. Выберите **Запустить сейчас**.

6. Перейдите на вкладку **Мониторинг** слева. Здесь вы увидите, что конвейер запущен вручную. Нажмите кнопку **Обновить**, чтобы обновить список.

## <a name="next-steps"></a>Дополнительная информация

Конвейер Фабрики данных Azure в этом руководстве копирует данные из таблицы в экземпляре Базы данных SQL для пограничных вычислений в расположение в хранилище BLOB-объектов Azure с почасовой частотой. Перейдите к [этим руководствам](../data-factory/tutorial-copy-data-portal.md), чтобы узнать об использовании Фабрики данных в других сценариях.
