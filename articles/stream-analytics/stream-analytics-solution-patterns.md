---
title: Шаблоны решений Azure Stream Analytics
description: Узнайте об общих шаблонах решений для Azure Stream Analytics, таких как dashboarding, обмен сообщениями о событиях, хранилища данных, обогащение справочных данных и мониторинг.
author: mamccrea
ms.author: mamccrea
ms.reviewer: mamccrea
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 06/21/2019
ms.openlocfilehash: 3b95863c1ae53bd0642aec356f55aba1faf8ef09
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "79535788"
---
# <a name="azure-stream-analytics-solution-patterns"></a>Шаблоны решений Azure Stream Analytics

Как и многие другие службы Azure, Stream Analytics лучше всего использовать с другими службами для создания более масштабного сквозного решения. В этой статье рассматриваются простые решения Azure Stream Analytics и различные архитектурные шаблоны. Вы можете опираться на эти шаблоны для разработки более сложных решений. Шаблоны, описанные в этой статье, могут быть использованы в самых разных сценариях. Примеры шаблонов, специфичным для сценариев, доступны в [архитектурах решений Azure.](https://azure.microsoft.com/solutions/architecture/?product=stream-analytics)

## <a name="create-a-stream-analytics-job-to-power-real-time-dashboarding-experience"></a>Создайте работу Stream Analytics для создания в режиме реального времени опыта dashboarding

С помощью Azure Stream Analytics вы можете быстро вставать панели мониторинга и оповещения в режиме реального времени. Простое решение впитывает события из концентраторов событий или Концентратора IoT и [питает панель мониторинга Power BI с помощью набора потоковых данных.](/power-bi/service-real-time-streaming) Для получения дополнительной информации, смотрите подробный учебник [Анализ данных телефонных звонков с Stream Analytics и визуализировать результаты в панели мониторинга Power BI](stream-analytics-manage-job.md).

![Dashboard ASA Power BI](media/stream-analytics-solution-patterns/pbidashboard.png)

Это решение может быть построено всего за несколько минут от портала Azure. Существует не обширное кодирование участие, и язык S'L используется для выражения бизнес-логики.

Этот шаблон решения предлагает самую низкую задержку от источника событий до панели мониторинга Power BI в браузере. Azure Stream Analytics является единственной службой Azure с этой встроенной возможностью.

## <a name="use-sql-for-dashboard"></a>Используйте S'L для панели мониторинга

Приборная панель Power BI предлагает низкую задержку, но она не может быть использована для получения полноценных отчетов Power BI. Общим шаблоном отчетности является вывод данных в первую очередь в базу данных S'L. Затем используйте разъем Для запроса данных о S'L для запроса на получение более последних данных.

![Панель мониторинга АСА СЗЛ](media/stream-analytics-solution-patterns/sqldashboard.png)

Использование базы данных S'L дает вам большую гибкость, но за счет несколько более высокой задержки. Это решение оптимально для заданий с требованиями к задержкам более одной секунды. С помощью этого метода можно максимизировать возможности Power BI для дальнейшего нарезки и кости данных для отчетов, и гораздо больше вариантов визуализации. Вы также получаете гибкость в использовании других решений панели мониторинга, таких как Tableau.

S'L не является хранилищем высокопроизводительных данных. Максимальная пропускная связь с базой данных СЗЛ от Azure Stream Analytics в настоящее время составляет около 24 Мб/с. Если источники событий в вашем решении производят данные с более высокой скоростью, необходимо использовать логику обработки в Stream Analytics, чтобы снизить скорость вывода до S'L. Могут использоваться такие методы, как фильтрация, оконные агрегаты, шаблон, соответствующий временным соединениям, и аналитические функции. Скорость вывода в S'L может быть дополнительно оптимизирована с использованием методов, описанных в [выходе Azure Stream Analytics в базу данных Azure S'L.](stream-analytics-sql-output-perf.md)

## <a name="incorporate-real-time-insights-into-your-application-with-event-messaging"></a>Включите информацию в режиме реального времени в ваше приложение с помощью обмена сообщениями о событиях

Вторым по популярности использованием Stream Analytics является генерация оповещений в режиме реального времени. В этом шаблоне решений бизнес-логика в Stream Analytics может использоваться для обнаружения [временных и пространственных закономерностей](stream-analytics-geospatial-functions.md) или [аномалий,](stream-analytics-machine-learning-anomaly-detection.md)а затем производить сигналы оповещения. Однако, в отличие от решения панели мониторинга, в котором Stream Analytics использует Power BI в качестве предпочтительной конечной точки, можно использовать ряд промежуточных приемников данных. Эти поглотители включают концентраторы событий, сервисный автобус и функции Azure. Вам, как строителю приложения, необходимо решить, какая раковина данных лучше всего подходит для вашего сценария.

Логика потребителя событий ниже по течению должна быть реализована для создания оповещений в существующем бизнес-процессе. Поскольку пользовательская логика может быть реализована в функциях Azure, функции Azure — это самый быстрый способ выполнения этой интеграции. Учебник по использованию функции Azure в качестве вывода для задания Stream Analytics можно найти в [функциях Run Azure от заданий Azure Stream Analytics.](stream-analytics-with-azure-functions.md) Функции Azure также поддерживают различные типы уведомлений, включая текст и электронную почту. Logic App также может быть использован для такой интеграции, с событиями концентраторов между Stream Analytics и логики App.

![Приложение для обмена сообщениями событий ASA](media/stream-analytics-solution-patterns/eventmessagingapp.png)

С другой стороны, Event Hubs предлагает наиболее гибкую точку интеграции. Многие другие службы, такие как Azure Data Explorer и Time Series Insights, могут использовать события из концентраторов событий. Службы могут быть подключены непосредственно к потоку событий из Azure Stream Analytics для завершения решения. Концентраторы событий также является самым высокодоступным брокером обмена сообщениями, доступным на Azure для таких сценариев интеграции.

## <a name="dynamic-applications-and-websites"></a>Динамические приложения и веб-сайты

Можно создавать пользовательские визуализации в реальном времени, такие как панель мониторинга или визуализация карты, используя аналитику Azure Stream Analytics и службу СигналR Azure. Используя SignalR, веб-клиенты могут обновляться и показывать динамический контент в режиме реального времени.

![AsA динамическое приложение](media/stream-analytics-solution-patterns/dynamicapp.png)

## <a name="incorporate-real-time-insights-into-your-application-through-data-stores"></a>Включите информацию в режиме реального времени в ваше приложение через хранилища данных

Большинство веб-служб и веб-приложений сегодня используют шаблон запроса/ответа для обслуживания уровня презентации. Шаблон запроса/ответа прост в построении и может быть легко масштабируется с низким временем отклика с помощью апатизмов и масштабируемых магазинов, таких как Cosmos DB.

Высокий объем данных часто создает узкие места в системе, основанной на КРУД. [Шаблон решения поиска событий](/azure/architecture/patterns/event-sourcing) используется для устранения узких мест производительности. Временные шаблоны и идеи также трудно и неэффективно извлечь из традиционного хранилища данных. Современные приложения, управляемые данными с высоким объемом данных, часто внимают архитектуру, основанную на потоках данных. Аналитика Azure Stream Как вычислительный движок для данных в движении является стержнем в этой архитектуре.

![Приложение для поиска событий ASA](media/stream-analytics-solution-patterns/eventsourcingapp.png)

В этом шаблоне решений события обрабатываются и агрегируются в хранилища данных Azure Stream Analytics. Слой приложения взаимодействует с хранилищами данных, используя традиционный шаблон запроса/ответа. Из-за способности Stream Analytics обрабатывать большое количество событий в режиме реального времени приложение является высоко масштабируемым без необходимости накладывая слой хранилища данных. Слой хранилища данных по существу представляет собой материализованное представление в системе. [Выход Azure Stream Analytics в DB Azure Cosmos](stream-analytics-documentdb-output.md) описывает, как Cosmos DB используется в качестве вывода Stream Analytics.

В реальных приложениях, где логика обработки сложна и существует необходимость обновления определенных частей логики независимо, несколько заданий Stream Analytics могут быть составлены вместе с Event Hubs в качестве посредника по средних событий.

![Приложение asA комплексное поиск событий](media/stream-analytics-solution-patterns/eventsourcingapp2.png)

Эта модель повышает устойчивость и управляемость системы. Однако, несмотря на то, что Stream Analytics гарантирует точно после обработки, существует небольшая вероятность того, что дублирующие события могут приземлиться в промежуточных концентрах событий. Важно, чтобы работа по снижению потока Analytics dedupe событий с помощью логических ключей в окне поиска. Для получения дополнительной информации [Event Delivery Guarantees](/stream-analytics-query/event-delivery-guarantees-azure-stream-analytics) о доставке событий см.

## <a name="use-reference-data-for-application-customization"></a>Используйте справочные данные для настройки приложений

Функция справочных данных Azure Stream Analytics разработана специально для настройки конечных пользователей, таких как порог оповещения, правила обработки и [геоограждения.](geospatial-scenarios.md) Слой приложения может принимать изменения параметров и хранить их в базе данных S'L. Задание Stream Analytics периодически запрашивает изменения из базы данных и делает параметры настройки доступными через соединение справочных данных. Для получения дополнительной информации о том, как использовать справочные данные для настройки приложения, [см.](sql-reference-data.md) [reference data join](/stream-analytics-query/reference-data-join-azure-stream-analytics)

Этот шаблон также может быть использован для реализации движка правил, где пороги правил определяются на справочных данных. Более подробную информацию о правилах можно узнать в [аналитике Azure Stream Analytics.](stream-analytics-threshold-based-rules.md)

![Приложение справочных данных ASA](media/stream-analytics-solution-patterns/refdataapp.png)

## <a name="add-machine-learning-to-your-real-time-insights"></a>Добавление машинного обучения в свои идеи в реальном времени

Встроенная модель обнаружения [аномалий](stream-analytics-machine-learning-anomaly-detection.md) Azure Stream Analytics — это удобный способ внедрения машинного обучения в приложение в режиме реального времени. Для более широкого спектра потребностей в машинном обучении смотрите, [как Azure Stream Analytics интегрируется с сервисом скоринга Azure Machine Learning.](stream-analytics-machine-learning-integration-tutorial.md)

Для продвинутых пользователей, которые хотят включить онлайн-обучение и скоринга в том же поток Analytics трубопровода, смотрите этот пример того, как сделать это с [линейной регрессии](stream-analytics-high-frequency-trading.md).

![Приложение для машинного обучения ASA](media/stream-analytics-solution-patterns/mlapp.png)

## <a name="near-real-time-data-warehousing"></a>Почти в режиме реального времени складирование данных

Другой распространенной моделью является хранение данных в режиме реального времени, также называемое хранилищем потоковых данных. Помимо событий, поступающих в Центры событий и IoT Hub из вашего приложения, [Аналитика Потокового потока Azure, работающая на IoT Edge,](stream-analytics-edge.md) может использоваться для выполнения потребностей в обработке данных, сокращении данных, а также хранения данных и передовых данных. Stream Analytics, работающий на IoT Edge, может изящно обрабатывать проблемы ограничения пропускной способности и подключения в системе. Адаптер вывода S'L может быть использован для вывода на склад данных S'L; однако максимальная пропускная выливка ограничена 10 МБ/с.

![Складирование данных ASA](media/stream-analytics-solution-patterns/datawarehousing.png)

Одним из способов улучшения пропускной памяти с некоторым компромиссом задержки является архивирование событий в хранилище Azure Blob, а затем [импортировать их в хранилище данных СЗЛ с помощью Polybase.](../synapse-analytics/sql-data-warehouse/load-data-from-azure-blob-storage-using-polybase.md) Необходимо вручную сшить выход из Stream Analytics в хранилище каблов и ввод данных от хранилища каблов до хранилища данных s'L путем [архивирования данных по метке времени](stream-analytics-custom-path-patterns-blob-storage-output.md) и периодически импорта.

В этом шаблоне использования Azure Stream Analytics используется в качестве движка ETL в режиме реального времени. Вновь прибывающие события постоянно преобразуются и хранятся для потребления услуг аналитики ниже по течению.

![ASA высокой пропускной всей вмещаемой данных складирования](media/stream-analytics-solution-patterns/datawarehousing2.png)

## <a name="archiving-real-time-data-for-analytics"></a>Архивирование данных в режиме реального времени для аналитики

Большинство данных науки и аналитики деятельности по-прежнему происходят в автономном режиме. Данные могут быть заархивированы Azure Stream Analytics через форматы выхода Azure Data Lake Store Gen2 и Parquet. Эта возможность удаляет трение для передачи данных непосредственно в аналитику озер данных Azure, Azure Databricks и Azure HDInsight. В этом решении Azure Stream Analytics используется в качестве движка ETL в режиме реального времени. Вы можете изучить архивные данные в Data Lake с помощью различных вычислительных систем.

![Аналитика ASA в автономном режиме](media/stream-analytics-solution-patterns/offlineanalytics.png)

## <a name="use-reference-data-for-enrichment"></a>Используйте справочные данные для обогащения

Обогащение данных часто является обязательным требованием для двигателей ETL. Azure Stream Analytics поддерживает обогащение данных с [помощью справочных данных](stream-analytics-use-reference-data.md) как из базы данных S'L, так и из хранилища Azure Blob. Обогащение данных может быть осуществлено для посадки данных как в Azure Data Lake, так и в Хранилище данных S'L.

![ASA офлайн-аналитика с обогащением данных](media/stream-analytics-solution-patterns/offlineanalytics.png)

## <a name="operationalize-insights-from-archived-data"></a>Эксплуатация информации из архивных данных

Если вы объедините автономный шаблон аналитики с практически режимом приложения в реальном времени, можно создать цикл обратной связи. Цикл обратной связи позволяет приложению автоматически настраиваться на изменение шаблонов в данных. Этот цикл обратной связи может быть таким же простым, как изменение порогового значения для оповещения, или сложным, чем переподготовка моделей машинного обучения. Одиникову архитектору решений можно применить как для заданий ASA, работающих в облаке, так и для IoT Edge.

![Операционная информация ASA](media/stream-analytics-solution-patterns/insightsoperationalization.png)

## <a name="how-to-monitor-asa-jobs"></a>Как контролировать работу ASA

Задание Azure Stream Analytics можно запускать 24/7 для непрерывной обработки входящих событий в режиме реального времени. Его гарантия простоя имеет решающее значение для работоспособности всего приложения. В то время как Stream Analytics является единственным потоковым аналитики службы в отрасли, которая предлагает [99,9% гарантии доступности,](https://azure.microsoft.com/support/legal/sla/stream-analytics/v1_0/)вы все еще можете понести определенный уровень времени простоя. На протяжении многих лет Stream Analytics вводила метрики, журналы и состояния заданий, отражающие состояние заданий. Все они всплывают через службу Azure Monitor и могут быть экспортированы в OMS. Для получения дополнительной [информации см.](stream-analytics-monitoring.md)

![Мониторинг ASA](media/stream-analytics-solution-patterns/monitoring.png)

Есть две ключевые вещи для мониторинга:

- [Состояние выполнения работы](job-states.md)

    Прежде всего, вы должны убедиться, что работа работает. Без задания в запущенном состоянии не генерируются новые метрики или журналы. Вакансии могут меняться в недееспособное состояние по различным причинам, включая высокий уровень использования SU (т.е. иссякшие ресурсы).

- [Метрика задержки водяного знака](https://azure.microsoft.com/blog/new-metric-in-azure-stream-analytics-tracks-latency-of-your-streaming-pipeline/)

    Эта метрика отражает, насколько далеко позади конвейера обработки в настенные часы (секунды). Некоторые задержки объясняются присущей обработке логики. В результате мониторинг растущей тенденции гораздо важнее, чем мониторинг абсолютного значения. Устойчивая задержка состояния должна быть решена с помощью дизайна приложения, а не мониторинга или оповещения.

После сбоя журналы активности и [журналы диагностики](stream-analytics-job-diagnostic-logs.md) являются лучшими местами для поиска ошибок.

## <a name="build-resilient-and-mission-critical-applications"></a>Создание устойчивых и критически важных приложений

Независимо от гарантии sLA SLA SLA Компании Azure Stream Analytics и того, насколько тщательно вы работаете с помощью сквозного приложения, происходят сбои. Если ваша заявка имеет решающее значение для миссии, вы должны быть готовы к простоям, чтобы восстановить изящно.

Для оповещения приложений, самое главное, чтобы обнаружить следующее предупреждение. Вы можете перезапустить задание с текущего времени при восстановлении, игнорируя прошлые предупреждения. Семантика времени начала работы — это первый выход, а не первый ввод времени. Ввод переворачивается назад, соответствующее время, чтобы гарантировать, что первый выход в указанное время является полным и правильным. Вы не получите частичных агрегатов и вызвать оповещения неожиданно в результате.

Вы также можете начать выход из определенного периода времени в прошлом. Как концентраторы событий, так и политики хранения IoT Hub содержат разумный объем данных, позволяющий обрабатывать данные из прошлого. Компромисс заключается в том, как быстро вы можете догнать текущее время и начать генерировать своевременные новые оповещения. Данные быстро теряют свою ценность с течением времени, поэтому важно быстро догнать текущее время. Есть два способа быстро наверстать упущенное:

- Предоставление дополнительных ресурсов (SU) при догонянии.
- Перезагрузка с текущего времени.

Перезапуск с текущего время просто сделать, с компромиссом оставить пробел во время обработки. Перезапуск этого способа может быть ОК для оповещений сценариев, но может быть проблематичным для сценариев панели мониторинга и является нестартером для архивирования и складирования данных сценариев.

Выделение большего объема ресурсов может ускорить процесс, но эффект от резкого увеличения скорости обработки является сложным.

- Проверьте, что ваша работа масштабируется для большего числа SUs. Не все запросы можно масштабировать. Необходимо убедиться, что ваш запрос [проходит параллель.](stream-analytics-parallelization.md)

- Убедитесь, что в концентрах событий вверх по течению или концентраторе IoT достаточно ещр, чтобы добавить больше единиц пропускной воды (ТУ) для масштабирования входной пропускной выдавливания. Помните, что каждый event Hubs TU maxes на выходе скорость 2 МБ / с.

- Убедитесь, что у вас достаточно ресурсов в выходных раковинах (т.е. база данных S'L, Cosmos DB), чтобы они не задушили всплеск вывода, что иногда может привести к блокировке системы.

Самое главное , чтобы предвидеть изменение скорости обработки, проверить эти сценарии перед выходом в производство, и быть готовым к масштабирования обработки правильно во время восстановления отказа.

В крайнем случае, когда входящие события задерживаются, [возможно, все отложенные события отогнаны,](stream-analytics-time-handling.md) если вы применили окно позднего прибытия к вашей работе. Падение событий может показаться таинственным поведением в начале; однако, учитывая, что Stream Analytics — это движок обработки в реальном времени, он ожидает, что входящие события будут близки к времени настенных часов. Он должен отказаться от событий, которые нарушают эти ограничения.

### <a name="lambda-architectures-or-backfill-process"></a>Lambda Архитектура или процесс заполнения

К счастью, предыдущий шаблон архивирования данных может быть использован для обработки этих поздних событий изящно. Идея заключается в том, что архивирование задания обрабатывает входящие события во времени прибытия и архивирует события в нужное ведро времени в Azure Blob или Azure Data Lake Store с их временем событий. Не имеет значения, как поздно событие прибывает, оно никогда не будет удалено. Он всегда приземлится в нужное время ведро. Во время восстановления можно переработать архивные события и заполнить результаты в хранилище выбора. Это похоже на то, как реализуются шаблоны lambda.

![ASA засылка](media/stream-analytics-solution-patterns/backfill.png)

Процесс задней заполнения должен осуществляться с помощью автономной системы обработки пакетов, которая, скорее всего, имеет другую модель программирования, чем Azure Stream Analytics. Это означает, что необходимо повторно реализовать всю логику обработки.

Для засыпки, по-прежнему важно, по крайней мере временно предоставить больше ресурсов для выходных раковин для обработки более высокой пропускной мощности, чем устойчивые потребности в обработке состояния.

|Сценарии  |Перезапуск только с этого момента  |Перезагрузка с последнего остановленного времени |Перезагрузка с этого момента - заполните заднюю часть с архивными событиями|
|---------|---------|---------|---------|
|**Дашбординг**   |Создает зазор    |OK для короткого простоя    |Использовать для длительного простоя |
|**Оповещения**   |Хорошо |OK для короткого простоя    |Не обязательно |
|**Приложение для поиска событий** |Хорошо |OK для короткого простоя    |Использовать для длительного простоя |
|**Хранение данных**   |Потеря данных  |Хорошо |Не обязательно |
|**Оффлайн-аналитика**  |Потеря данных  |Хорошо |Не обязательно|

## <a name="putting-it-all-together"></a>Сборка

Нетрудно представить, что все шаблоны решений, упомянутые выше, могут быть объединены в сложную сквозную систему. Объединенная система может включать панели мониторинга, оповещения, приложения для поиска событий, хранение данных и возможности автономной аналитики.

Ключ заключается в разработке системы в композитных шаблонах, так что каждая подсистема может быть построена, протестирована, модернизирована и восстановлена независимо.

## <a name="next-steps"></a>Дальнейшие действия

Теперь вы видели различные шаблоны решений с помощью Azure Stream Analytics. Теперь вы можете вникнуть в детали и создать свое первое задание Stream Analytics:

* [Создайте задание Stream Analytics с помощью портала Azure.](stream-analytics-quick-create-portal.md)
* [Руководство по созданию задания Stream Analytics с помощью Azure PowerShell](stream-analytics-quick-create-powershell.md)
* [Краткое руководство. Создание задания Stream Analytics с использованием инструментов Azure Stream Analytics для Visual Studio](stream-analytics-quick-create-vs.md).
