---
title: Описание выходных данных из Azure Stream Analytics
description: В этой статье описываются параметры вывода данных для анализа результатов, доступные в Azure Stream Analytics, включая Power BI.
author: mamccrea
ms.author: mamccrea
ms.reviewer: mamccrea
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 05/8/2020
ms.openlocfilehash: d1eda3671b52a1e4bbae9af2d97010657880c383
ms.sourcegitcommit: bb0afd0df5563cc53f76a642fd8fc709e366568b
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/19/2020
ms.locfileid: "83585408"
---
# <a name="understand-outputs-from-azure-stream-analytics"></a>Описание выходных данных из Azure Stream Analytics

В этой статье описываются типы выходных данных, доступные для задания Azure Stream Analytics. Выходные данные дают возможность сохранить и хранить результаты задания Stream Analytics. Используя выходные данные, можно выполнять расширенную бизнес-аналитику и хранить свои данные.

При составлении запроса Stream Analytics укажите имя выходных данных с помощью [предложения INTO](https://docs.microsoft.com/stream-analytics-query/into-azure-stream-analytics). Можно использовать одни выходные данные на задание или несколько выходных данных для задания потоковой передачи, если требуется, указав в запросе несколько предложений INTO.

Для создания, изменения и проверки выходных данных задания Stream Analytics можно использовать [портал Azure](stream-analytics-quick-create-portal.md#configure-job-output), [Azure PowerShell](stream-analytics-quick-create-powershell.md#configure-output-to-the-job), [API .NET](https://docs.microsoft.com/dotnet/api/microsoft.azure.management.streamanalytics.ioutputsoperations?view=azure-dotnet), [REST API](https://docs.microsoft.com/rest/api/streamanalytics/stream-analytics-output) и [Visual Studio](stream-analytics-quick-create-vs.md).

Некоторые типы выходных данных поддерживают [секционирование](#partitioning). [Размеры выходных пакетов](#output-batch-size) изменяются для оптимизации пропускной способности.


## <a name="azure-data-lake-storage-gen-1"></a>Azure Data Lake Storage 1-го поколения

Stream Analytics поддерживает [Azure Data Lake Storage 1-го поколения](../data-lake-store/data-lake-store-overview.md). Azure Data Lake Storage — это гипермасштабируемый репозиторий корпоративного уровня для аналитических рабочих нагрузок больших данных. Azure Data Lake Storage позволяет сохранять данные с любым размером, типом и скоростью приема в одном месте для эксплуатационной и исследовательской аналитики. Stream Analytics необходимо разрешение на доступ к Azure Data Lake Storage.

Выходные данные Azure Data Lake Storage из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (21Vianet) и Azure для Германии (T-Systems International).

В таблице ниже перечислены имена свойств и их описания для настройки выходных данных Data Lake Storage 1-го поколения.   

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Понятное имя, которое используется в запросах для направления выходных данных запроса в Azure Data Lake Storage. |
| Подписка | Подписка, содержащая учетную запись Azure Data Lake Storage. |
| Имя учетной записи | Имя учетной записи Azure Data Lake Storage, в которую отправляются выходные данные. Появится раскрывающийся список учетных записей Azure Data Lake Storage, доступных в вашей подписке. |
| Шаблон префикса пути | Путь к файлу, используемый для записи файлов в указанной учетной записи Azure Data Lake Storage. Вы можете указать один или несколько экземпляров переменных {date} и {time}.<br /><ul><li>Пример 1. folder1/logs/{дата}/{время}</li><li>Пример 2. folder1/logs/{дата}</li></ul><br />Метка времени создаваемой структуры папок соответствует времени UTC, а не местному времени.<br /><br />Если шаблон пути к файлу не содержит символ "/", то последний шаблон в пути к файлу будет рассматриваться в качестве префикса имени файла. <br /><br />Новые файлы создаются в следующих ситуациях:<ul><li>изменения в схеме выходных данных;</li><li>внешний или внутренний перезапуск задания.</li></ul> |
| Формат даты | Необязательный параметр. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример ГГГГ/ММ/ДД |
|Формат времени | Необязательный параметр. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro.|
| Кодирование | Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8.|
| Разделитель | Применяется только для сериализации в формате CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.|
| Формат | Применяется только для сериализации в формате JSON. Вариант **строки-разделители** предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Если выбрать вариант **строки-разделители**, то JSON считывает по одному объекту за раз. Все содержимое само по себе не будет допустимым форматом JSON.  Вариант **массив** означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл.|
| Режим проверки подлинности | Доступ к учетной записи Data Lake Storage можно авторизовать с помощью [управляемого удостоверения](stream-analytics-managed-identities-adls.md) или маркера безопасности пользователя. После предоставления доступ можно отозвать, изменив пароль учетной записи пользователя, удалив выходные данные Data Lake Storage для этого задания или удалив задание Stream Analytics. |

## <a name="sql-database"></a>База данных SQL

[База данных SQL Azure](https://azure.microsoft.com/services/sql-database/) может служить местом назначения для выходных реляционных данных, а также для выходных данных приложений, которые зависят от содержимого, размещенного в реляционной базе данных. Задания Stream Analytics будут записывать данные в существующую таблицу в базе данных SQL. Схема таблицы должна в точности соответствовать полям и их типам в выходных данных задания. С помощью параметра вывода данных для базы данных SQL можно также указать [хранилище данных SQL Azure](https://azure.microsoft.com/documentation/services/sql-data-warehouse/) в качестве выходных данных. Сведения о способах улучшения пропускной способности операций записи см. в статье [Azure Stream Analytics output to Azure SQL Database](stream-analytics-sql-output-perf.md) (Запись выходных данных Azure Stream Analytics в службу "База данных SQL Azure").

Вы также можете использовать [Управляемый экземпляр Базы данных SQL Azure](https://docs.microsoft.com/azure/sql-database/sql-database-managed-instance). Необходимо [настроить общедоступную конечную точку в Управляемый экземпляр Базы данных SQL Azure](https://docs.microsoft.com/azure/sql-database/sql-database-managed-instance-public-endpoint-configure) а затем вручную настроить следующие параметры в Azure Stream Analytics. Виртуальная машина Azure, на которой работает SQL Server с подключенной базой данных, также дополнительно настраивается вручную с использованием параметров ниже.

В таблице ниже приведены имена и описание свойств для создания выходных данных Базы данных SQL.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую базу данных. |
| База данных | Имя базы данных, в которую отправляются выходные данные. |
| Имя сервера | Имя сервера Базы данных SQL. Для Управляемого экземпляра Базы данных SQL Azure необходимо указать порт 3342. Например, *sampleserver.public.database.windows.net,3342* |
| Имя пользователя | Имя пользователя, имеющего доступ к базе данных на запись. Stream Analytics поддерживает только проверку подлинности SQL. |
| Пароль | Пароль для подключения к базе данных. |
| Таблица | Имя таблицы, в которую записываются выходные данные. В имени таблицы учитывается регистр. Схема этой таблицы должна точно соответствовать количеству полей и их типов, формируемых выходными данными задания. |
|Наследование схемы секционирования| Это свойство позволяет наследовать схему разделов, используемую на предыдущем шаге запроса, для включения топологии полной параллельной обработки с несколькими модулями записи для таблицы. Дополнительные сведения см. в статье [Вывод данных Azure Stream Analytics в базу данных SQL Azure](stream-analytics-sql-output-perf.md).|
|Максимальное количество пакетов| Рекомендованное максимальное число записей, отправляемых с каждой транзакцией массовой вставки.|

Существует два адаптера, которые позволяют выводить данные из Azure Stream Analytics в Azure Synapse Analytics (ранее — хранилище данных SQL): База данных SQL и Azure Synapse. Рекомендуется выбрать адаптер Azure Synapse Analytics вместо адаптера Базы данных SQL, если выполняется одно из следующих условий.

* **Пропускная способность**. Если ожидаемая пропускная способность сейчас или в будущем больше 10 Мбит/с, используйте параметр вывода данных Azure Synapse для повышения производительности.

* **Входные разделы**: Если у вас есть восемь или более входных разделов, используйте параметр вывода данных Azure Synapse, чтобы улучшить горизонтальное увеличение масштаба.

## <a name="azure-synapse-analytics-preview"></a>Azure Synapse Analytics (предварительная версия)

[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics) (ранее — Хранилище данных SQL) — это служба аналитики без ограничений, которая объединяет корпоративные хранилища данных и аналитику больших данных. 

Задания Azure Stream Analytics могут выводиться в таблицу пула SQL в Azure Synapse Analytics и обрабатывать пропускную способность до 200 МБ/сек. Это удовлетворяет самые ресурсоемкие требования аналитики в режиме реального времени и обработку данных по горячим путям для таких рабочих нагрузок, как составление отчетов и панель мониторинга.  

Таблица пула SQL должна существовать, прежде чем ее можно будет добавить в качестве выходных данных в задание Stream Analytics. Схема таблицы должна соответствовать полям и их типам в выходных данных задания. 

Чтобы использовать Azure Synapse в качестве выходных данных, необходимо убедиться в том, что учетная запись хранения настроена. Перейдите к параметрам учетной записи хранения, чтобы настроить ее. Разрешены только типы учетных записей хранения, поддерживающие таблицы: Общего назначения версии 2 и общего назначения версии 1.   

В таблице ниже перечислены имена свойств и даны их описания для создания выходных данных Azure Synapse Analytics.

|Имя свойства|Описание|
|-|-|
|Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую базу данных. |
|База данных |Имя пула SQL, куда отправляются выходные данные. |
|Имя сервера |Имя сервера Azure Synapse.  |
|Имя пользователя |Имя пользователя, имеющего доступ к базе данных на запись. Stream Analytics поддерживает только проверку подлинности SQL. |
|Пароль |Пароль для подключения к базе данных. |
|Таблица  | Имя таблицы, в которую записываются выходные данные. В имени таблицы учитывается регистр. Схема этой таблицы должна точно соответствовать количеству полей и их типов, формируемых выходными данными задания.|

## <a name="blob-storage-and-azure-data-lake-gen2"></a>Хранилище BLOB-объектов и Azure Data Lake 2-го поколения

Data Lake Storage 2-го поколения использует службу хранилища Azure в качестве основы для создания корпоративных хранилищ данных в Azure. Разработанное для обслуживания нескольких петабайт информации, поддержки пропускной способности сети на уровне нескольких сотен гигабит, Data Lake Storage 2-го поколения позволяет легко управлять большими объемами данных. Основной частью Data Lake Storage 2-го поколения является добавление иерархического пространства имен в хранилище BLOB-объектов.

Хранилище BLOB-объектов Azure предоставляет экономичное и масштабируемое решение для хранения в облаке больших объемов неструктурированных данных. Общие сведения о хранилище BLOB-объектов и его использовании см. в разделе [Передача, скачивание и составление списка больших двоичных объектов с помощью портала Azure](../storage/blobs/storage-quickstart-blobs-portal.md).

В таблице ниже приведены имена и описание свойств для создания выходных данных большого двоичного объекта или ADLS 2-го поколения.

| Имя свойства       | Описание                                                                      |
| ------------------- | ---------------------------------------------------------------------------------|
| Псевдоним выходных данных        | Понятное имя, которое используется в запросах для направления выходных данных запроса в хранилище BLOB-объектов. |
| Учетная запись хранения     | Имя учетной записи хранения, в которую отправляются выходные данные.               |
| Ключ учетной записи хранения | Секретный ключ, связанный с учетной записью хранения.                              |
| Контейнер хранилища   | Логическая группировка больших двоичных объектов, хранящихся в службе BLOB-объектов Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер. |
| Шаблон пути | Необязательный параметр. Шаблон пути к файлу, используемый для записи BLOB-объектов в указанном контейнере. <br /><br /> Чтобы указать периодичность записи больших двоичных объектов, в шаблоне пути можно использовать один или несколько экземпляров переменных даты и времени: <br /> {date}, {time} <br /><br />Пользовательское секционирование большого двоичного объекта можно использовать для предварительной версии, указав одно пользовательское имя поля {field} из данных события. Имя поля может содержать буквы, цифры, дефисы и символы подчеркивания. Существуют следующие ограничения для пользовательских полей. <ul><li>Имена полей нечувствительны к регистру. Например, служба не может различить столбец "ID" и столбец "id".</li><li>Вложенные поля недопустимы. Вместо этого используйте псевдоним в запросе задания для "сведения" поля.</li><li>В имени поля запрещено использовать выражения.</li></ul> <br />Этот компонент допускается использовать в пути конфигурации описателей пользовательских форматов даты и времени. Пользовательские форматы даты и времени необходимо указывать по одному в виде ключевого слова {datetime:\<описатель>}. Допустимые входные данные для элемента \<описатель>: yyyy, MM, M, dd, d, HH, H, mm, m, ss или s (гггг, ММ, М, дд, д, ЧЧ, Ч, мм, м, сс или с). Ключевое слово {datetime:\<описатель>} можно использовать в составе пути несколько раз для создания пользовательских конфигураций даты и времени. <br /><br />Примеры: <ul><li>Пример 1: cluster1/logs/{date}/{time}</li><li>Пример 2: cluster1/logs/{date}</li><li>Пример 3: cluster1/{client_id}/{date}/{time}</li><li>Пример 4: cluster1/{datetime:ss}/{myField}, где запрос имеет следующий вид: SELECT data.myField AS myField FROM Input;</li><li>Пример 5: cluster1/year={datetime:yyyy}/month={datetime:MM}/day={datetime:dd}</ul><br />Метка времени создаваемой структуры папок соответствует времени UTC, а не местному времени.<br /><br />При именовании файлов используется следующее соглашение: <br /><br />{Шаблон префикса пути}/schemaHashcode_Guid_Number.extension<br /><br />Выходные файлы примера:<ul><li>Myoutput/20170901/00/45434_gguid_1.csv</li>  <li>Myoutput/20170901/01/45434_gguid_1.csv</li></ul> <br />Дополнительные сведения об этом компоненте см. в статье [Пользовательские шаблоны даты и времени в пути для выходных данных хранилища BLOB-объектов Azure Stream Analytics (предварительная версия)](stream-analytics-custom-path-patterns-blob-storage-output.md). |
| Формат даты | Необязательный параметр. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример ГГГГ/ММ/ДД |
| Формат времени | Необязательный параметр. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV, Avro и Parquet. |
|Минимальные строки (только Parquet)|Количество минимальных строк на пакет. Для Parquet каждый пакет создаст файл. Текущее значение по умолчанию — 2000 строк, а допустимое максимальное — 10 000 строк.|
|Максимальное время (только Parquet)|Максимальное время ожидания на пакет. По истечении этого времени пакет будет записан в выходные данные, даже если не будет выполнено требование к минимальному числу строк. Текущее значение по умолчанию — 1 минута, а максимально допустимое — 2 часа. Если у выходных данных большого двоичного объекта есть частота шаблона пути, время ожидания не может превышать диапазон времени раздела.|
| Кодирование    | Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель   | Применяется только для сериализации в формате CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат      | Применяется только для сериализации в формате JSON. Вариант **строки-разделители** предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Если выбрать вариант **строки-разделители**, то JSON считывает по одному объекту за раз. Все содержимое само по себе не будет допустимым форматом JSON. Вариант **массив** означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл. |

При использовании хранилища BLOB-объектов для выходных данных в большом двоичном объекте создается файл в следующих случаях:

* Если размер файла превышает максимально допустимое количество блоков (в настоящее время 50 000). Максимальное количество блоков может быть достигнуто без превышения максимально допустимого размера большого двоичного объекта. Например, при высокой скорости вывода данных в блоке будет большее число байтов, значит, и размер файла будет большим. В случае низкой скорости вывода данных в каждом блоке будет меньше данных, а значит, и размер файла будет меньшим.
* Если схема изменена в выходных данных, а для формата выходных данных требуется фиксированная схема (CSV или Avro).
* При перезапуске задания извне пользователем, который останавливает и затем запускает его, или изнутри для обслуживания системы или восстановления после сбоя.
* Если запрос полностью разделен, для каждого раздела выходных данных создается файл.
* Если пользователь удаляет файл или контейнер учетной записи хранения.
* Если выходные данные разделены по времени с использованием шаблона префикса пути, новый блок применяется, когда запрос переходит к следующему часу.
* Если выходные данные разделяются по пользовательскому полю, то для каждого ключа раздела создается большой двоичный объект, если он не существует.
* Если выходные данные разделяются по пользовательскому полю и кратность ключа раздела превышает 8000, то для каждого ключа раздела может быть создан большой двоичный объект.

## <a name="event-hubs"></a>Центры событий

[Центры событий Azure](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может принимать миллионы событий в секунду. Концентратор событий может использоваться в качестве места назначения выходных данных, например в случае, когда выходные данные задания Stream Analytics становятся входными данными для другого задания потоковой передачи. Сведения о максимальном размере сообщений и оптимизации размера пакета см. в разделе [Размер пакета выходных данных](#output-batch-size).

Для настройки потоков данных из концентраторов событий в качестве выходных данных необходимо использовать несколько параметров.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Понятное имя, которое используется в запросах для направления выходных данных запроса в концентратор событий. |
| Пространство имен концентратора событий | Контейнер для набора сущностей обмена сообщениями. При создании концентратора событий создается также пространство имен концентратора событий. |
| Имя концентратора событий | Имя выходных данных концентратора событий. |
| Имя политики концентратора событий | Политика общего доступа, которую можно создать на вкладке **Настройка** концентратора событий. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики концентратора событий | Ключ общего доступа, используемый для аутентификации доступа к пространству имен концентратора событий. |
| Столбец ключа раздела | Необязательный параметр. Этот столбец содержит ключ раздела для выходных данных концентратора событий. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель | Применяется только для сериализации в формате CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат | Применяется только для сериализации в формате JSON. Вариант **строки-разделители** предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Если выбрать вариант **строки-разделители**, то JSON считывает по одному объекту за раз. Все содержимое само по себе не будет допустимым форматом JSON. Вариант **массив** означает, что выходные данные будут отформатированы как массив объектов JSON.  |
| Столбцы свойств | Необязательный параметр. Столбцы с разделителями-запятыми, которые необходимо присоединить как пользовательские свойства исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства пользовательских метаданных для выходных данных](#custom-metadata-properties-for-output). |

## <a name="power-bi"></a>Power BI

Вы можете использовать [Power BI](https://powerbi.microsoft.com/) в качестве выходных данных для задания Stream Analytics, чтобы обеспечить широкие возможности визуализации результатов анализа. Визуализация позволяет создавать панели мониторинга оперативных данных, формировать отчеты и составлять отчетности на основе метрик.

Выходные данные Power BI из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (21Vianet) и Azure для Германии (T-Systems International).

В таблице ниже перечислены имена свойств и их описания для настройки выходных данных Power BI.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Укажите понятное имя, которое используется в запросах для направления выходных данных запроса в эти выходные данные Power BI. |
| Рабочая область группы |Чтобы иметь возможность обмениваться данными с другими пользователями Power BI, вы можете выбрать группы в своей учетной записи Power BI или щелкнуть **Моя рабочая область**, если не хотите записывать данные в группу. Для обновления существующей группы требуется повторно выполнить проверку подлинности в службе Power BI. |
| Имя набора данных |Укажите имя набора данных, который будет использоваться для выходных данных Power BI. |
| Имя таблицы |Имя таблицы в наборе выходных данных Power BI. Сейчас для вывода выходных данных из заданий Stream Analytics в Power BI можно использовать только одну таблицу в наборе данных. |
| Авторизовать подключение | Чтобы продолжить настройку параметров вывода, необходима авторизация Power BI. После предоставления этим выходным данным доступа к панели мониторинга Power BI доступ можно отозвать, изменив пароль учетной записи пользователя, удалив выходные данные задания или удалив задание Stream Analytics. | 

Пошаговые инструкции по настройке выходных данных и панели мониторинга Power BI приведены в учебнике [Stream Analytics и Power BI. Панель мониторинга для анализа потоковой передачи данных](stream-analytics-power-bi-dashboard.md).

> [!NOTE]
> Не создавайте вручную набор данных и таблицу на панели мониторинга Power BI. Они будут автоматически созданы при запуске задания, когда задание начнет вносить выходные данные в Power BI. Если запрос задания не создает никаких результатов, набор данных и таблица не создаются. Если в Power BI уже есть набор данных и таблица с именем, аналогичным указанному в этом задании Stream Analytics, существующие данные будут перезаписаны.
>

### <a name="create-a-schema"></a>Создание схемы
Azure Stream Analytics создает набор данных Power BI и схему таблицы для пользователя, если они еще не существуют. Во всех остальных случаях таблица обновляется с использованием новых значений. В настоящее время в наборе данных может существовать только одна таблица. 

Power BI использует политику хранения "первым поступил — первым обслужен" (FIFO). Данные будут собираться в таблице до тех пор, пока не будет собрано 200 000 строк.

### <a name="convert-a-data-type-from-stream-analytics-to-power-bi"></a>Преобразование типа данных из Stream Analytics в Power BI
Azure Stream Analytics обновляет модель данных динамически во время выполнения, если меняется схема вывода. Изменяются имена и типы столбцов, а добавление или удаление столбцов отслеживается.

В этой таблице представлено преобразование [типов данных Stream Analytics](https://docs.microsoft.com/stream-analytics-query/data-types-azure-stream-analytics) в [типы данных Entity Data Model (EDM)](https://docs.microsoft.com/dotnet/framework/data/adonet/entity-data-model), используемые в Power BI, если набор данных и таблица Power BI не созданы.

Из Stream Analytics | В Power BI
-----|-----
BIGINT | Int64
nvarchar(max) | Строка
DATETIME | Datetime
FLOAT | Double
Record array | Тип String, постоянное значение IRecord или IArray

### <a name="update-the-schema"></a>Обновление схемы
Stream Analytics определяет схему модели данных на основе первого набора событий в выходных данных. Затем (при необходимости) схема модели данных обновляется для размещения входящих событий, которые могут не соответствовать исходной схеме.

Избегайте запроса `SELECT *`, чтобы предотвратить динамическое обновление схемы в строках. Помимо потенциального влияния на производительность, это также может привести к неопределенности значения времени, затраченного на результаты. Выберите те поля, которые должны отображаться на панели мониторинга Power BI. Кроме того, значения данных должны соответствовать выбранному типу данных.


Предыдущий или текущий | Int64 | Строка | Datetime | Double
-----------------|-------|--------|----------|-------
Int64 | Int64 | Строка | Строка | Double
Double | Double | Строка | Строка | Double
Строка | Строка | Строка | Строка | Строка 
Datetime | Строка | Строка |  Datetime | Строка

## <a name="table-storage"></a>Хранилище таблиц

[Табличное хранилище Azure](../storage/common/storage-introduction.md) отличается высокой степенью доступности и масштабируемости, позволяя приложению автоматически осуществлять масштабирование в соответствии с нуждами пользователя. Табличное хранилище является хранилищем ключей и атрибутов NoSQL корпорации Майкрософт и позволяет работать со структурированными данными с менее жесткими ограничениями схемы. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения.

В таблице ниже приведены имена и описание свойств для создания выходных данных таблицы.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в табличное хранилище. |
| Учетная запись хранения |Имя учетной записи хранения, в которую отправляются выходные данные. |
| Ключ учетной записи хранения |Ключ доступа, связанный с учетной записью хранения. |
| Имя таблицы |Имя таблицы. Создается таблица, если она еще существует. |
| Ключ секции |Имя выходного столбца, содержащее ключ раздела. Ключ раздела — это уникальный идентификатор раздела в пределах таблицы, являющийся первой частью первичного ключа сущности. Это строковое значение размером до 1 КБ. |
| Ключ строки. |Имя выходного столбца, содержащее ключ строки. Ключ строки — это уникальный идентификатор сущности внутри раздела. Он является второй частью первичного ключа сущности. Ключ строки — это строковое значение размером до 1 КБ. |
| Размер пакета |Количество записей в пакетной операции. Значения по умолчанию (100) достаточно для большинства заданий. Дополнительные сведения об изменении этого параметра см. в статье о [спецификациях пакетных операций с таблицами](https://docs.microsoft.com/java/api/com.microsoft.azure.storage.table.tablebatchoperation). |

## <a name="service-bus-queues"></a>Очереди служебной шины

[Очереди служебной шины](../service-bus-messaging/service-bus-queues-topics-subscriptions.md) доставляют сообщения конкурирующим потребителям по типу FIFO. Как правило, сообщения принимаются и обрабатываются получателями в том порядке, в котором они были добавлены в очередь. Каждое сообщение получается и обрабатывается только одним объектом-получателем сообщения.

На [уровне совместимости 1.2](stream-analytics-compatibility-level.md) Azure Stream Analytics использует для записи в очереди и разделы служебной шины протокол передачи сообщений [AMQP](../service-bus-messaging/service-bus-amqp-overview.md). AMQP позволяет создавать кроссплатформенные гибридные приложения, использующие протокол открытого стандарта.

В таблице ниже приведены имена и описание свойств для создания выходных данных очереди.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в очередь служебной шины. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. |
| Имя очереди |Имя очереди служебной шины. |
| Имя политики очереди |При создании очереди можно также создать политики общего доступа на вкладке **Настройка** очереди. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики очереди |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель |Применяется только для сериализации в формате CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат |Применимо только для типа JSON. Вариант **строки-разделители** предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Если выбрать вариант **строки-разделители**, то JSON считывает по одному объекту за раз. Все содержимое само по себе не будет допустимым форматом JSON. Вариант **массив** означает, что выходные данные будут отформатированы как массив объектов JSON. |
| Столбцы свойств | Необязательный параметр. Столбцы с разделителями-запятыми, которые необходимо присоединить как пользовательские свойства исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства пользовательских метаданных для выходных данных](#custom-metadata-properties-for-output). |
| Столбцы системных свойств | Необязательный параметр. Пары "ключ-значение" системных свойств и соответствующих имен столбцов, которые необходимо присоединить к исходящему сообщению вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Системные свойства для выходных данных очереди или раздела служебной шины](#system-properties-for-service-bus-queue-and-topic-outputs).  |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.

## <a name="service-bus-topics"></a>Разделы шины обслуживания
Очереди служебной шины предоставляют метод связи "один к одному" между отправителем и получателем. [Разделы служебной шины](https://msdn.microsoft.com/library/azure/hh367516.aspx) предоставляют тип связи "один ко многим".

В таблице ниже приведены имена и описание свойств для создания выходных данных раздела служебной шины.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в раздел служебной шины. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. При создании концентратора событий создается также пространство имен служебной шины. |
| Имя раздела |Разделы являются сущностями обмена сообщениями, как концентраторы событий и очереди. Они предназначены для сбора потоков событий из устройств и служб. Созданному разделу присваивается определенное имя. Сообщения, отправленные в раздел, будут недоступны, пока не создана подписка. Поэтому убедитесь, что раздел содержит одну или несколько подписок. |
| Имя политики раздела |При создании раздела служебной шины можно также создать политики общего доступа на вкладке **Настройка** раздела. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики раздела |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель |Применяется только для сериализации в формате CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Столбцы свойств | Необязательный параметр. Столбцы с разделителями-запятыми, которые необходимо присоединить как пользовательские свойства исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства пользовательских метаданных для выходных данных](#custom-metadata-properties-for-output). |
| Столбцы системных свойств | Необязательный параметр. Пары "ключ-значение" системных свойств и соответствующих имен столбцов, которые необходимо присоединить к исходящему сообщению вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Системные свойства для выходных данных очереди или раздела служебной шины](#system-properties-for-service-bus-queue-and-topic-outputs). |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.

## <a name="azure-cosmos-db"></a>Azure Cosmos DB
[Azure Cosmos DB](https://azure.microsoft.com/services/documentdb/) — это глобально распределенная служба базы данных, предоставляющая гибкое масштабирование без ограничений по всему миру, расширенные возможности запросов и автоматическое индексирование по моделям данных, не зависящим от схемы. Дополнительные сведения о параметрах контейнера Azure Cosmos DB для Stream Analytics см. в статье [Stream Analytics с Azure Cosmos DB в качестве выходных данных](stream-analytics-documentdb-output.md).

Выходные данные Azure Cosmos DB из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (21Vianet) и Azure для Германии (T-Systems International).

> [!Note]
> В настоящее время Azure Stream Analytics поддерживает соединение только с Azure Cosmos DB при помощи SQL API.
> Другие API Azure Cosmos DB в данный момент не поддерживаются. Если указать модулю Azure Stream Analytics учетные записи Azure Cosmos DB, созданные при помощи других API, это может привести к неправильному сохранению данных.

В следующей таблице описаны свойства для создания выходных данных Azure Cosmos DB.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Псевдоним для ссылки на эти выходные данные в запросе Stream Analytics. |
| Приемник | Azure Cosmos DB. |
| Вариант импорта | Выберите один из вариантов: **Select Cosmos DB from your subscription** (Выбрать Cosmos DB из своей подписки) или **Provide Cosmos DB settings manually** (Указать параметры Cosmos DB вручную).
| Идентификатор учетной записи | Имя или универсальный код ресурса (URI) конечной точки учетной записи Azure Cosmos DB. |
| Ключ учетной записи | Общедоступный ключ доступа к учетной записи Azure Cosmos DB. |
| База данных | Имя базы данных Azure Cosmos DB. |
| Имя контейнера | Имя используемого контейнера, который должен существовать в Cosmos DB. Пример  <br /><ul><li> _MyContainer_: Контейнер с именем "MyContainer" должен существовать.</li>|
| Идентификатор документа |Необязательный параметр. Имя поля в выходных событиях, используемое для указания первичного ключа, на котором основываются операции вставки или обновления.

## <a name="azure-functions"></a>Функции Azure
Функции Azure — это бессерверная служба вычислений, которая позволяет выполнять код по требованию без необходимости явно подготавливать или администрировать инфраструктуру. Они позволяют реализовать код, который запускается событиями, возникающими в Azure или партнерских службах. Эта возможность Функций Azure реагировать на триггеры упрощает вывод данных Azure Stream Analytics. Этот выходной адаптер позволяет пользователям подключать Stream Analytics к Функциям Azure и запускать сценарий или часть кода в ответ на ряд событий.

Выходные данные Функций Azure из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (21Vianet) и Azure для Германии (T-Systems International).

Azure Stream Analytics вызывает Функции Azure через триггеры HTTP. Адаптер выходных данных функции Azure доступен со следующими настраиваемыми свойствами:

| Имя свойства | Описание |
| --- | --- |
| Приложение-функция |Имя приложения-функции Azure. |
| Компонент |Имя функции в приложении-функции Azure. |
| Клавиши |Если нужно использовать службу "Функции Azure" из другой подписки это можно сделать, предоставив ключ для доступа к функции. |
| Максимальный размер пакета |Свойство, которое позволяет задать максимальный размер для каждого выходного пакета, отправляемого в функцию Azure. Объем входных данных задается в байтах. По умолчанию это значение равно 262 144 байт (256 КБ). |
| Максимальное количество пакетов  |Свойство, которое позволяет указать максимальное число событий в каждом пакете, отправляемых в функции Azure. По умолчанию используется значение 100. |

Azure Stream Analytics ожидает HTTP-состояние 200 из приложения-функции для пакетов, которые были успешно обработаны.

Когда служба Azure Stream Analytics получает исключение 413 (сущность запроса HTTP слишком большая) из службы "Функции Azure", размер пакетов, отправляемых в службу "Функции Azure", уменьшается. В коде функции Azure это исключение позволяет убедится, что Azure Stream Analytics не отправляет пакеты слишком большого размера. Кроме того, убедитесь, что максимальное количество пакетов и размеры значений, используемые в функции, соответствуют значениям, введенным на портале Stream Analytics.

> [!NOTE]
> Во время тестового подключения Stream Analytics отправляет пустой пакет в Функции Azure для проверки, если соединение между ними работает. Убедитесь, что приложение-функция обрабатывает пустые пакетные запросы, чтобы убедиться, что проверка пройдена.

Кроме того, в ситуации, когда во временном окне не происходит целевое событие, никакие выходные данные не генерируются. В результате функция **computeResult** не вызывается. Такое поведение согласуется со встроенными оконными агрегатными функциями.

## <a name="custom-metadata-properties-for-output"></a>Свойства пользовательских метаданных для выходных данных 

Столбцы запросов можно прикреплять к исходящим сообщениям как пользовательские свойства. Эти столбцы не переходят в полезные данные. Свойства представлены в виде словаря в выходном сообщении. *Ключ* — это имя столбца, а *значение* — это значение столбца в словаре свойств. Поддерживаются все типы данных Stream Analytics, кроме записи и массива.  

Поддерживаемые выходные данные: 
* Очередь служебной шины 
* Раздел служебной шины 
* концентратор событий; 

В следующем примере мы добавим в метаданные два поля `DeviceId` и `DeviceStatus`. 
* Запрос: `select *, DeviceId, DeviceStatus from iotHubInput`
* Конфигурация выходных данных: `DeviceId,DeviceStatus`

![Столбцы свойств](./media/stream-analytics-define-outputs/10-stream-analytics-property-columns.png)

На следующем снимке экрана показаны свойства исходящих сообщений, проверенные в EventHub с помощью [Service Bus Explorer](https://github.com/paolosalvatori/ServiceBusExplorer).

![Настраиваемые свойства события](./media/stream-analytics-define-outputs/09-stream-analytics-custom-properties.png)

## <a name="system-properties-for-service-bus-queue-and-topic-outputs"></a>Свойства системы для выходных данных очереди и раздела служебной шины 
Столбцы запросов можно присоединять как [системные свойства](https://docs.microsoft.com/dotnet/api/microsoft.servicebus.messaging.brokeredmessage?view=azure-dotnet#properties) к исходящим сообщениям очереди или раздела служебной шины. Эти столбцы не попадают в полезные данные, а соответствующее [системное свойство](https://docs.microsoft.com/dotnet/api/microsoft.servicebus.messaging.brokeredmessage?view=azure-dotnet#properties) BrokeredMessage заполняется значениями столбцов запроса.
Эти системные свойства поддерживаются — `MessageId, ContentType, Label, PartitionKey, ReplyTo, SessionId, CorrelationId, To, ForcePersistence, TimeToLive, ScheduledEnqueueTimeUtc`.
Строковые значения этих столбцов анализируются как соответствующие типы значений системных свойств, а все ошибки синтаксического анализа обрабатываются как ошибки данных.
Это поле предоставляется в формате объекта JSON. Ниже приведены сведения об этом формате.
* Заключаются в фигурные скобки {}.
* Записываются в виде пар "ключ — значение".
* Ключи и значения должны быть строковыми.
* Ключ — это имя системного свойства, а значение — имя столбца запроса.
* Ключи и значения разделяются двоеточием.
* Каждая пара "ключ — значение" разделяется запятой.

В этом примере показано, как использовать это свойство —

* Запрос: `select *, column1, column2 INTO queueOutput FROM iotHubInput`
* Столбцы системных свойств: `{ "MessageId": "column1", "PartitionKey": "column2"}`

Задает `MessageId` в сообщениях очереди служебной шины со значениями `column1`, а для PartitionKey задаются значения `column2`.

## <a name="partitioning"></a>Секционирование

В следующей таблице указаны поддержка секционирования и количество записей выходных данных для каждого типа данных:

| Тип выходных данных | Поддержка секционирования | Ключ секции  | Количество записей выходных данных |
| --- | --- | --- | --- |
| Хранилище озера данных Azure | Да | Используйте токены {date} и {time} в шаблоне префикса пути. Выберите формат даты, например ГГГГ-ММ-ДД, ДД-ММ-ГГГГ, ММ-ДД-ГГГГ. Для времени используется формат ЧЧ. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| База данных SQL Azure | Да, необходимо включить. | Основано на предложении PARTITION BY в запросе | Если включен параметр Inherit Partitioning (Наследование секционирования), выполните секционирование входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). Дополнительные сведения об улучшении пропускной способности операций записи при загрузке данных в базу данных SQL Azure см. в статье [Вывод данных Azure Stream Analytics в базу данных SQL Azure](stream-analytics-sql-output-perf.md). |
| Хранилище BLOB-объектов Azure | Да | Используйте в шаблоне пути токены {date} и {time} {fieldname} из полей событий. Выберите формат даты, например ГГГГ-ММ-ДД, ДД-ММ-ГГГГ, ММ-ДД-ГГГГ. Для времени используется формат ЧЧ. Выходные данные большого двоичного объекта можно секционировать с помощью одного атрибута настраиваемого события {fieldname} или {datetime:\<specifier>}. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Центры событий Azure | Да | Да | Изменяется в зависимости от выравнивания секций.<br /> Если ключ раздела для выходных данных концентратора событий не согласован с вышестоящим (предыдущим) шагом запроса, то число модулей записи совпадает с количеством разделов в выходных данных концентратора событий. Каждый модуль записи использует [класс EventHubSender](/dotnet/api/microsoft.servicebus.messaging.eventhubsender?view=azure-dotnet) для отправки событий в определенный раздел. <br /> Если ключ раздела для выходных данных концентратора событий не согласован с вышестоящим (предыдущим) шагом запроса, то число модулей записи совпадает с количеством разделов на этом предыдущем шаге. Каждый модуль записи использует [класс SendBatchAsync](/dotnet/api/microsoft.servicebus.messaging.eventhubclient.sendasync?view=azure-dotnet) в **EventHubClient** для отправки событий во все разделы выходных данных. |
| Power BI | нет | None | Неприменимо. |
| табличное хранилище Azure; | Да | Любой выходной столбец.  | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Раздел служебной шины Azure | Да | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.| Совпадает с количеством секций в разделе выходных данных.  |
| Очередь служебной шины Azure | Да | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.| Совпадает с количеством секций в очереди выходных данных. |
| Azure Cosmos DB | Да | Основано на предложении PARTITION BY в запросе | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Функции Azure | Да | Основано на предложении PARTITION BY в запросе | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |

Количество модулей записи вывода также можно контролировать с помощью предложения `INTO <partition count>` (см. [INTO](https://docs.microsoft.com/stream-analytics-query/into-azure-stream-analytics#into-shard-count)) в запросе, что может оказаться полезным при достижении необходимой топологии задания. Если выходной адаптер не секционирован, нехватка данных в одном входном разделе приведет к задержке вплоть до установленного допустимого интервала. В таком случае выходные данные объединяются в общий модуль записи, что может привести к возникновению узких мест в конвейере. Дополнительные сведения о политике допустимого интервала поступления с задержкой см. в статье [Рассмотрение порядка событий Azure Stream Analytics](stream-analytics-out-of-order-and-late-events.md).

## <a name="output-batch-size"></a>Размер выходного пакета
Azure Stream Analytics использует пакеты переменного размера для обработки событий и записи выходных данных. Обычно модуль Stream Analytics не записывает сообщение по одному, и использует пакеты для повышения эффективности. Если интенсивность входящих и исходящих событий высока, Stream Analytics использует большие пакеты. При низкой интенсивности исходящего трафика используются пакеты меньшего размера, чтобы обеспечить низкую задержку.

В следующей таблице описаны некоторые аспекты пакетной обработки выходных данных.

| Тип выходных данных |    Максимальный размер сообщения | Оптимизация размера пакета |
| :--- | :--- | :--- |
| Хранилище озера данных Azure | Ознакомьтесь с [ограничениями для Azure Data Lake Storage](../azure-resource-manager/management/azure-subscription-service-limits.md#data-lake-store-limits). | Используйте до 4 МБ на операцию записи. |
| База данных SQL Azure | Можно настроить с помощью параметра максимального количества пакетов. Максимум 10 000 и минимум 100 строк на одну операцию групповой вставки по умолчанию.<br />См. раздел [Ограничения для Azure SQL](../sql-database/sql-database-resource-limits.md). |  Каждый пакет первоначально вставляется с максимальным количеством пакетов. Пакет разделяется надвое (до достижения минимального количества пакетов) в зависимости от повторяемых ошибок SQL. |
| Хранилище BLOB-объектов Azure | Ознакомьтесь с [ограничениями службы хранилища Azure](../azure-resource-manager/management/azure-subscription-service-limits.md#storage-limits). | Максимальный размер блока большого двоичного объекта — 4 МБ.<br />Максимальное количество блоков большого двоичного объекта — 50 000. |
| Центры событий Azure    | 256 КБ или 1 МБ на сообщение. <br />См. раздел [Ограничения Центров событий](../event-hubs/event-hubs-quotas.md). |    Если секционирование ввода-вывода не согласовано, каждое событие упаковывается по отдельности в `EventData` и отправляется в составе пакета, пока его размер не достигнет максимального размера сообщения. Это также происходит, если используются [пользовательские свойства метаданных](#custom-metadata-properties-for-output). <br /><br />  Если разделение входных и выходных данных согласовано, то нескольких событий упаковываются в один экземпляр `EventData`, пока его размер не достигнет максимального размера сообщения, после чего он отправляется.    |
| Power BI | См. раздел [Ограничения Power BI REST API](https://msdn.microsoft.com/library/dn950053.aspx). |
| табличное хранилище Azure; | Ознакомьтесь с [ограничениями службы хранилища Azure](../azure-resource-manager/management/azure-subscription-service-limits.md#storage-limits). | Значение по умолчанию — 100 сущностей на одну транзакцию. При необходимости можно настроить меньшее значение. |
| Очередь служебной шины Azure    | 256 КБ на сообщение для уровня "Стандартный", 1 МБ для уровня "Премиум".<br /> См. раздел [Ограничения служебной шины](../service-bus-messaging/service-bus-quotas.md). | Используйте одно событие на сообщение. |
| Раздел служебной шины Azure | 256 КБ на сообщение для уровня "Стандартный", 1 МБ для уровня "Премиум".<br /> См. раздел [Ограничения служебной шины](../service-bus-messaging/service-bus-quotas.md). | Используйте одно событие на сообщение. |
| Azure Cosmos DB    | См. раздел [Ограничения Azure Cosmos DB](../azure-resource-manager/management/azure-subscription-service-limits.md#azure-cosmos-db-limits). | Размер пакета и частота записи корректируются динамически с учетом ответов Azure Cosmos DB. <br /> Предопределенные ограничения для Stream Analytics отсутствуют. |
| Функции Azure    | | Размер пакета по умолчанию составляет 262 144 байт (256 КБ). <br /> Число событий в пакете по умолчанию равно 100. <br /> Размер пакета можно настроить, его можно увеличить или уменьшить в [параметрах вывода](#azure-functions) Stream Analytics.

## <a name="next-steps"></a>Дальнейшие действия
> [!div class="nextstepaction"]
> 
> [Краткое руководство. по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: https://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: https://go.microsoft.com/fwlink/?LinkId=517301
