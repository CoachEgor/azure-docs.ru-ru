---
title: Описание выходных данных из Azure Stream Analytics
description: В этой статье описываются параметры вывода данных для анализа результатов, доступные в Azure Stream Analytics, включая Power BI.
services: stream-analytics
author: mamccrea
ms.author: mamccrea
ms.reviewer: jasonh
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 05/31/2019
ms.openlocfilehash: b29f3168b7ecc1ec8f783a7ce7a6dea83318fa14
ms.sourcegitcommit: d4dfbc34a1f03488e1b7bc5e711a11b72c717ada
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/13/2019
ms.locfileid: "66455711"
---
# <a name="understand-outputs-from-azure-stream-analytics"></a>Описание выходных данных из Azure Stream Analytics

В этой статье описываются типы выходных данных, доступных для задания Azure Stream Analytics. Выходные данные дают возможность сохранить и хранить результаты задания Stream Analytics. С помощью выходных данных, можно выполнить дополнительные бизнес-аналитики и данных для хранения данных.

При проектировании запроса Stream Analytics, ссылки на имя выходных данных с помощью [предложение INTO](https://msdn.microsoft.com/azure/stream-analytics/reference/into-azure-stream-analytics). Можно использовать один выход на задание или несколько выходных данных для потоковой передачи задания (если они нужны), предоставляя несколько предложений INTO в запросе.

Для создания, изменения и проверки задания Stream Analytics были выведены, вы можете использовать [портала Azure](stream-analytics-quick-create-portal.md#configure-job-output), [Azure PowerShell](stream-analytics-quick-create-powershell.md#configure-output-to-the-job), [.NET API](https://docs.microsoft.com/dotnet/api/microsoft.azure.management.streamanalytics.ioutputsoperations?view=azure-dotnet), [REST API](https://docs.microsoft.com/rest/api/streamanalytics/stream-analytics-output), и [Visual Studio](stream-analytics-quick-create-vs.md).

Некоторые типы поддерживают выходы [секционирование](#partitioning). [Выходные данные размером пакета](#output-batch-size) различаются для оптимизации пропускной способности.


## <a name="azure-data-lake-storage-gen-1"></a>Хранилище Azure Data Lake поколения 1

Stream Analytics поддерживает [Azure Data Lake хранилища Gen 1](../data-lake-store/data-lake-store-overview.md). Azure Data Lake Store является корпоративного уровня, гипермасштабируемый репозиторий для рабочих нагрузок анализа больших данных. Хранилище Data Lake можно использовать для хранения данных любого размера, типа и скоростью приема в одном месте для эксплуатационной и исследовательской аналитики. Stream Analytics должен быть авторизован для доступа к хранилищу Озера данных.

Выходные данные хранилища Озера данных Azure Stream Analytics сейчас недоступна в Azure для Китая (21Vianet) и регионы Azure для Германии (T-Systems International).

Ниже перечислены имена свойств и их описания для настройки выходных данных Data Lake хранилища Gen 1.   

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных | Понятное имя, используемое в запросах для направления выходных данных запроса в Data Lake Store. |
| Подписка | Подписка, которая содержит учетную запись хранения Озера данных Azure. |
| Имя учетной записи | Имя учетной записи Data Lake Store, где вы отправляете ваши выходные данные. Вам предоставляется стрелку раскрывающегося списка учетных записей Data Lake Store, доступных в вашей подписке. |
| Шаблон префикса пути | Путь к файлу, который используется для сохранения файлов в указанной учетной записи Data Lake Store. Можно указать один или несколько экземпляров {date} и {time} переменные:<br /><ul><li>Пример 1. folder1/logs/{дата}/{время}</li><li>Пример 2. folder1/logs/{дата}</li></ul><br />Метка времени структуры созданной папке соответствует UTC и не местное время.<br /><br />Если шаблон пути к файлу не содержит косой чертой (/), последний шаблон в пути к файлу рассматривается как префикс имени файла. <br /><br />Новые файлы создаются в следующих ситуациях:<ul><li>изменения в схеме выходных данных;</li><li>Внешний или внутренний перезапуск задания</li></ul> |
| Формат даты | Необязательный элемент. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
|Формат времени | Необязательный элемент. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro.|
| Кодирование | Если вы используете формат CSV или JSON, должна быть указана кодировка. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8.|
| Разделитель | Применимо только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.|
| Формат | Применяется только для сериализации JSON. **Строки-разделители** указывает, что выходные данные форматируются по каждый объект JSON, разделенных точкой с новой строки. **Массив** указывает, что выходные данные форматируются как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. Как правило это предпочтительнее использовать JSON, разделенных на строки, так как он не требует никакой специальной обработки, пока выходного файла по-прежнему записывается.|
| Режим проверки подлинности | Вы можете предоставить права доступа к учетной записи хранения Озера данных с помощью [управляемое удостоверение](stream-analytics-managed-identities-adls.md) или токен пользователя. После предоставления доступа, вы можете отозвать доступ, изменение пароля учетной записи пользователя, удаление выходных данных хранилища Озера данных для этого задания или удаление задания Stream Analytics. |

## <a name="sql-database"></a>База данных SQL

Можно использовать [базы данных SQL Azure](https://azure.microsoft.com/services/sql-database/) в выходных данных для данных, реляционных по своей природе или для приложений, которые зависят от содержимого, размещенного в реляционной базе данных. Задания Stream Analytics запись в существующую таблицу в базе данных SQL. Схема таблицы должно совпадать, полям и их типам в выходных данных задания. Можно также указать [хранилище данных SQL Azure](https://azure.microsoft.com/documentation/services/sql-data-warehouse/) как выходные данные с помощью базы данных SQL выходной параметр. Чтобы узнать о способах повышения пропускной способности записи, см. в разделе [Stream Analytics с базой данных SQL Azure как выходные данные](stream-analytics-sql-output-perf.md) статьи. 

Ниже перечислены имена свойств и описание для создания выходных данных в базе данных SQL.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую базу данных. |
| База данных | Имя базы данных, где вы отправляете ваши выходные данные. |
| Имя сервера | Имя сервера Базы данных SQL. |
| Имя пользователя | Имя пользователя, который имеет доступ на запись к базе данных. Stream Analytics поддерживает только проверку подлинности SQL. |
| Пароль | Пароль для подключения к базе данных. |
| Таблица | Имя таблицы, в которую записываются выходные данные. В имени таблицы учитывается регистр. Схема этой таблицы должно полностью совпадать количество полей и их типы, которые создает выходные данные задания. |
|Наследование схемы секционирования| Параметр наследования схемы секционирования на предыдущем шаге запроса для включения полной параллельной обработкой топологии с несколькими модулями записи в таблицу. Дополнительные сведения см. в статье [Вывод данных Azure Stream Analytics в Базу данных SQL Azure](stream-analytics-sql-output-perf.md).|
|Соответствие количеству пакетов| Рекомендуемое предельное количество записей, отправляемых с каждой одновременной Вставить транзакцию.|

> [!NOTE]
> База данных SQL Azure, предлагая поддерживается для задания выходных данных в Stream Analytics, но виртуальной машины Azure к SQL Server с подключенной базой данных не поддерживается.

## <a name="blob-storage"></a>Хранилище BLOB-объектов

Хранилище BLOB-объектов предлагает экономичное и масштабируемое решение для хранения больших объемов неструктурированных данных в облаке. Общие сведения о хранилище BLOB-объектов и их использование, см. в разделе [отправка, скачивание и перечисление больших двоичных объектов с помощью портала Azure](../storage/blobs/storage-quickstart-blobs-portal.md).

Ниже перечислены имена свойств и их описания для создания выходных данных в BLOB-объектов.

| Имя свойства       | ОПИСАНИЕ                                                                      |
| ------------------- | ---------------------------------------------------------------------------------|
| Псевдоним выходных данных        | Понятное имя, которое используется в запросах для направления выходных данных запроса в хранилище BLOB-объектов. |
| Учетная запись хранения     | Имя учетной записи хранения, где вы отправляете ваши выходные данные.               |
| Ключ учетной записи хранения | Секретный ключ, связанный с учетной записью хранения.                              |
| Контейнер хранилища   | Логическое группирование для больших двоичных объектов, хранящихся в службе BLOB-объектов Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер. |
| Шаблон пути | Необязательный элемент. Шаблон пути к файлу, который используется для записи больших двоичных объектов в указанном контейнере. <br /><br /> В шаблоне пути вы можете использовать один или несколько экземпляров переменных даты и времени, чтобы указать периодичность записи BLOB-объектов: <br /> {date}, {time} <br /><br />Пользовательское секционирование большого двоичного объекта можно использовать для предварительной версии, указав одно пользовательское имя поля {field} из данных события. Имя поля может содержать буквы, цифры, дефисы и символы подчеркивания. Существуют следующие ограничения для пользовательских полей. <ul><li>Имена полей не учитывается. Например служба не различает столбец «ID» и столбце «id».</li><li>Не допускаются вложенные поля. Вместо этого используйте псевдоним в запросе задания для «уплощения» в поле.</li><li>Выражения нельзя использовать в качестве имени поля.</li></ul> <br />Этот компонент допускается использовать в пути конфигурации описателей пользовательских форматов даты и времени. Пользовательские форматы даты и времени необходимо указывать по одному в виде ключевого слова {datetime:\<описатель>}. Допустимые входные данные для \<описатель > являются гггг, мм, M, дд, d, HH, H, mm, m, ss или s. {Datetime:\<описатель >} ключевое слово может использоваться несколько раз в путь для формирования конфигурации пользовательские даты и времени. <br /><br />Примеры: <ul><li>Пример 1: cluster1/logs/{date}/{time}</li><li>Пример 2: cluster1/logs/{date}</li><li>Пример 3: cluster1/{client_id}/{date}/{time}</li><li>Пример 4: cluster1/{datetime:ss}/{myField}, где запрос имеет следующий вид: SELECT data.myField AS myField FROM Input;</li><li>Пример 5: cluster1/year={datetime:yyyy}/month={datetime:MM}/day={datetime:dd}</ul><br />Метка времени структуры созданной папке соответствует UTC и не местное время.<br /><br />Именования файлов использует следующее соглашение: <br /><br />{Шаблон префикса пути}/schemaHashcode_Guid_Number.extension<br /><br />Выходные файлы примера:<ul><li>Myoutput/20170901/00/45434_gguid_1.csv</li>  <li>Myoutput/20170901/01/45434_gguid_1.csv</li></ul> <br />Дополнительные сведения об этой функции см. в разделе [Azure Stream Analytics пользовательский BLOB-объектов в выходных данных секционирования](stream-analytics-custom-path-patterns-blob-storage-output.md). |
| Формат даты | Необязательный элемент. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
| Формат времени | Необязательный элемент. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование    | Если вы используете формат CSV или JSON, должна быть указана кодировка. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель   | Применимо только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат      | Применяется только для сериализации JSON. **Строки-разделители** указывает, что выходные данные форматируются по каждый объект JSON, разделенных точкой с новой строки. **Массив** указывает, что выходные данные форматируются как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. Как правило это предпочтительнее использовать JSON, разделенных на строки, так как он не требует никакой специальной обработки, пока выходного файла по-прежнему записывается. |

При использовании хранилища BLOB-объектов как выходные данные, новый файл создается в большой двоичный объект в следующих случаях:

* Если размер файла превышает максимально допустимое количество блоков (в настоящее время 50 000). Максимальное количество блокировок может достигнуть без достижения максимальный допустимый BLOB-объектов. Например, при высокой скорости вывода данных в блоке будет большее число байтов, значит, и размер файла будет большим. В случае низкой скорости вывода данных в каждом блоке будет меньше данных, а значит, и размер файла будет меньшим.
* Если имеется схема изменена в выходных данных, и формат выходных данных требуется фиксированная схема ("CSV" и "Avro").
* При перезапуске задания извне пользователем, который останавливает и затем запускает его, или изнутри для обслуживания системы или восстановления после сбоя.
* Если запрос полностью секционирован, а также новый файл создается для каждого выходного секции.
* Если пользователь удаляет файл или контейнер учетной записи хранения.
* Если выходные данные — время, разделенного с применением шаблона префикса пути, а новый большой двоичный объект используется в том случае, когда запрос переходит на следующий час.
* Если выход является секционированным, пользовательское поле, а каждого ключа if секции создается новый большой двоичный объект существует.
* Если выход является секционированным настраиваемое поле, где секции ключа количество элементов превышает 8000, а на ключ секции создается новый большой двоичный объект.

## <a name="event-hubs"></a>Центры событий;

[Центры событий Azure](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может принимать миллионы событий в секунду. Один из способов использования концентратора событий как выходные данные при выходные данные задания Stream Analytics становятся входными данными для другого задания потоковой передачи.

Вам потребуется несколько параметров для настройки потоков данных из концентраторов событий в качестве выходных данных.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных | Понятное имя, используемое в запросах для направления выходных данных запроса в концентратор событий. |
| Пространство имен концентратора событий |Контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий, вы также создали пространство имен концентратора событий. |
| Имя концентратора событий | Имя выходных данных концентратора событий. |
| Имя политики концентратора событий | Политика общего доступа, которую можно создать в концентраторе событий **Настройка** вкладки. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики концентратора событий | Ключ общего доступа, используемый для проверки подлинности при доступе к пространство имен концентратора событий. |
| Столбец ключа секции | Необязательный элемент. Столбец, содержащий ключ раздела для выходных данных концентратора событий. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель | Применимо только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат | Применяется только для сериализации JSON. **Строки-разделители** указывает, что выходные данные форматируются по каждый объект JSON, разделенных точкой с новой строки. **Массив** указывает, что выходные данные форматируются как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. Как правило это предпочтительнее использовать JSON, разделенных на строки, так как он не требует никакой специальной обработки, пока выходного файла по-прежнему записывается. |
| Столбцы свойств | Необязательный элемент. Столбцы, с разделителями запятыми, которые необходимо подключить как пользователя свойства исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции находится в разделе [пользовательские метаданные свойства для выходных данных](#custom-metadata-properties-for-output). |

## <a name="power-bi"></a>Power BI

Можно использовать [Power BI](https://powerbi.microsoft.com/) качестве выходных данных для задания Stream Analytics для обеспечения более широкие возможности визуализации возможности анализа результатов. Эту возможность можно использовать для панели мониторинга оперативных, создания отчетов и отчетов на основе метрики.

Выходные данные Power BI из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (21Vianet) и Azure для Германии (T-Systems International).

В следующей таблице перечислены имена свойств и их описания для настройки выходных данных Power BI.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Укажите понятное имя, которое используется в запросах для направления выходных данных запроса в этих выходных данных Power BI. |
| Рабочая область группы |Чтобы использовать данные совместно с другими пользователями Power BI, можно выбрать группы в учетную запись Power BI или выбрать **Моя рабочая область** Если вы не хотите для записи в группу. Для обновления существующей группы требуется повторно выполнить проверку подлинности в службе Power BI. |
| Имя набора данных |Укажите имя набора данных, выходные данные Power BI для использования. |
| Имя таблицы |Имя таблицы в наборе выходных данных Power BI. Сейчас для вывода выходных данных из заданий Stream Analytics в Power BI можно использовать только одну таблицу в наборе данных. |
| Авторизовать подключение | Необходимо авторизовать с помощью Power BI, чтобы настроить параметры вывода. После вывода доступ можно предоставить панель мониторинга Power BI, вы можете отозвать доступ, изменение пароля учетной записи пользователя, удаление выходных данных задания или удаление задания Stream Analytics. | 

Пошаговое руководство по настройке выходных данных Power BI и панели мониторинга, см. в разделе [Azure Stream Analytics и Power BI](stream-analytics-power-bi-dashboard.md) руководства.

> [!NOTE]
> Не явно создать набор данных и таблицу на панели мониторинга Power BI. Набор данных и таблицы заполняются автоматически при запуске задания и задание начнет вносить выходные данные в Power BI. Если запрос задания не создает никаких результатов, набор данных и таблица не создаются. Если в Power BI уже есть набор данных и таблицы с тем же именем, которое было указано в этом задании Stream Analytics, существующие данные перезаписываются.
>

### <a name="create-a-schema"></a>Создание схемы
Azure Stream Analytics создает схему набора данных и таблицы Power BI для пользователя, если они еще не существуют. Во всех остальных случаях таблица обновляется с использованием новых значений. В настоящее время в набор данных может существовать только одна таблица. 

Power BI использует политику хранения первым пришел, первым обслужен (FIFO). Данные будут собираться в таблицу, пока она не достигнет 200 000 строк.

### <a name="convert-a-data-type-from-stream-analytics-to-power-bi"></a>Преобразование типа данных из Stream Analytics в Power BI
Azure Stream Analytics обновляет модель данных динамически во время выполнения, если меняется схема вывода. Изменяются имена и типы столбцов, а добавление или удаление столбцов отслеживается.

В этой таблице описываются преобразования типов данных из [типов данных Stream Analytics](https://msdn.microsoft.com/library/azure/dn835065.aspx) в Power BI [Entity Data Model (EDM) типы](https://docs.microsoft.com/dotnet/framework/data/adonet/entity-data-model), если панель мониторинга Power BI и таблица не существует.

Из Stream Analytics | В Power BI
-----|-----
bigint | Int64
nvarchar(max) | Строка
Datetime | DateTime
float; | Double
Record array | Строковый тип, постоянное значение «IRecord» или «IArray»

### <a name="update-the-schema"></a>Обновление схемы
Stream Analytics определяет схему модели данных на основе первого набора событий в выходных данных. Позже при необходимости в схему модели данных обновляется для размещения входящих событий, которые могут не соответствовать исходной схеме.

Избегайте `SELECT *` запрос, чтобы запретить динамическое обновление схемы по строкам. Помимо потенциального влияния на производительность он может привести к неопределенности времени, затраченного на результаты. Выберите определенные поля, которые должны отображаться на панели мониторинга Power BI. Кроме того, значения данных должны соответствовать выбранному типу данных.


Предыдущий и текущий | Int64 | Строка | DateTime | Double
-----------------|-------|--------|----------|-------
Int64 | Int64 | Строка | Строка | Double
Double | Double | Строка | Строка | Double
Строка | String | String | String | Строка 
DateTime | Строка | Строка |  DateTime | String

## <a name="table-storage"></a>Хранилище таблиц

[Табличное хранилище Azure](../storage/common/storage-introduction.md) отличается высокой степенью доступности и масштабируемости, позволяя приложению автоматически осуществлять масштабирование в соответствии с нуждами пользователя. Хранилище таблиц — хранилище ключей и атрибутов NoSQL корпорации Майкрософт, который можно использовать для структурированных данных с менее жесткими ограничениями схемы. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения.

Ниже перечислены имена свойств и их описания для создания выходных данных в таблице.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в табличное хранилище. |
| Учетная запись хранения |Имя учетной записи хранения, где вы отправляете ваши выходные данные. |
| Ключ учетной записи хранения |Ключ доступа, связанный с учетной записью хранения. |
| Имя таблицы |Это имя таблицы. При этом создается таблица, если он не существует. |
| Ключ секции |Имя выходного столбца, который содержит ключ раздела. Ключ секции — это уникальный идентификатор для секции в пределах таблицы, являющийся первой частью первичного ключа сущности. Это строковое значение, может быть размером до 1 КБ. |
| Ключ строки. |Имя выходного столбца, содержащего ключ строки. Ключ строки — это уникальный идентификатор для сущности внутри секции. Он является второй частью первичного ключа сущности. Ключ строки — строковое значение, может быть размером до 1 КБ. |
| Размер пакета |Количество записей в пакетной операции. Значения по умолчанию (100) достаточно для большинства заданий. См. в разделе [спецификации пакетных операций](https://docs.microsoft.com/java/api/com.microsoft.azure.storage.table._table_batch_operation) Дополнительные сведения об изменении этого параметра. |

## <a name="service-bus-queues"></a>Очереди служебной шины

[Очереди служебной шины](../service-bus-messaging/service-bus-queues-topics-subscriptions.md) предлагают доставку сообщений конкурирующим потребителям FIFO. Как правило сообщения принимаются и обрабатываются получателями во временной последовательности, в котором они были добавлены в очередь. Каждое сообщение является принимается и обрабатывается только одним потребителем сообщений.

Ниже перечислены имена свойств и их описания для создания выходных данных в очереди.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, используемое в запросах для направления выходных данных запроса в эту очередь служебной шины. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. |
| Имя очереди |Имя очереди служебной шины. |
| Имя политики очереди |При создании очереди можно также создать политики общего доступа в очередь **Настройка** вкладки. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики очереди |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель |Применимо только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат |Применимо только для типа JSON. **Строки-разделители** указывает, что выходные данные форматируются по каждый объект JSON, разделенных точкой с новой строки. **Массив** указывает, что выходные данные форматируются как массив объектов JSON. |
| Столбцы свойств | Необязательный элемент. Столбцы, с разделителями запятыми, которые необходимо подключить как пользователя свойства исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции находится в разделе [пользовательские метаданные свойства для выходных данных](#custom-metadata-properties-for-output). |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.

## <a name="service-bus-topics"></a>Разделы служебной шины
Очереди служебной шины предоставляют метод связи один к одному от отправителя получателю. [Разделы служебной шины](https://msdn.microsoft.com/library/azure/hh367516.aspx) предоставляют вид связи один ко многим.

В следующей таблице перечислены имена свойств и их описания для создания выходных данных в разделе.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, используемое в запросах для направления выходных данных запроса в этом разделе служебной шины. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. При создании концентратора событий создается также пространство имен служебной шины. |
| Имя раздела |Разделы являются сущностями обмена сообщениями, как концентраторы событий и очереди. Они предназначены для сбора потоков событий из устройств и служб. При создании раздела, она также предоставил конкретное имя. Сообщения, отправленные в раздел недоступны, пока не будет создана подписка, поэтому убедитесь, имеется один или несколько подписок. |
| Имя политики раздела |При создании раздела можно также создать политики общего доступа по этой теме **Настройка** вкладки. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики раздела |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |Если вы используете формат CSV или JSON, должна быть указана кодировка. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель |Применимо только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Столбцы свойств | Необязательный элемент. Столбцы, с разделителями запятыми, которые необходимо подключить как пользователя свойства исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции находится в разделе [пользовательские метаданные свойства для выходных данных](#custom-metadata-properties-for-output). |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ секции — это уникальное целое значение для каждой секции.

## <a name="azure-cosmos-db"></a>Azure Cosmos DB
[Azure Cosmos DB](https://azure.microsoft.com/services/documentdb/) — это глобально распределенная база данных служба, которая предоставляет функции неограниченного гибкого масштабирования по всему миру, расширенные возможности выполнения запросов и автоматического индексирования с помощью моделей данных без использования схемы. Дополнительные сведения о параметрах сбора данных Cosmos Azure Stream Analytics, см. в разделе [Stream Analytics с Azure Cosmos DB в качестве выходных данных](stream-analytics-documentdb-output.md) статьи.

Выходные данные Azure Cosmos DB из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (21Vianet) и Azure для Германии (T-Systems International).

> [!Note]
> В настоящее время Azure Stream Analytics поддерживает только подключения к Azure Cosmos DB с помощью SQL API.
> Другие API Azure Cosmos DB в данный момент не поддерживаются. Если указать модулю Azure Stream Analytics учетные записи Azure Cosmos DB, созданные при помощи других API, это может привести к неправильному сохранению данных.

В следующей таблице описаны свойства для создания выходных данных Azure Cosmos DB.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных | Псевдоним для ссылки на эти выходные данные в запросе Stream Analytics. |
| Приемник | Azure Cosmos DB. |
| Вариант импорта | Выберите либо **выберите Cosmos DB из своей подписки** или **параметры предоставляют Cosmos DB вручную**.
| Идентификатор учетной записи | Имя или универсальный код ресурса (URI) конечной точки учетной записи Azure Cosmos DB. |
| Ключ учетной записи | Общедоступный ключ доступа к учетной записи Azure Cosmos DB. |
| База данных | Имя базы данных Azure Cosmos DB. |
| Шаблон имен коллекций | Имя коллекции или шаблон для используемых коллекций. <br />Формат имени коллекции можно создать с помощью маркера необязательным {partition}, где разделы начинаются с 0. Два примера:  <br /><ul><li> _MyCollection_: Должна существовать одна коллекция с именем «MyCollection».</li>  <li> _MyCollection {partition}_ : На основании столбца секционирования.</li></ul> Должен существовать секционирования коллекциях столбцов: «MyCollection0,» «MyCollection1», «MyCollection2», и т. д. |
| Ключ секции | Необязательный элемент. Это необходимо только в том случае, если в шаблоне имени коллекции используется маркер {partition}.<br /> Ключ секции — это имя поля в выходных событиях, который используется для указания ключа выходных данных секционирования в коллекциях.<br /> Для выходных данных одной коллекции можно использовать любой столбец произвольных выходных данных. Например, PartitionId. |
| Идентификатор документа |Необязательный элемент. Имя поля в выходных событиях, который используется для указания первичного ключа, на какие insert или update основаны операции.

## <a name="azure-functions"></a>Функции Azure
Функции Azure — это бессерверная служба вычислений, можно использовать для выполнения кода по требованию без необходимости явно подготавливать или администрировать инфраструктуру. Они позволяют реализовать код, активируемый по событиям, возникающим в службах Azure или партнеру. Эта возможность функций Azure реагировать на триггеры упрощает вывод данных для Azure Stream Analytics. Этот выходной адаптер позволяет пользователям подключать Stream Analytics для функций Azure и выполнить сценарий или часть кода в ответ на ряд событий.

Выходные данные Функций Azure из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (21Vianet) и Azure для Германии (T-Systems International).

Azure Stream Analytics вызывает Функции Azure через триггеры HTTP. Выходной адаптер функций Azure входит в состав следующие свойства:

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Приложение-функция |Имя приложения функций Azure. |
| Функция |Имя функции в приложения функций Azure. |
| Ключ |Если вы хотите использовать функцию из другой подписки Azure, это можно сделать, предоставив ключ для доступа к функции. |
| Максимальный размер пакета |Свойство, которое позволяет задать максимальный размер каждого пакета выходных данных, передаваемый функции Azure. Объем входных данных задается в байтах. По умолчанию это значение равно 262 144 байт (256 КБ). |
| Максимальное количество пакетов  |Свойство, позволяет указать максимальное число событий в каждом пакете, который отправляется в функциях Azure. По умолчанию используется значение 100. |

Когда Azure Stream Analytics получает 413 («http слишком большая сущность запроса) исключения из функции Azure, уменьшается размер пакетов, отправляемых в функциях Azure. В коде функции Azure это исключение позволяет убедится, что Azure Stream Analytics не отправляет пакеты слишком большого размера. Кроме того убедитесь, что в функции значениями числа и размера пакета соответствуют значениям, введенным на портале Stream Analytics.

Кроме того, в ситуации, когда не происходит в рамках интервала целевое событие, выходные данные создаются. В результате **computeResult** функция не вызывается. Такое поведение согласуется со встроенными оконными агрегатными функциями.

## <a name="custom-metadata-properties-for-output"></a>Свойства пользовательских метаданных для выходных данных 

Запрос столбцов можно присоединить в качестве свойств пользователя для исходящих сообщений. Эти столбцы не нужно вдаваться в полезных данных. Свойства, имеющиеся в виде словаря на выходное сообщение. *Ключ* является имя столбца и *значение* значение столбца в словаре свойств. За исключением записи и массив поддерживаются все типы данных Stream Analytics.  

Поддерживаемые источники выходных данных: 
* Очередь служебной шины 
* Раздел служебной шины 
* концентратор событий; 

В следующем примере мы добавляем два поля `DeviceId` и `DeviceStatus` к метаданным. 
* Запрос: `select *, DeviceId, DeviceStatus from iotHubInput`
* Конфигурация выходных данных: `DeviceId,DeviceStatus`

![Столбцы свойств](./media/stream-analytics-define-outputs/10-stream-analytics-property-columns.png)

На следующем снимке экрана показан выходные свойства сообщения, проверен в EventHub через [Service Bus Explorer](https://github.com/paolosalvatori/ServiceBusExplorer).

![Пользовательские свойства события](./media/stream-analytics-define-outputs/09-stream-analytics-custom-properties.png)

## <a name="partitioning"></a>Секционирование

В следующей таблице указаны поддержка секционирования и количество записей выходных данных для каждого типа данных:

| Тип выходных данных | Поддержка секционирования | Ключ секции  | Количество записей выходных данных |
| --- | --- | --- | --- |
| Хранилище озера данных Azure | Yes | Используйте {date} и {time} маркеров в шаблон префикса пути. Выберите формат даты, например гггг/мм/дд, дд/мм/гггг и мм-дд-гггг. ЧЧ используется формат. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Базы данных SQL Azure | Yes | На основании предложение PARTITION BY в запросе. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). Чтобы узнать больше о достижении лучше написать пропускной способности при загрузке данных в базу данных SQL Azure, см. в разделе [выходные данные Azure Stream Analytics к базе данных SQL Azure](stream-analytics-sql-output-perf.md). |
| Хранилище больших двоичных объектов Azure | Yes | Используйте {date} и {time} токенов из полей событий в шаблоне пути. Выберите формат даты, например гггг/мм/дд, дд/мм/гггг и мм-дд-гггг. ЧЧ используется формат. Выходные данные большого двоичного объекта можно секционировать с помощью одного атрибута настраиваемого события {fieldname} или {datetime:\<specifier>}. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Центры событий Azure | Yes | Yes | Изменяется в зависимости от выравнивания секций.<br /> Когда ключ раздела для выходных данных концентратора событий одинаково выравнивается вышестоящего действия (предыдущая) запроса, число модулей записи совпадает со значением количество секций в выходные данные концентраторов событий. Каждый модуль записи использует [EventHubSender класс](/dotnet/api/microsoft.servicebus.messaging.eventhubsender?view=azure-dotnet) для отправки событий в определенной секции. <br /> Если ключ раздела для выходных данных концентратора событий не согласована с вышестоящим (предыдущая) этап запроса, число модулей записи совпадает со значением число секций, в предыдущем шаге. Каждый модуль записи использует [класс SendBatchAsync](/dotnet/api/microsoft.servicebus.messaging.eventhubclient.sendasync?view=azure-dotnet) в **EventHubClient** для отправки событий для всех секций выходных данных. |
| Power BI | Нет | Нет | Не применяется |
| табличное хранилище Azure; | Yes | Любой выходной столбец.  | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Раздел служебной шины Azure | Yes | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ секции — это уникальное целое значение для каждой секции.| Совпадает с количеством секций в разделе выходных данных.  |
| Очередь служебной шины Azure | Yes | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ секции — это уникальное целое значение для каждой секции.| Совпадает с количеством секций в очереди выходных данных. |
| Azure Cosmos DB | Yes | Используется маркер {partition} в шаблон имени коллекции. Значение {partition} основано на предложении PARTITION BY в запросе. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Функции Azure | Нет | Нет | Не применяется |

Если выходной адаптер не секционирован, нехватка данных в одном входном разделе приведет к задержке вплоть до установленного допустимого интервала. В таком случае выходные данные объединяются в единый модуль записи, что может привести к узких мест в конвейере. Дополнительные сведения о политике позднее прибытия, см. в разделе [рассмотрение порядка событий Azure Stream Analytics](stream-analytics-out-of-order-and-late-events.md).

## <a name="output-batch-size"></a>Размер выходного пакета
Azure Stream Analytics использует переменного размера пакетов для обработки событий и записи выходных данных. Обычно модуль Stream Analytics не написать одно сообщение за раз и использует пакеты для повышения эффективности. При высокой скорости входящих и исходящих событий, Stream Analytics использует большие пакеты. При низкой интенсивности исходящего трафика используются пакеты меньшего размера, чтобы обеспечить низкую задержку.

В следующей таблице описаны некоторые аспекты, для пакетной обработки выходных данных:

| Тип выходных данных | Максимальный размер сообщения | Оптимизация размера пакета |
| :--- | :--- | :--- |
| Хранилище озера данных Azure | См. в разделе [ограничения хранилища Озера данных](../azure-subscription-service-limits.md#data-lake-store-limits). | Используйте до 4 МБ в операции записи. |
| Базы данных SQL Azure | 10 000 максимальное число строк в одной инструкции bulk insert.<br />100 минимальному числу строк для одной инструкции bulk insert. <br />См. в разделе [Azure SQL ограничивает](../sql-database/sql-database-resource-limits.md). |  Каждый пакет изначально является неполным, вставленные с помощью максимальный размер пакета. Вы можете разделить пакетной службы в два раза (пока не дойдете до размера минимальное пакета) в зависимости от ошибки, допускающие повторение, от SQL. |
| Хранилище больших двоичных объектов Azure | См. в разделе [ограничения службы хранилища Azure](../azure-subscription-service-limits.md#storage-limits). | Размер блока максимальное BLOB-объектов — 4 МБ.<br />Число bock максимальное BLOB-объектов — 50 000. |
| Центры событий Azure  | 256 КБ на одно сообщение. <br />См. в разделе [ограничивает концентраторов событий](../event-hubs/event-hubs-quotas.md). |  При секционировании ввода вывода не выровнены, каждое событие упаковывается по отдельности в **EventData** и отправляются в пакете до максимально допустимый размер сообщения (1 МБ для SKU "премиум"). <br /><br />  Если секционирование ввода вывода имеет значение aligned, несколько событий, упакованы в один **EventData** экземпляра, вплоть до максимально допустимый размер сообщения и отправляются.  |
| Power BI | См. в разделе [ограничивает Power BI Rest API](https://msdn.microsoft.com/library/dn950053.aspx). |
| табличное хранилище Azure; | См. в разделе [ограничения службы хранилища Azure](../azure-subscription-service-limits.md#storage-limits). | Значение по умолчанию — 100 сущностей в одной транзакции. Его можно настроить в меньшее значение, при необходимости. |
| Очередь служебной шины Azure   | 256 КБ на одно сообщение.<br /> См. в разделе [служебная шина ограничивает](../service-bus-messaging/service-bus-quotas.md). | Используйте одно событие одного сообщения. |
| Раздел служебной шины Azure | 256 КБ на одно сообщение.<br /> См. в разделе [служебная шина ограничивает](../service-bus-messaging/service-bus-quotas.md). | Используйте одно событие одного сообщения. |
| Azure Cosmos DB   | См. в разделе [ограничивает Azure Cosmos DB](../azure-subscription-service-limits.md#azure-cosmos-db-limits). | Размер пакета и записи, частота динамически корректируются на основе Azure Cosmos DB ответов. <br /> Отсутствуют предопределенные ограничения из Stream Analytics. |
| Функции Azure   | | Размер пакета по умолчанию — 262 144 байт (256 КБ). <br /> Количество событий в пакете по умолчанию — 100. <br /> Размер пакета можно настроить, его можно увеличить или уменьшить в [параметрах вывода](#azure-functions) Stream Analytics.

## <a name="next-steps"></a>Дальнейшие действия
> [!div class="nextstepaction"]
> 
> [Краткое руководство по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: https://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: https://go.microsoft.com/fwlink/?LinkId=517301
