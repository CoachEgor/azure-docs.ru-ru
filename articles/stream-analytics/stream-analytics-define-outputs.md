---
title: Описание выходных данных из Azure Stream Analytics
description: В этой статье описываются параметры вывода данных для анализа результатов, доступные в Azure Stream Analytics, включая Power BI.
author: mamccrea
ms.author: mamccrea
ms.reviewer: mamccrea
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 02/14/2020
ms.openlocfilehash: e0b4bcac8494f136dde21b03422e12b72cecb8f3
ms.sourcegitcommit: 07d62796de0d1f9c0fa14bfcc425f852fdb08fb1
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/27/2020
ms.locfileid: "80366445"
---
# <a name="understand-outputs-from-azure-stream-analytics"></a>Описание выходных данных из Azure Stream Analytics

В этой статье описаны типы выходных данных, доступных для задания Azure Stream Analytics. Выходные данные дают возможность сохранить и хранить результаты задания Stream Analytics. Используя выходные данные, можно продолжить бизнес-аналитику и хранение данных.

При проектировании запроса Stream Analytics, обратитесь к названию вывода, используя [оговорку INTO.](https://docs.microsoft.com/stream-analytics-query/into-azure-stream-analytics) Можно использовать один выход на задание или несколько выходных на работу потоковой передачи (если они вам нужны), предоставляя несколько положений INTO в запросе.

Для создания, отработки и тестирования рабочих результатов Stream Analytics можно использовать [портал Azure,](stream-analytics-quick-create-portal.md#configure-job-output) [Azure PowerShell,](stream-analytics-quick-create-powershell.md#configure-output-to-the-job) [.NET API,](https://docs.microsoft.com/dotnet/api/microsoft.azure.management.streamanalytics.ioutputsoperations?view=azure-dotnet) [REST API](https://docs.microsoft.com/rest/api/streamanalytics/stream-analytics-output)и [Visual Studio.](stream-analytics-quick-create-vs.md)

Некоторые типы выводов поддерживают [раздел.](#partitioning) [Размеры выходных пакетов](#output-batch-size) варьируются для оптимизации пропускной всей входной.


## <a name="azure-data-lake-storage-gen-1"></a>Лазурное хранилище озер данных Gen 1

Stream Analytics поддерживает [Azure Data Lake Storage Gen 1.](../data-lake-store/data-lake-store-overview.md) Azure Data Lake Storage — это репозиторий гипермасштабного анализа больших данных. Хранение data Lake Можно использовать для хранения данных любого размера, типа и скорости приема данных для оперативной и исследовательской аналитики. Stream Analytics должен быть уполномочен для доступа к хранилищу озера данных.

Выход Azure Data Lake Storage от Stream Analytics в настоящее время недоступен в регионах Azure China 21Vianet и Azure Germany (T-Systems International).

В следующей таблице перечислены имена свойств и их описания для настройки вывода Data Lake Storage Gen 1.   

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Дружественное имя, используемое в запросах для направления вывода запроса в Data Lake Store. |
| Подписка | Подписка, содержащая учетную запись хранения озер данных Azure. |
| Имя учетной записи | Название учетной записи Data Lake Store, куда отправляется выход. Вам предоставляется список учетных записей Data Lake Store, которые доступны в вашей подписке. |
| Шаблон префикса пути | Путь файла, используемый для записи файлов в указанной учетной записи Data Lake Store. Можно указать один или несколько экземпляров переменных «дата» и «время»:<br /><ul><li>Пример 1. folder1/logs/{дата}/{время}</li><li>Пример 2. folder1/logs/{дата}</li></ul><br />Отметка времени созданной структуры папок следует за UTC, а не по местному времени.<br /><br />Если шаблон маршрута файла не содержит задней слэйс (/), последний шаблон в пути файла рассматривается как префикс имени файла. <br /><br />Новые файлы создаются в следующих ситуациях:<ul><li>изменения в схеме выходных данных;</li><li>Внешний или внутренний перезапуск задания</li></ul> |
| Формат даты | Необязательный параметр. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
|Формат времени | Необязательный параметр. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro.|
| Кодирование | Если вы используете формат CSV или JSON, необходимо указать кодирование. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8.|
| Разделитель | Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.|
| Формат | Применяется только для сериализации JSON. **Разделенная строка** определяет, что вывод отформатирован путем разделения каждого объекта JSON новой строкой. При выборе **раздельной линии**JSON считывается по одному объекту за раз. Все содержимое само по себе не будет действительным JSON.  **Array** указывает, что вывод отформатирован как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем, предпочтительнее использовать отделяемый линией JSON, поскольку он не требует специальной обработки, пока выводной файл все еще пишется.|
| Режим проверки подлинности | Вы можете авторизовать доступ к учетной записи Хранилища озер данных с помощью [управляемой идентификации](stream-analytics-managed-identities-adls.md) или маркера пользователя. После предоставления доступа можно отозвать доступ, изменив пароль учетной записи пользователя, удаляя выход хранилища данных для этой работы или удаляя задание Stream Analytics. |

## <a name="sql-database"></a>База данных SQL

База [данных Azure S'L](https://azure.microsoft.com/services/sql-database/) может использоваться в качестве вывода данных, которые являются реляционными по своему характеру, или для приложений, зависят от содержимого, размещаемого в реляционной базе данных. Задания Stream Analytics записываются в существующую таблицу в базе данных S'L. Схема таблицы должна точно соответствовать полям и их типам в выходе вашей работы. Вы также можете указать [хранилище данных Azure S'L](https://azure.microsoft.com/documentation/services/sql-data-warehouse/) в качестве вывода через выходную опцию базы данных S'L. Чтобы узнать о способах улучшения [Stream Analytics with Azure SQL Database as output](stream-analytics-sql-output-perf.md) пропускной записи, см.

В качестве вывода можно также использовать [управляемую базу данных Azure S'L.](https://docs.microsoft.com/azure/sql-database/sql-database-managed-instance) Необходимо [настроить общедоступную точку в управляемой базе данных Azure S'L,](https://docs.microsoft.com/azure/sql-database/sql-database-managed-instance-public-endpoint-configure) а затем вручную настроить следующие параметры в Azure Stream Analytics. Виртуальная машина Azure под управлением сервера S'L Server с прикрепленной базой данных также поддерживается вручную настройкой настроек ниже.

В следующей таблице перечислены имена свойств и их описание для создания вывода базы данных S'L.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую базу данных. |
| База данных | Название базы данных, куда вы отправляете выход. |
| Имя сервера | Имя сервера Базы данных SQL. Для управляемой базы данных Azure S'L Требуется указать порт 3342. Например, *sampleserver.public.database.windows.net,3342* |
| Имя пользователя | Имя пользователя, накоторое имя пользователя, которое имеет доступ к базе данных. Stream Analytics поддерживает только проверку подлинности S'L. |
| Пароль | Пароль для подключения к базе данных. |
| Таблица | Имя таблицы, в которую записываются выходные данные. Название таблицы является чувствительным к делу. Схема этой таблицы должна точно соответствовать количеству полей и их типам, генерируемым результатом работы. |
|Наследование схемы секционирования| Вариант для наследования схемы раздела вашего предыдущего шага запроса, чтобы включить полностью параллельную топологию с несколькими писателями к столу. Дополнительные сведения см. в статье [Вывод данных Azure Stream Analytics в базу данных SQL Azure](stream-analytics-sql-output-perf.md).|
|Максимальное количество пакетов| Рекомендуемый верхний предел на количество записей, отправленных с каждой транзакцией навалом вставки.|

## <a name="blob-storage-and-azure-data-lake-gen2"></a>Хранение blob и Azure Data Lake Gen2

Data Lake Storage 2-го поколения использует службу хранилища Azure в качестве основы для создания корпоративных хранилищ данных в Azure. Разработанный с самого начала для обслуживания нескольких петабайт информации, сохраняя при этом сотни гигабит пропускной связи, Data Lake Storage Gen2 позволяет легко управлять огромными объемами данных. Фундаментальной частью data Lake Storage Gen2 является добавление иерархического пространства имен в хранилище Blob.

Хранение Azure Blob предлагает экономное и масштабируемое решение для хранения больших объемов неструктурированных данных в облаке. Для введения на blob хранения и его использования, см [Загрузка, загрузка и список капли с порталом Azure](../storage/blobs/storage-quickstart-blobs-portal.md).

В следующей таблице перечислены имена свойств и их описания для создания выхода blob или ADLS Gen2.

| Имя свойства       | Описание                                                                      |
| ------------------- | ---------------------------------------------------------------------------------|
| Псевдоним выходных данных        | Понятное имя, которое используется в запросах для направления выходных данных запроса в хранилище BLOB-объектов. |
| Учетная запись хранения     | Имя учетной записи хранилища, куда вы отправляете выход.               |
| Ключ учетной записи хранения | Секретный ключ, связанный с учетной записью хранения.                              |
| Контейнер для хранения   | Логическая группировка для капли, хранящиеся в службе Azure Blob. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер. |
| Шаблон пути | Необязательный параметр. Шаблон траектории файла, используемый для записи капли в указанном контейнере. <br /><br /> В шаблоне пути можно использовать один или несколько экземпляров переменных даты и времени, чтобы указать частоту записи капли: <br /> {date}, {time} <br /><br />Пользовательское секционирование большого двоичного объекта можно использовать для предварительной версии, указав одно пользовательское имя поля {field} из данных события. Имя поля может содержать буквы, цифры, дефисы и символы подчеркивания. Существуют следующие ограничения для пользовательских полей. <ul><li>Имена полей не чувствительны к случаям. Например, служба не может различать столбец "ID" и столбец "id".</li><li>Вложенные поля не допускаются. Вместо этого используйте псевдоним в запросе задания, чтобы «сгладить» поле.</li><li>Выражения не могут использоваться в качестве имени поля.</li></ul> <br />Этот компонент допускается использовать в пути конфигурации описателей пользовательских форматов даты и времени. Пользовательские форматы даты и времени необходимо указывать по одному в виде ключевого слова {datetime:\<описатель>}. Допустимые входы для \<> осколков являются yyyy, MM, M, dd, d, HH, H, mm, m, ss или s. Ключевое слово\<«дата: specifier>» может быть использовано несколько раз на пути для формирования пользовательских конфигураций даты/времени. <br /><br />Примеры: <ul><li>Пример 1: cluster1/logs/{date}/{time}</li><li>Пример 2: cluster1/logs/{date}</li><li>Пример 3: cluster1/{client_id}/{date}/{time}</li><li>Пример 4: cluster1/ 'datetime:ss/'myField' где находится запрос: SELECT data.myField AS myField FROM Input;</li><li>Пример 5: cluster1/year={datetime:yyyy}/month={datetime:MM}/day={datetime:dd}</ul><br />Отметка времени созданной структуры папок следует за UTC, а не по местному времени.<br /><br />Именование файлов использует следующую конвенцию: <br /><br />{Шаблон префикса пути}/schemaHashcode_Guid_Number.extension<br /><br />Выходные файлы примера:<ul><li>Myoutput/20170901/00/45434_gguid_1.csv</li>  <li>Myoutput/20170901/01/45434_gguid_1.csv</li></ul> <br />Для получения дополнительной [информации](stream-analytics-custom-path-patterns-blob-storage-output.md)об этой функции см. |
| Формат даты | Необязательный параметр. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
| Формат времени | Необязательный параметр. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддержка поддерживается JSON, CSV, Avro и Паркет. |
|Минимальные ряды (только паркет)|Количество минимальных строк на одну партию. Для паркета каждая партия создаст новый файл. Текущее значение по умолчанию составляет 2000 строк, а допустимое максимум 10 000 строк.|
|Максимальное время (только паркет)|Максимальное время ожидания на каждую партию. По истечением этого времени пакет будет записан на выходные, даже если требование о минимальных строках не будет выполнено. Текущее значение по умолчанию составляет 1 минуту, а допустимое максимум 2 часа. Если выход капли имеет частоту шаблона действия, время ожидания не может быть выше диапазона времени раздела.|
| Кодирование    | Если вы используете формат CSV или JSON, необходимо указать кодирование. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель   | Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат      | Применяется только для сериализации JSON. **Разделенная строка** определяет, что вывод отформатирован путем разделения каждого объекта JSON новой строкой. При выборе **раздельной линии**JSON считывается по одному объекту за раз. Все содержимое само по себе не будет действительным JSON. **Array** указывает, что вывод отформатирован как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем, предпочтительнее использовать отделяемый линией JSON, поскольку он не требует специальной обработки, пока выводной файл все еще пишется. |

При использовании хранилища Blob в качестве вывода в blob создается новый файл в следующих случаях:

* Если размер файла превышает максимально допустимое количество блоков (в настоящее время 50 000). Вы можете достичь максимально допустимого количества блоков, не достигнув максимально допустимого размера капли. Например, при высокой скорости вывода данных в блоке будет большее число байтов, значит, и размер файла будет большим. В случае низкой скорости вывода данных в каждом блоке будет меньше данных, а значит, и размер файла будет меньшим.
* При изменении схемы вывода и формата вывода требуется фиксированная схема (CSV и Avro).
* При перезапуске задания извне пользователем, который останавливает и затем запускает его, или изнутри для обслуживания системы или восстановления после сбоя.
* Если запрос полностью разделен и для каждого раздела вывода создается новый файл.
* Если пользователь удаляет файл или контейнер учетной записи хранения.
* Если выход времени разделен с помощью шаблона префикса пути, и новый blob используется, когда запрос перемещается на следующий час.
* Если выход разделен пользовательским полем, и создается новый каброй на ключ раздела, если он не существует.
* Если выход разделен пользовательским полем, где ключ раздела превышает 8000, и создается новый каплю на ключ раздела.

## <a name="event-hubs"></a>Центры событий

[Центры событий Azure](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может принимать миллионы событий в секунду. Одно из видов использования концентратора событий в качестве вывода — это когда выход задания Stream Analytics становится входной частью другого задания потоковой передачи. Для получения информации о максимальном размере сообщения [output batch size](#output-batch-size) и оптимизации размера пакета см.

Для настройки потоков данных из концентраторов событий в виде вывода требуется несколько параметров.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Дружественное имя, используемое в запросах для направления вывода запроса в этот концентратор событий. |
| Пространство имен концентратора событий | Контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий также создается пространство имен концентратора событий. |
| Имя концентратора событий | Название вывода концентратора событий. |
| Имя политики концентратора событий | Политика общего доступа, которую можно создать на вкладке **настройки** концентратора событий. Каждая общая политика доступа имеет имя, установленные разрешения и ключи доступа. |
| Ключ политики концентратора событий | Общий ключ доступа, используемый для проверки подлинности доступа к пространству имен концентратора событий. |
| Колонка ключа раздела | Необязательный параметр. Столбец, содержащий ключ раздела для вывода концентратора событий. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель | Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат | Применяется только для сериализации JSON. **Разделенная строка** определяет, что вывод отформатирован путем разделения каждого объекта JSON новой строкой. При выборе **раздельной линии**JSON считывается по одному объекту за раз. Все содержимое само по себе не будет действительным JSON. **Array** указывает, что вывод отформатирован как массив объектов JSON.  |
| Столбцы свойств | Необязательный параметр. Столбцы, разделенные запятой, которые должны быть прикреплены в качестве пользовательских свойств исходящего сообщения вместо полезной нагрузки. Более подробная информация об этой функции находится в разделе [Пользовательские свойства метаданных для вывода.](#custom-metadata-properties-for-output) |

## <a name="power-bi"></a>Power BI

Можно использовать [Power BI](https://powerbi.microsoft.com/) в качестве вывода для работы Stream Analytics, чтобы обеспечить богатый опыт визуализации результатов анализа. Эту возможность можно использовать для работы панелей мониторинга, генерации отчетов и отчетности с метрической информацией.

Выход BI от Stream Analytics в настоящее время недоступен в регионах Azure China 21Vianet и Azure Germany (T-Systems International).

В следующей таблице перечислены имена свойств и их описания для настройки вывода Power BI.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Предоставьте дружественное имя, используемое в запросах для направления вывода запроса на этот выход Power BI. |
| Рабочая область группы |Для обмена данными с другими пользователями Power BI можно выбрать группы внутри учетной записи Power BI или выбрать **My Workspace,** если вы не хотите писать группе. Для обновления существующей группы требуется повторно выполнить проверку подлинности в службе Power BI. |
| Имя набора данных |Укажите имя набора данных, которое требуется для использования выходом Power BI. |
| Имя таблицы |Имя таблицы в наборе выходных данных Power BI. Сейчас для вывода выходных данных из заданий Stream Analytics в Power BI можно использовать только одну таблицу в наборе данных. |
| Авторизация подключения | Для настройки настроек вывода необходимо разрешить Power BI. После предоставления этого вывода доступ к панели мониторинга Power BI можно отозвать доступ, изменив пароль учетной записи пользователя, удаляя выход задания или удаляя задание Stream Analytics. | 

Для получения инструкций по настройке выходной и приборной панели [Power](stream-analytics-power-bi-dashboard.md) BI см.

> [!NOTE]
> Не создавайте четко набор данных и таблицу в панели мониторинга Power BI. Набор данных и таблица автоматически заполняются при запуске задания и начале задания накачки выходных данных в Power BI. Если запрос задания не генерирует результатов, набор данных и таблица не создаются. Если у Power BI уже есть набор данных и таблица с тем же именем, что и в этом задания Stream Analytics, существующие данные перезаписаны.
>

### <a name="create-a-schema"></a>Создание схемы
Аналитика Потокового потока Azure создает набор данных Power BI и схему таблицы для пользователя, если они еще не существуют. Во всех остальных случаях таблица обновляется с использованием новых значений. В настоящее время в наборе данных может существовать только одна таблица. 

Power BI использует политику удержания первого, первого выхода (FIFO). Данные будут собираться в таблице до тех пор, пока они не уйдут в 200 000 строк.

### <a name="convert-a-data-type-from-stream-analytics-to-power-bi"></a>Преобразование типа данных из Stream Analytics в Power BI
Azure Stream Analytics обновляет модель данных динамически во время выполнения, если меняется схема вывода. Изменяются имена и типы столбцов, а добавление или удаление столбцов отслеживается.

В этой таблице рассматриваются преобразования типа данных из [типов данных Stream Analytics](https://docs.microsoft.com/stream-analytics-query/data-types-azure-stream-analytics) в типы Power BI Entity Data Model [(EDM),](https://docs.microsoft.com/dotnet/framework/data/adonet/entity-data-model)если нет набора данных и таблицы Power BI.

Из Stream Analytics | В Power BI
-----|-----
BIGINT | Int64
nvarchar(max) | Строка
DATETIME | Datetime
FLOAT | Double
Record array | Тип строки, постоянное значение "IRecord" или "IArray"

### <a name="update-the-schema"></a>Обновление схемы
Stream Analytics определяет схему модели данных на основе первого набора событий в выходных данных. Позже, при необходимости, схема модели данных обновляется для размещения входящих событий, которые могут не вписаться в исходную схему.

Избегайте `SELECT *` запроса, чтобы предотвратить динамическое обновление схемы в строках. Помимо потенциальных последствий для производительности, это может привести к неопределенности времени, заработаваемого для получения результатов. Выберите точные поля, которые должны быть показаны на панели мониторинга Power BI. Кроме того, значения данных должны соответствовать выбранному типу данных.


Предыдущий/текущий | Int64 | Строка | Datetime | Double
-----------------|-------|--------|----------|-------
Int64 | Int64 | Строка | Строка | Double
Double | Double | Строка | Строка | Double
Строка | Строка | Строка | Строка | Строка 
Datetime | Строка | Строка |  Datetime | Строка

## <a name="table-storage"></a>Хранилище таблиц

[Табличное хранилище Azure](../storage/common/storage-introduction.md) отличается высокой степенью доступности и масштабируемости, позволяя приложению автоматически осуществлять масштабирование в соответствии с нуждами пользователя. Таблица хранения — это магазин ключей/атрибутов microsoft NoS'L, который можно использовать для структурированных данных с меньшими ограничениями на схему. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения.

В следующей таблице перечислены имена свойств и их описания для создания вывода таблицы.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в табличное хранилище. |
| Учетная запись хранения |Имя учетной записи хранилища, куда вы отправляете выход. |
| Ключ учетной записи хранения |Ключ доступа, связанный с учетной записью хранения. |
| Имя таблицы |Имя таблицы. Таблица создается, если она не существует. |
| Ключ секции |Название выходного столбца, содержащего ключ раздела. Ключ раздела является уникальным идентификатором для раздела в таблице, которая формирует первую часть основного ключа сущности. Это значение строки, размер которого может составить до 1 кБ. |
| Ключ строки. |Имя выходного столбца, содержащего ключ строки. Ключ строки является уникальным идентификатором для сущности в рамках раздела. Он является второй частью первичного ключа сущности. Ключ строки представляет собой значение строки, размер которого может составить до 1 кБ. |
| Размер пакета |Количество записей в пакетной операции. Значения по умолчанию (100) достаточно для большинства заданий. Дополнительную информацию об изменении этой настройки можно узнать на [специальной информации.](https://docs.microsoft.com/java/api/com.microsoft.azure.storage.table._table_batch_operation) |

## <a name="service-bus-queues"></a>Очереди служебной шины

[Очереди сервисных автобусов](../service-bus-messaging/service-bus-queues-topics-subscriptions.md) предлагают доставку сообщений FIFO одному или большему потребителю. Как правило, сообщения принимаются и обрабатываются приемниками в височном порядке, в котором они были добавлены в очередь. Каждое сообщение получается и обрабатывается только одним потребителем сообщений.

На [уровне совместимости 1.2](stream-analytics-compatibility-level.md)Azure Stream Analytics использует протокол обмена сообщениями [Advanced Message Queueing Protocol (АМЗП)](../service-bus-messaging/service-bus-amqp-overview.md) для записи в очереди и темы службы. AMQP позволяет создавать кроссплатформенные гибридные приложения, использующие протокол открытого стандарта.

В следующей таблице перечислены имена свойств и их описания для создания вывода очереди.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Дружественное имя, используемое в запросах для направления вывода запроса в эту очередь Service Bus. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. |
| Имя очереди |Название очереди сервисного автобуса. |
| Имя политики очереди |При создании очереди можно также создать общие политики доступа на вкладке **настройки** очереди. Каждая общая политика доступа имеет имя, установленные разрешения и ключи доступа. |
| Ключ политики очереди |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель |Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат |Применяется только для типа JSON. **Разделенная строка** определяет, что вывод отформатирован путем разделения каждого объекта JSON новой строкой. При выборе **раздельной линии**JSON считывается по одному объекту за раз. Все содержимое само по себе не будет действительным JSON. **Array** указывает, что вывод отформатирован как массив объектов JSON. |
| Столбцы свойств | Необязательный параметр. Столбцы, разделенные запятой, которые должны быть прикреплены в качестве пользовательских свойств исходящего сообщения вместо полезной нагрузки. Более подробная информация об этой функции находится в разделе [Пользовательские свойства метаданных для вывода.](#custom-metadata-properties-for-output) |
| Столбцы системного свойства | Необязательный параметр. Ключевые пары значений System Properties и соответствующие названия столбцов, которые должны быть прикреплены к исходящего сообщению вместо полезной нагрузки. Более подробная информация об этой функции находится в разделе [Свойства системы для выхода очереди и темы обслуживания](#system-properties-for-service-bus-queue-and-topic-outputs)  |

Количество перегородок [основано на Сервисном автобусе SKU и размере.](../service-bus-messaging/service-bus-partitioning.md) Ключ раздела — это уникальное целое значение для каждого раздела.

## <a name="service-bus-topics"></a>Разделы шины обслуживания
Очереди сервисных автобусов обеспечивают один-к-одному метод связи от отправителя к получателю. [Темы сервисного автобуса](https://msdn.microsoft.com/library/azure/hh367516.aspx) обеспечивают один к многим форму общения.

В следующей таблице перечислены имена свойств и их описания для создания вывода темы «Автобус обслуживания».

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Дружественное имя, используемое в запросах для направления вывода запроса на эту тему Service Bus. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. При создании концентратора событий создается также пространство имен служебной шины. |
| Имя раздела |Разделы являются сущностями обмена сообщениями, как концентраторы событий и очереди. Они предназначены для сбора потоков событий с устройств и служб. Когда тема создается, ей также дается определенное имя. Сообщения, отправленные на тему, недоступны, если не будет создана подписка, поэтому убедитесь, что есть одна или несколько подписок по этой теме. |
| Имя политики раздела |При создании темы «Автобус обслуживания» можно также создавать общие политики доступа на вкладке **«Настройка»** темы. Каждая общая политика доступа имеет имя, установленные разрешения и ключи доступа. |
| Ключ политики раздела |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |Если вы используете формат CSV или JSON, необходимо указать кодирование. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель |Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Столбцы свойств | Необязательный параметр. Столбцы, разделенные запятой, которые должны быть прикреплены в качестве пользовательских свойств исходящего сообщения вместо полезной нагрузки. Более подробная информация об этой функции находится в разделе [Пользовательские свойства метаданных для вывода.](#custom-metadata-properties-for-output) |
| Столбцы системного свойства | Необязательный параметр. Ключевые пары значений System Properties и соответствующие названия столбцов, которые должны быть прикреплены к исходящего сообщению вместо полезной нагрузки. Более подробная информация об этой функции находится в разделе [Свойства системы для выхода очереди и темы обслуживания](#system-properties-for-service-bus-queue-and-topic-outputs) |

Количество перегородок [основано на Сервисном автобусе SKU и размере.](../service-bus-messaging/service-bus-partitioning.md) Ключ раздела является уникальным значением для каждого раздела.

## <a name="azure-cosmos-db"></a>Azure Cosmos DB
[Azure Cosmos DB](https://azure.microsoft.com/services/documentdb/) — это глобально распределенная служба баз данных, которая предлагает безграничную эластичную шкалу по всему миру, богатый запрос и автоматическую индексацию по моделям схемно-агностичных данных. Чтобы узнать о вариантах контейнеров Azure Cosmos DB для Stream Analytics, см. [Stream Analytics with Azure Cosmos DB as output](stream-analytics-documentdb-output.md)

Выход Azure Cosmos DB из Stream Analytics в настоящее время недоступен в регионах Azure China 21Vianet и Azure Germany (T-Systems International).

> [!Note]
> В настоящее время Azure Stream Analytics поддерживает подключение к DB Azure Cosmos только с помощью API.1, с помощью API.
> Другие API Azure Cosmos DB в данный момент не поддерживаются. Если указать модулю Azure Stream Analytics учетные записи Azure Cosmos DB, созданные при помощи других API, это может привести к неправильному сохранению данных.

В следующей таблице описаны свойства для создания выходных данных Azure Cosmos DB.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Псевдоним для ссылки на эти выходные данные в запросе Stream Analytics. |
| Приемник | Azure Космос DB. |
| Вариант импорта | Выберите либо **Select Cosmos DB из подписки,** либо **предоставьте настройки Cosmos DB вручную.**
| Идентификатор учетной записи | Имя или универсальный код ресурса (URI) конечной точки учетной записи Azure Cosmos DB. |
| Ключ учетной записи | Общедоступный ключ доступа к учетной записи Azure Cosmos DB. |
| База данных | Имя базы данных Azure Cosmos DB. |
| Имя контейнера | Название контейнера, которое будет использоваться, которое должно существовать в Cosmos DB. Пример  <br /><ul><li> _MyContainer_: Контейнер под названием "MyContainer" должен существовать.</li>|
| Идентификатор документа |Необязательный параметр. Имя поля в событиях вывода, используемое для определения основного ключа, на котором основаны операции вставки или обновления.

## <a name="azure-functions"></a>Проверка
Azure Functions — это бессерверная вычислительная служба, которую можно использовать для запуска кода по требованию без явного предоставления или управления инфраструктурой. Он позволяет реализовать код, вызванный событиями, происходящими в службах Azure или партнерской службы. Эта способность функций Azure реагировать на триггеры делает ее естественной выходной частью для Azure Stream Analytics. Этот адаптер вывода позволяет пользователям подключать Stream Analytics к функциям Azure И запустить сценарий или фрагмент кода в ответ на различные события.

Выход Azure Functions из Stream Analytics в настоящее время недоступен в регионах Azure China 21Vianet и Azure Germany (T-Systems International).

Azure Stream Analytics вызывает Функции Azure через триггеры HTTP. Адаптер Azure Functions доступен со следующими настраиваемыми свойствами:

| Имя свойства | Описание |
| --- | --- |
| Приложение-функция |Название приложения Azure Functions. |
| Компонент |Название функции в приложении Azure Functions. |
| Ключ |Если вы хотите использовать функцию Azure из другой подписки, вы можете сделать это, предоставив ключ для доступа к вашей функции. |
| Максимальный размер пакета |Свойство, которое позволяет установить максимальный размер для каждой выходной партии, отправленной на функцию Azure. Объем входных данных задается в байтах. По умолчанию это значение составляет 262 144 байта (256 кБ). |
| Максимальное количество пакетов  |Свойство, которое позволяет указать максимальное количество событий в каждой партии, отправленной в функции Azure. По умолчанию используется значение 100. |

Azure Stream Analytics ожидает статус ам.с. HTTP 200 от приложения Функции для пакетов, которые были успешно обработаны.

Когда Azure Stream Analytics получает исключение 413 ("http Request entity Too Large") из функции Azure, это уменьшает размер пакетов, которые он отправляет функциям Azure. В коде функции Azure это исключение позволяет убедится, что Azure Stream Analytics не отправляет пакеты слишком большого размера. Кроме того, убедитесь, что максимальное количество пакетов и значения размера, используемые в функции, соответствуют значениям, введенным в портале Stream Analytics.

> [!NOTE]
> Во время тестового соединения Stream Analytics отправляет пустую партию в функции Azure для проверки наличия соединения между двумя работами. Убедитесь, что приложение Функции обрабатывает пустые пакетные запросы, чтобы убедиться, что тестподключение проходит.

Кроме того, в ситуации, когда нет посадки события в временное окно, не генерируется выход. В результате функция **computeResult** не называется. Такое поведение согласуется со встроенными оконными агрегатными функциями.

## <a name="custom-metadata-properties-for-output"></a>Пользовательские свойства метаданных для вывода 

К исходящие сообщения можно прикрепить столбцы запросов в качестве свойств пользователей. Эти столбцы не входят в полезную нагрузку. Свойства присутствуют в виде словаря на выходе сообщения. *Ключ* — это имя столбца, а *значение* — значение столбца в словаре свойств. Все типы данных Stream Analytics поддерживаются, за исключением Record и Array.  

Поддерживаемые выходы: 
* Очередь служебной шины 
* Сервис Автобус тема 
* концентратор событий; 

В следующем примере мы добавляем два поля `DeviceId` и `DeviceStatus` метаданные. 
* Запрос: `select *, DeviceId, DeviceStatus from iotHubInput`
* Конфигурация вывода:`DeviceId,DeviceStatus`

![Столбцы свойств](./media/stream-analytics-define-outputs/10-stream-analytics-property-columns.png)

На следующем скриншоте показаны свойства выходных сообщений, проверенные в EventHub через [Service Bus Explorer.](https://github.com/paolosalvatori/ServiceBusExplorer)

![Пользовательские свойства событий](./media/stream-analytics-define-outputs/09-stream-analytics-custom-properties.png)

## <a name="system-properties-for-service-bus-queue-and-topic-outputs"></a>Свойства системы для выходов очереди и темы обслуживания автобусов 
Вы можете прикрепить столбцы запросов в качестве [системных свойств](https://docs.microsoft.com/dotnet/api/microsoft.servicebus.messaging.brokeredmessage?view=azure-dotnet#properties) к исходящего сообщений автобуса обслуживания Очередь или Тема. Эти столбцы не входят в полезную нагрузку, вместо этого соответствующее [свойство системы](https://docs.microsoft.com/dotnet/api/microsoft.servicebus.messaging.brokeredmessage?view=azure-dotnet#properties) BrokeredMessage заполняется значениями столбца запроса.
Эти системные свойства `MessageId, ContentType, Label, PartitionKey, ReplyTo, SessionId, CorrelationId, To, ForcePersistence, TimeToLive, ScheduledEnqueueTimeUtc`поддерживаются - .
Значения строк этих столбцов разбираются как соответствующий тип значения свойства системы, а любые сбои разбора рассматриваются как ошибки данных.
Это поле предоставляется в виде формата объекта JSON. Подробная информация об этом формате такова -
* Окруженный фигурными скобками. {}
* Написано в парах ключей/значений.
* Ключи и значения должны быть строками.
* Ключ — это имя свойства системы, а значение — это имя столбца запроса.
* Ключи и значения разделены толстой кишка.
* Каждая пара ключей/значений разделена запятой.

Это показывает, как использовать это свойство -

* Запрос: `select *, column1, column2 INTO queueOutput FROM iotHubInput`
* Колонки системной собственности:`{ "MessageId": "column1", "PartitionKey": "column2"}`

Это устанавливает `MessageId` на сообщение очереди шины обслуживания с `column1`значениями 's и PartitionKey установлено с `column2`значениями'.

## <a name="partitioning"></a>Секционирование

В следующей таблице указаны поддержка секционирования и количество записей выходных данных для каждого типа данных:

| Тип выходных данных | Поддержка секционирования | Ключ секции  | Количество записей выходных данных |
| --- | --- | --- | --- |
| Хранилище озера данных Azure | Да | Используйте маркеры «дата» и «время» в шаблоне префикса пути. Выберите формат даты, например YYYY/MM/DD, DD/MM/YYYY или MM-DD-YYYY. HH используется для формата времени. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| База данных SQL Azure | Да, нужно включено. | На основе положения PARTITION BY в запросе. | При включении опции «Наследовать раздел» следует раздел входной части для [полностью параллельных запросов.](stream-analytics-scale-jobs.md) Чтобы узнать больше о достижении лучшей производительности записи при загрузке [Azure Stream Analytics output to Azure SQL Database](stream-analytics-sql-output-perf.md)данных в базу данных Azure S'L, см. |
| Хранилище BLOB-объектов Azure | Да | Используйте токены «Дата» и «время» из полей событий в шаблоне пути. Выберите формат даты, например YYYY/MM/DD, DD/MM/YYYY или MM-DD-YYYY. HH используется для формата времени. Выходные данные большого двоичного объекта можно секционировать с помощью одного атрибута настраиваемого события {fieldname} или {datetime:\<specifier>}. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Центры событий Azure | Да | Да | Изменяется в зависимости от выравнивания секций.<br /> Когда ключ раздела для вывода концентратора событий в равной степени выравнивается с восходящим (предыдущим) шагом запроса, количество авторов совпадает с числом разделов в выходе концентратора событий. Каждый автор использует [класс EventHubSender](/dotnet/api/microsoft.servicebus.messaging.eventhubsender?view=azure-dotnet) для отправки событий в определенный раздел. <br /> Когда ключ раздела для вывода концентратора событий не выравнивается с восходящим (предыдущим) шагом запроса, количество авторов совпадает с числом разделов на предыдущем этапе. Каждый автор использует [класс SendBatchAsync](/dotnet/api/microsoft.servicebus.messaging.eventhubclient.sendasync?view=azure-dotnet) в **EventHubClient** для отправки событий во все выходные разделы. |
| Power BI | нет | None | Неприменимо. |
| табличное хранилище Azure; | Да | Любой выходной столбец.  | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Раздел служебной шины Azure | Да | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела является уникальным значением для каждого раздела.| Совпадает с количеством секций в разделе выходных данных.  |
| Очередь служебной шины Azure | Да | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела является уникальным значением для каждого раздела.| Совпадает с количеством секций в очереди выходных данных. |
| Azure Cosmos DB | Да | На основе положения PARTITION BY в запросе. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Проверка | Да | На основе положения PARTITION BY в запросе. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |

Количество выводных писателей также `INTO <partition count>` можно контролировать с помощью (см. [into)](https://docs.microsoft.com/stream-analytics-query/into-azure-stream-analytics#into-shard-count)положение в вашем запросе, которое может быть полезно в достижении желаемой топологии заданий. Если выходной адаптер не секционирован, нехватка данных в одном входном разделе приведет к задержке вплоть до установленного допустимого интервала. В таких случаях вывод сливается с одним автором, что может привести к узким местам в конвейере. Чтобы узнать больше о [Azure Stream Analytics event order considerations](stream-analytics-out-of-order-and-late-events.md)политике позднего прибытия, см.

## <a name="output-batch-size"></a>Размер выходного пакета
Azure Stream Analytics использует пакеты переменного размера для обработки событий и записи в выводы. Обычно движок Stream Analytics не записывает одно сообщение одновременно и использует пакеты для повышения эффективности. При высокой скорости входящих и исходящих событий Stream Analytics используются более крупные партии. При низкой интенсивности исходящего трафика используются пакеты меньшего размера, чтобы обеспечить низкую задержку.

В следующей таблице приводятся некоторые соображения, учитывающие объемы вывода:

| Тип выходных данных | Максимальный размер сообщения | Оптимизация размера пакета |
| :--- | :--- | :--- |
| Хранилище озера данных Azure | [См.](../azure-resource-manager/management/azure-subscription-service-limits.md#data-lake-store-limits) | Используйте до 4 МБ на операцию записи. |
| База данных SQL Azure | Настраиваемый с использованием количества пакетов Max. 10000 максимальных и 100 минимальных строк на одну навалом вставить по умолчанию.<br />Посмотреть [ограничения Azure S'L](../sql-database/sql-database-resource-limits.md). |  Каждая партия изначально навалом вставляется с максимальным количеством партии. Пакет делится пополам (до минимального количества пакетов) на основе ошибок, ошибок, выбытых в S'L. |
| Хранилище BLOB-объектов Azure | Посмотреть [лимиты хранения Azure](../azure-resource-manager/management/azure-subscription-service-limits.md#storage-limits). | Максимальный размер блока капли составляет 4 МБ.<br />Максимальное количество капли бока составляет 50 000. |
| Центры событий Azure  | 256 КБ или 1 МБ за сообщение. <br />Посмотреть [лимиты концентратов событий](../event-hubs/event-hubs-quotas.md). |  Когда раздел ввода/вывода не выровнен, каждое событие упаковывается индивидуально `EventData` и отправляется в пакет до максимального размера сообщения. Это также происходит при использовании [пользовательских свойств метаданных.](#custom-metadata-properties-for-output) <br /><br />  При выравнивании ввода/вывода несколько событий упаковываются `EventData` в один экземпляр, до максимального размера сообщения и отправляются. |
| Power BI | Смотрите [ограничения API Power BI Rest.](https://msdn.microsoft.com/library/dn950053.aspx) |
| табличное хранилище Azure; | Посмотреть [лимиты хранения Azure](../azure-resource-manager/management/azure-subscription-service-limits.md#storage-limits). | По умолчанию 100 объектов за одну транзакцию. Вы можете настроить его на меньшее значение по мере необходимости. |
| Очередь служебной шины Azure   | 256 кБ за сообщение для стандартного уровня, 1 МБ для премиум-уровня.<br /> [См.](../service-bus-messaging/service-bus-quotas.md) | Используйте одно событие в сообщении. |
| Раздел служебной шины Azure | 256 кБ за сообщение для стандартного уровня, 1 МБ для премиум-уровня.<br /> [См.](../service-bus-messaging/service-bus-quotas.md) | Используйте одно событие в сообщении. |
| Azure Cosmos DB   | Смотрите [ограничения DB Azure Cosmos.](../azure-resource-manager/management/azure-subscription-service-limits.md#azure-cosmos-db-limits) | Размер пакета и частота записи корректируются динамически на основе ответов Azure Cosmos DB. <br /> Нет предопределенных ограничений от Stream Analytics. |
| Проверка   | | Размер партии по умолчанию составляет 262 144 байта (256 кБ). <br /> Количество событий по умолчанию на одну партию составляет 100. <br /> Размер пакета можно настроить, его можно увеличить или уменьшить в [параметрах вывода](#azure-functions) Stream Analytics.

## <a name="next-steps"></a>Дальнейшие действия
> [!div class="nextstepaction"]
> 
> [Краткое руководство по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: https://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: https://go.microsoft.com/fwlink/?LinkId=517301
