---
title: Описание выходных данных из Azure Stream Analytics
description: В этой статье описываются параметры вывода данных для анализа результатов, доступные в Azure Stream Analytics, включая Power BI.
services: stream-analytics
author: mamccrea
ms.author: mamccrea
ms.reviewer: jasonh
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 10/8/2019
ms.openlocfilehash: d867cceb3e7261f658e2406617144c9150e36f2a
ms.sourcegitcommit: 42748f80351b336b7a5b6335786096da49febf6a
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/09/2019
ms.locfileid: "72173442"
---
# <a name="understand-outputs-from-azure-stream-analytics"></a>Описание выходных данных из Azure Stream Analytics

В этой статье описываются типы выходов, доступные для задания Azure Stream Analytics. Выходные данные дают возможность сохранить и хранить результаты задания Stream Analytics. Используя выходные данные, можно выполнять дальнейшее бизнес-аналитику и хранение данных.

При проектировании Stream Analytics запроса обратитесь к имени выходных данных с помощью [предложения into](https://docs.microsoft.com/stream-analytics-query/into-azure-stream-analytics). Можно использовать один выход для каждого задания или несколько выходных данных для задания потоковой передачи (если это необходимо), предоставив несколько предложений INTO в запросе.

Для создания, изменения и тестирования выходных данных задания Stream Analytics можно использовать [портал Azure](stream-analytics-quick-create-portal.md#configure-job-output), [Azure PowerShell](stream-analytics-quick-create-powershell.md#configure-output-to-the-job), [API .NET](https://docs.microsoft.com/dotnet/api/microsoft.azure.management.streamanalytics.ioutputsoperations?view=azure-dotnet), [REST API](https://docs.microsoft.com/rest/api/streamanalytics/stream-analytics-output)и [Visual Studio](stream-analytics-quick-create-vs.md).

Некоторые типы выходных данных поддерживают [секционирование](#partitioning). [Размер выходных пакетов](#output-batch-size) различается для оптимизации пропускной способности.


## <a name="azure-data-lake-storage-gen-1"></a>Хранилище Azure Data Lake поколения 1

Stream Analytics поддерживает [Azure Data Lake Storage Gen 1](../data-lake-store/data-lake-store-overview.md). Azure Data Lake Storage является репозиторием масштаба всего предприятия для рабочих нагрузок аналитики больших данных. Data Lake Storage можно использовать для хранения данных любого размера, типа и скорости приема для оперативной и исследовательской аналитики. Stream Analytics должен иметь права на доступ к Data Lake Storage.

Azure Data Lake Storage выходные данные из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (Microsoft) для китайского региона и в Германии (T-Systems International).

В следующей таблице перечислены имена свойств и их описания для настройки выходных данных Data Lake Storage Gen 1.   

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Понятное имя, используемое в запросах для направления выходных данных запроса в Data Lake Store. |
| Subscription | Подписка, содержащая учетную запись Azure Data Lake Storage. |
| Имя учетной записи | Имя учетной записи Data Lake Store, куда отправляются выходные данные. Вы увидите раскрывающийся список Data Lake Store учетных записей, доступных в вашей подписке. |
| Шаблон префикса пути | Путь к файлу, используемый для записи файлов в указанной Data Lake Store учетной записи. Можно указать один или несколько экземпляров переменных {Date} и {Time}:<br /><ul><li>Пример 1. folder1/logs/{дата}/{время}</li><li>Пример 2. folder1/logs/{дата}</li></ul><br />Метка времени для созданной структуры папок соответствует времени в формате UTC и не является местным временем.<br /><br />Если шаблон пути к файлу не содержит косую черту (/), последний шаблон в пути к файлу рассматривается как префикс имени файла. <br /><br />Новые файлы создаются в следующих ситуациях:<ul><li>изменения в схеме выходных данных;</li><li>Внешнее или внутреннее Перезагрузка задания</li></ul> |
| Формат даты | Необязательный элемент. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
|Формат времени | Необязательный элемент. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro.|
| Кодирование | Если вы используете формат CSV или JSON, необходимо указать кодировку. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8.|
| Разделитель | Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.|
| Формат | Применяется только для сериализации JSON. **Строка с разделителем** указывает, что выходные данные форматируются путем разделения каждого объекта JSON на новую строку. **Массив** указывает, что выходные данные форматируются в виде массива объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. Как правило, предпочтительнее использовать JSON, разделенный строками, поскольку он не требует специальной обработки, пока не будет записан выходной файл.|
| Режим проверки подлинности | Вы можете авторизовать доступ к учетной записи Data Lake Storage с помощью [управляемого удостоверения](stream-analytics-managed-identities-adls.md) или токена пользователя. После предоставления доступа можно отозвать доступ, изменив пароль учетной записи пользователя, удалив Data Lake Storage вывода для этого задания или удалив задание Stream Analytics. |

## <a name="sql-database"></a>База данных SQL

Вы можете использовать [базу данных SQL Azure](https://azure.microsoft.com/services/sql-database/) в качестве результата для данных, которые являются реляционными по природе или для приложений, зависящих от содержимого, размещенного в реляционной базе данных. Stream Analyticsные задания записываются в существующую таблицу в базе данных SQL. Схема таблицы должна точно соответствовать полям и их типам в выходных данных задания. Вы также можете указать [хранилище данных SQL Azure](https://azure.microsoft.com/documentation/services/sql-data-warehouse/) в качестве выходных данных с помощью параметра вывода базы данных SQL. Дополнительные сведения о способах повышения пропускной способности записи см. в статье [Stream Analytics с базой данных SQL Azure в качестве выходных](stream-analytics-sql-output-perf.md) данных.

В качестве выходных данных можно также использовать [управляемый экземпляр базы данных SQL Azure](https://docs.microsoft.com/azure/sql-database/sql-database-managed-instance) . Необходимо [настроить общедоступную конечную точку в управляемый экземпляр базы данных SQL Azure](https://docs.microsoft.com/azure/sql-database/sql-database-managed-instance-public-endpoint-configure) а затем вручную настроить следующие параметры в Azure Stream Analytics. Виртуальная машина Azure, на которой работает SQL Server с подключенной базой данных, также поддерживается путем ручной настройки параметров ниже.

В следующей таблице перечислены имена свойств и их описание для создания выходных данных SQL.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую базу данных. |
| База данных | Имя базы данных, в которой вы отправляете выходные данные. |
| Имя сервера | Имя сервера Базы данных SQL. Для Управляемый экземпляр Базы данных SQL Azure необходимо указать порт 3342. Например, *самплесервер. public. Database. Windows. NET, 3342* |
| Имя пользователя | Имя пользователя, имеющее доступ на запись к базе данных. Stream Analytics поддерживает только проверку подлинности SQL. |
| Пароль | Пароль для подключения к базе данных. |
| Таблица | Имя таблицы, в которую записываются выходные данные. В имени таблицы учитывается регистр. Схема этой таблицы должна точно соответствовать количеству полей и их типов, формируемых выходными данными задания. |
|Наследование схемы секционирования| Параметр для наследования схемы секционирования предыдущего шага запроса для включения полной параллельной топологии с несколькими модулями записи в таблицу. Дополнительные сведения см. в статье [Вывод данных Azure Stream Analytics в базу данных SQL Azure](stream-analytics-sql-output-perf.md).|
|Максимальное количество пакетов| Рекомендуемый верхний предел числа записей, отправляемых при каждой транзакции с массовыми вставками.|

## <a name="blob-storage-and-azure-data-lake-gen2"></a>Хранилище BLOB-объектов и Azure Data Lake Gen2

Исходящий трафик в Azure Data Lake Gen2 предоставляется в виде предварительной версии в ограниченных регионах по всему миру. Вы можете запросить доступ к предварительной версии, предоставив дополнительные сведения в нашей [форме запроса](https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR2EUNXd_ZNJCq_eDwZGaF5VURjFLTDRGS0Q4VVZCRFY5MUVaTVJDTkROMi4u).

Хранилище BLOB-объектов Azure предлагает экономичное и масштабируемое решение для хранения больших объемов неструктурированных данных в облаке. Общие сведения о хранилище BLOB-объектов и его использовании см. [в разделе Отправка, скачивание и вывод списка больших двоичных объектов с помощью портал Azure](../storage/blobs/storage-quickstart-blobs-portal.md).

В следующей таблице перечислены имена свойств и их описания для создания выходных данных большого двоичного объекта.

| Имя свойства       | Описание                                                                      |
| ------------------- | ---------------------------------------------------------------------------------|
| Псевдоним выходных данных        | Понятное имя, которое используется в запросах для направления выходных данных запроса в хранилище BLOB-объектов. |
| Учетная запись хранения     | Имя учетной записи хранения, куда отправляются выходные данные.               |
| Ключ учетной записи хранения | Секретный ключ, связанный с учетной записью хранения.                              |
| Контейнер хранилища   | Логическая группировка больших двоичных объектов, хранящихся в службе BLOB-объектов Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер. |
| Шаблон пути | Необязательный элемент. Шаблон пути к файлу, используемый для записи больших двоичных объектов в указанном контейнере. <br /><br /> В шаблоне пути можно выбрать использование одного или нескольких экземпляров переменных даты и времени, чтобы указать частоту, с которой будут записываться большие двоичные объекты: <br /> {date}, {time} <br /><br />Пользовательское секционирование большого двоичного объекта можно использовать для предварительной версии, указав одно пользовательское имя поля {field} из данных события. Имя поля может содержать буквы, цифры, дефисы и символы подчеркивания. Существуют следующие ограничения для пользовательских полей. <ul><li>В именах полей не учитывается регистр. Например, служба не может различить столбец "ID" и столбец "ID".</li><li>Вложенные поля не допускаются. Вместо этого используйте псевдоним в запросе задания для "выравнивания" поля.</li><li>Выражения нельзя использовать в качестве имени поля.</li></ul> <br />Этот компонент допускается использовать в пути конфигурации описателей пользовательских форматов даты и времени. Пользовательские форматы даты и времени необходимо указывать по одному в виде ключевого слова {datetime:\<описатель>}. Допустимые входные данные для \<specifier >: гггг, MM, M, дд, d, чч, H, mm, M, SS или s. Ключевое слово {DateTime: \<specifier >} можно использовать несколько раз в пути для формирования настраиваемых конфигураций даты и времени. <br /><br />Примеры: <ul><li>Пример 1: cluster1/logs/{date}/{time}</li><li>Пример 2: cluster1/logs/{date}</li><li>Пример 3: cluster1/{client_id}/{date}/{time}</li><li>Пример 4: cluster1/{datetime:ss}/{myField}, где запрос имеет следующий вид: SELECT data.myField AS myField FROM Input;</li><li>Пример 5: cluster1/year={datetime:yyyy}/month={datetime:MM}/day={datetime:dd}</ul><br />Метка времени для созданной структуры папок соответствует времени в формате UTC и не является местным временем.<br /><br />При именовании файлов используется следующее соглашение: <br /><br />{Шаблон префикса пути}/schemaHashcode_Guid_Number.extension<br /><br />Выходные файлы примера:<ul><li>Myoutput/20170901/00/45434_gguid_1.csv</li>  <li>Myoutput/20170901/01/45434_gguid_1.csv</li></ul> <br />Дополнительные сведения об этой функции см. в разделе [Azure Stream Analytics настраиваемого секционирования выходных данных большого двоичного объекта](stream-analytics-custom-path-patterns-blob-storage-output.md). |
| Формат даты | Необязательный элемент. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
| Формат времени | Необязательный элемент. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются JSON, CSV, Avro и Parquet. |
|Минимальное число строк (только Parquet)|Количество строк минимума на пакет. Для Parquet каждый пакет создаст новый файл. Текущее значение по умолчанию — 2 000 строк, а допустимое максимальное — 10 000 строк.|
|Максимальное время (только Parquet)|Максимальное время ожидания для одного пакета. По истечении этого времени пакет будет записан в выходные данные, даже если не будет выполнено требование минимальных строк. Текущее значение по умолчанию — 1 минута, а максимально допустимое — 2 часа. Если у выходных данных большого двоичного объекта есть частота шаблона пути, время ожидания не может быть больше, чем диапазон времени раздела.|
| Кодирование    | Если вы используете формат CSV или JSON, необходимо указать кодировку. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель   | Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат      | Применяется только для сериализации JSON. **Строка с разделителем** указывает, что выходные данные форматируются путем разделения каждого объекта JSON на новую строку. **Массив** указывает, что выходные данные форматируются в виде массива объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. Как правило, предпочтительнее использовать JSON, разделенный строками, поскольку он не требует специальной обработки, пока не будет записан выходной файл. |

При использовании хранилища BLOB-объектов в качестве выходных данных в большом двоичном объекте создается новый файл в следующих случаях:

* Если размер файла превышает максимально допустимое количество блоков (в настоящее время 50 000). Возможно, достигнуто максимально допустимое количество блоков без достижения максимально допустимого размера BLOB-объекта. Например, при высокой скорости вывода данных в блоке будет большее число байтов, значит, и размер файла будет большим. В случае низкой скорости вывода данных в каждом блоке будет меньше данных, а значит, и размер файла будет меньшим.
* Если в выходных данных имеется изменение схемы, а формат вывода требует наличия фиксированной схемы (CSV и Avro).
* При перезапуске задания извне пользователем, который останавливает и затем запускает его, или изнутри для обслуживания системы или восстановления после сбоя.
* Значение, если запрос полностью секционирован, и для каждой выходной секции создается новый файл.
* Если пользователь удаляет файл или контейнер учетной записи хранения.
* Если выходные данные секционированы по времени с использованием шаблона префикса пути, а новый большой двоичный объект используется при переходе запроса на следующий час.
* Если выходные данные секционированы по настраиваемому полю, а для ключа секции создается новый BLOB-объект, если он не существует.
* Если выходные данные секционированы по настраиваемому полю, в котором количество элементов ключа секции превышает 8 000, а для ключа секции создается новый большой двоичный объект.

## <a name="event-hubs"></a>Центры событий

[Центры событий Azure](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может принимать миллионы событий в секунду. Использование концентратора событий в качестве выходных данных — когда выходные данные задания Stream Analytics становятся входными данными другого задания потоковой передачи.

Для настройки потоков данных из концентраторов событий в качестве выходных данных необходимо несколько параметров.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Понятное имя, используемое в запросах для направления вывода запроса в этот концентратор событий. |
| Пространство имен концентратора событий | Контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий вы также создали пространство имен концентратора событий. |
| Имя концентратора событий | Имя выходных данных концентратора событий. |
| Имя политики концентратора событий | Политика общего доступа, которую можно создать на вкладке **Настройка** концентратора событий. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики концентратора событий | Ключ общего доступа, используемый для проверки подлинности доступа к пространству имен концентратора событий. |
| Ключевой столбец секции | Необязательный элемент. Столбец, содержащий ключ секции для выходных данных концентратора событий. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель | Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат | Применяется только для сериализации JSON. **Строка с разделителем** указывает, что выходные данные форматируются путем разделения каждого объекта JSON на новую строку. **Массив** указывает, что выходные данные форматируются в виде массива объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. Как правило, предпочтительнее использовать JSON, разделенный строками, поскольку он не требует специальной обработки, пока не будет записан выходной файл. Дополнительные сведения см. в разделе [Размер выходного пакета](#output-batch-size) . |
| Столбцы свойств | Необязательный элемент. Столбцы с разделителями-запятыми, которые необходимо присоединить как свойства пользователя исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства пользовательских метаданных для выходных данных](#custom-metadata-properties-for-output). |

## <a name="power-bi"></a>Power BI

Вы можете использовать [Power BI](https://powerbi.microsoft.com/) в качестве выходных данных для задания Stream Analytics, чтобы обеспечить широкие возможности визуализации результатов анализа. Эту возможность можно использовать для рабочих панелей, создания отчетов и отчетов на основе метрик.

Power BI выходные данные из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (Microsoft) для китайского региона и в Германии (T-Systems International).

В следующей таблице перечислены имена свойств и их описания для настройки выходных данных Power BI.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Укажите понятное имя, используемое в запросах, чтобы направить выходные данные запроса в этот Power BI выход. |
| Рабочая область группы |Чтобы разрешить совместное использование данных с другими пользователями Power BI, можно выбрать группы в учетной записи Power BI или выбрать " **Моя рабочая область** ", если вы не хотите записывать данные в группу. Для обновления существующей группы требуется повторно выполнить проверку подлинности в службе Power BI. |
| Имя набора данных |Укажите имя набора данных, который будет использоваться Power BI выходных данных. |
| Имя таблицы |Имя таблицы в наборе выходных данных Power BI. Сейчас для вывода выходных данных из заданий Stream Analytics в Power BI можно использовать только одну таблицу в наборе данных. |
| Авторизовать подключение | Чтобы настроить параметры вывода, необходимо авторизоваться с Power BI. После предоставления этому выходному доступу к панели мониторинга Power BI можно отозвать доступ, изменив пароль учетной записи пользователя, удалив выходные данные задания или удалив задание Stream Analytics. | 

Пошаговое руководство по настройке Power BI вывода и панели мониторинга см. в руководстве по [Azure Stream Analytics и Power BI](stream-analytics-power-bi-dashboard.md) .

> [!NOTE]
> Не создавайте набор данных и таблицу явным образом на панели мониторинга Power BI. Набор данных и таблица автоматически заполняются при запуске задания, а задание начинает передавать выходные данные в Power BI. Если запрос задания не создает никаких результатов, набор данных и таблица не создаются. Если Power BI уже имел набор данных и таблицу с тем же именем, что и в этом задании Stream Analytics, существующие данные перезаписываются.
>

### <a name="create-a-schema"></a>Создание схемы
Azure Stream Analytics создает Power BI набор данных и схему таблицы для пользователя, если они еще не существуют. Во всех остальных случаях таблица обновляется с использованием новых значений. В настоящее время в наборе данных может существовать только одна таблица. 

Power BI использует политику хранения "первым поступил — первым обслужен" (FIFO). Данные будут собираются в таблицу, пока она не достигнет 200 000 строк.

### <a name="convert-a-data-type-from-stream-analytics-to-power-bi"></a>Преобразование типа данных из Stream Analytics в Power BI
Azure Stream Analytics обновляет модель данных динамически во время выполнения, если меняется схема вывода. Изменяются имена и типы столбцов, а добавление или удаление столбцов отслеживается.

В этой таблице приводятся преобразования типов [данных Stream Analytics](https://docs.microsoft.com/stream-analytics-query/data-types-azure-stream-analytics) в типы Power BI [EDM (EDM)](https://docs.microsoft.com/dotnet/framework/data/adonet/entity-data-model), если набор данных Power BI и таблица не существуют.

Из Stream Analytics | В Power BI
-----|-----
bigint | Int64
nvarchar(max) | Строка,
datetime | DateTime
float | Double
Record array | Строковый тип, постоянное значение "Ирекорд" или "Иаррай"

### <a name="update-the-schema"></a>Обновление схемы
Stream Analytics определяет схему модели данных на основе первого набора событий в выходных данных. Позднее при необходимости схема модели данных обновляется для размещения входящих событий, которые могут не поместиться в исходную схему.

Избегайте запроса `SELECT *`, чтобы предотвратить динамическое обновление схемы по строкам. В дополнение к потенциальным последствиям для производительности это может привести к неопределенности времени, затрачиваемого на результаты. Выберите точные поля, которые должны отображаться на панели мониторинга Power BI. Кроме того, значения данных должны соответствовать выбранному типу данных.


Предыдущий или текущий | Int64 | Строка, | DateTime | Double
-----------------|-------|--------|----------|-------
Int64 | Int64 | Строка, | Строка, | Double
Double | Double | Строка, | Строка, | Double
Строка, | String | String | String | Строка, 
DateTime | Строка, | Строка, |  DateTime | Строка,

## <a name="table-storage"></a>Хранилище таблиц

[Табличное хранилище Azure](../storage/common/storage-introduction.md) отличается высокой степенью доступности и масштабируемости, позволяя приложению автоматически осуществлять масштабирование в соответствии с нуждами пользователя. Хранилище таблиц — это хранилище ключей и атрибутов Майкрософт NoSQL, которое можно использовать для структурированных данных с меньшим ограничением на схему. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения.

В следующей таблице перечислены имена свойств и их описания для создания выходных данных таблицы.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в табличное хранилище. |
| Учетная запись хранения |Имя учетной записи хранения, куда отправляются выходные данные. |
| Ключ учетной записи хранения |Ключ доступа, связанный с учетной записью хранения. |
| Имя таблицы |Это имя таблицы. Таблица создается, если она не существует. |
| Ключ секции |Имя выходного столбца, содержащего ключ секции. Ключ секции — это уникальный идентификатор секции в таблице, которая образует первую часть первичного ключа сущности. Это строковое значение, размер которого может составлять до 1 КБ. |
| Ключ строки. |Имя выходного столбца, содержащего ключ строки. Ключ строки — это уникальный идентификатор сущности в секции. Он является второй частью первичного ключа сущности. Ключ строки — это строковое значение, размер которого может доставлять до 1 КБ. |
| Размер пакета |Количество записей в пакетной операции. Значения по умолчанию (100) достаточно для большинства заданий. Дополнительные сведения об изменении этого параметра см. в [спецификации пакетной операции таблицы](https://docs.microsoft.com/java/api/com.microsoft.azure.storage.table._table_batch_operation) . |

## <a name="service-bus-queues"></a>Очереди служебной шины

[Очереди служебной шины](../service-bus-messaging/service-bus-queues-topics-subscriptions.md) обеспечивают доставку сообщений FIFO одному или нескольким конкурирующим потребителям. Как правило, сообщения принимаются и обрабатываются получателями в том порядке, в котором они были добавлены в очередь. Каждое сообщение получается и обрабатывается только одним потребителем сообщения.

В следующей таблице перечислены имена свойств и их описания для создания выходных данных очереди.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, используемое в запросах для направления выходных данных запроса в эту очередь служебной шины. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. |
| Имя очереди |Имя очереди служебной шины. |
| Имя политики очереди |При создании очереди можно также создать политики общего доступа на вкладке **Настройка** очереди. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики очереди |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель |Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат |Применимо только для типа JSON. **Строка с разделителем** указывает, что выходные данные форматируются путем разделения каждого объекта JSON на новую строку. **Массив** указывает, что выходные данные форматируются в виде массива объектов JSON. |
| Столбцы свойств | Необязательный элемент. Столбцы с разделителями-запятыми, которые необходимо присоединить как свойства пользователя исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства пользовательских метаданных для выходных данных](#custom-metadata-properties-for-output). |
| Столбцы системных свойств | Необязательный элемент. Пары "ключ — значение" системных свойств и соответствующих имен столбцов, которые необходимо присоединить к исходящему сообщению вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства системы для выходных данных очереди и раздела служебной шины](#system-properties-for-service-bus-queue-and-topic-outputs) .  |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.

## <a name="service-bus-topics"></a>Разделы шины обслуживания
Очереди служебной шины предоставляют метод связи «один к одному» от отправителя к получателю. [Разделы служебной шины](https://msdn.microsoft.com/library/azure/hh367516.aspx) предоставляют форму связи «один ко многим».

В следующей таблице перечислены имена свойств и их описания для создания выходных данных раздела служебной шины.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, используемое в запросах для направления выходных данных запроса в этот раздел служебной шины. |
| Пространство имен служебной шины |Контейнер для набора сущностей обмена сообщениями. При создании концентратора событий создается также пространство имен служебной шины. |
| Имя раздела |Разделы являются сущностями обмена сообщениями, как концентраторы событий и очереди. Они предназначены для накопления потоков событий с устройств и служб. При создании раздела ему также присваивается определенное имя. Сообщения, отправленные в раздел, недоступны, если подписка не создана, поэтому убедитесь, что в разделе есть одна или несколько подписок. |
| Имя политики раздела |При создании раздела служебной шины можно также создать политики общего доступа на вкладке **Настройка** раздела. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики раздела |Ключ общего доступа, используемый для аутентификации доступа к пространству имен Служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |Если вы используете формат CSV или JSON, необходимо указать кодировку. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель |Применяется только для сериализации CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Столбцы свойств | Необязательный элемент. Столбцы с разделителями-запятыми, которые необходимо присоединить как свойства пользователя исходящего сообщения вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства пользовательских метаданных для выходных данных](#custom-metadata-properties-for-output). |
| Столбцы системных свойств | Необязательный элемент. Пары "ключ — значение" системных свойств и соответствующих имен столбцов, которые необходимо присоединить к исходящему сообщению вместо полезных данных. Дополнительные сведения об этой функции см. в разделе [Свойства системы для выходных данных очереди и раздела служебной шины](#system-properties-for-service-bus-queue-and-topic-outputs) . |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ секции — это уникальное целочисленное значение для каждой секции.

## <a name="azure-cosmos-db"></a>Azure Cosmos DB
[Azure Cosmos DB](https://azure.microsoft.com/services/documentdb/) — это глобально распределенная служба базы данных, предоставляющая гибкое масштабирование без ограничений по всему миру, расширенный запрос и автоматическое индексирование по моделям данных, не зависящим от схемы. Дополнительные сведения о параметрах контейнера Azure Cosmos DB для Stream Analytics см. в [Stream Analytics с Azure Cosmos DB в качестве выходных данных](stream-analytics-documentdb-output.md) .

Azure Cosmos DB выходные данные из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (Microsoft) для китайского региона и в Германии (T-Systems International).

> [!Note]
> В настоящее время Azure Stream Analytics поддерживает подключение только к Azure Cosmos DB с помощью API SQL.
> Другие API Azure Cosmos DB в данный момент не поддерживаются. Если указать модулю Azure Stream Analytics учетные записи Azure Cosmos DB, созданные при помощи других API, это может привести к неправильному сохранению данных.

В следующей таблице описаны свойства для создания выходных данных Azure Cosmos DB.

| Имя свойства | Описание |
| --- | --- |
| Псевдоним выходных данных | Псевдоним для ссылки на эти выходные данные в запросе Stream Analytics. |
| Приемник | Azure Cosmos DB. |
| Вариант импорта | Выберите либо **выбрать Cosmos DB из подписки** , либо **указать Cosmos DB параметры вручную**.
| Идентификатор учетной записи | Имя или универсальный код ресурса (URI) конечной точки учетной записи Azure Cosmos DB. |
| Ключ учетной записи | Общедоступный ключ доступа к учетной записи Azure Cosmos DB. |
| База данных | Имя базы данных Azure Cosmos DB. |
| Имя контейнера | Имя используемого контейнера, которое должно существовать в Cosmos DB. Пример:  <br /><ul><li> _MyContainer_: Должен существовать контейнер с именем "MyContainer".</li>|
| Идентификатор документа |Необязательный элемент. Имя поля в выходных событиях, которое используется для указания первичного ключа, на котором основаны операции вставки или обновления.

## <a name="azure-functions"></a>Функции Azure
Функции Azure — это бессерверная Служба вычислений, которую можно использовать для выполнения кода по требованию без необходимости явно подготавливать инфраструктуру или управлять ею. Она позволяет реализовать код, который активируется событиями, происходящими в Azure или партнерских службах. Эта способность функций Azure реагировать на триггеры делает его естественным выходом для Azure Stream Analytics. Этот выходной адаптер позволяет пользователям подключаться Stream Analytics к функциям Azure и выполнять сценарий или фрагмент кода в ответ на различные события.

Выходные данные функций Azure из Stream Analytics в настоящее время недоступны в регионах Azure для Китая (Microsoft) для китайского региона и в Германии (T-Systems International).

Azure Stream Analytics вызывает Функции Azure через триггеры HTTP. Выходной адаптер функций Azure доступен со следующими настраиваемыми свойствами:

| Имя свойства | Описание |
| --- | --- |
| Приложение-функция |Имя приложения функций Azure. |
| Функция |Имя функции в приложении "функции Azure". |
| Ключ |Если вы хотите использовать функцию Azure из другой подписки, это можно сделать, предоставив ключ для доступа к функции. |
| Максимальный размер пакета |Свойство, которое позволяет задать максимальный размер для каждого пакета вывода, отправляемого в функцию Azure. Объем входных данных задается в байтах. По умолчанию это значение равно 262 144 байт (256 КБ). |
| Максимальное количество пакетов  |Свойство, которое позволяет указать максимальное число событий в каждом пакете, отправляемых в функции Azure. По умолчанию используется значение 100. |

Когда Azure Stream Analytics получает исключение 413 ("слишком большая сущность запроса HTTP") из функции Azure, оно уменьшает размер пакетов, отправляемых в функции Azure. В коде функции Azure это исключение позволяет убедится, что Azure Stream Analytics не отправляет пакеты слишком большого размера. Кроме того, убедитесь, что максимальное количество пакетов и значения размера, используемые в функции, соответствуют значениям, указанным на портале Stream Analytics.

> [!NOTE]
> Во время тестового подключения Stream Analytics отправляет пустой пакет в функции Azure для проверки, если соединение между ними работает. Убедитесь, что приложение функций обрабатывает пустые пакетные запросы, чтобы убедиться, что проверка пройдена.

Кроме того, в ситуации, когда в временном окне нет целевых событий, выходные данные не создаются. В результате функция **computeResult** не вызывается. Такое поведение согласуется со встроенными оконными агрегатными функциями.

## <a name="custom-metadata-properties-for-output"></a>Свойства пользовательских метаданных для выходных данных 

Столбцы запросов можно прикреплять к исходящим сообщениям как свойства пользователя. Эти столбцы не переходят в полезные данные. Свойства представлены в виде словаря в выходном сообщении. *Key* — это имя столбца, а *значение* — значение столбца в словаре свойств. Поддерживаются все типы данных Stream Analytics, кроме записи и массива.  

Поддерживаемые выходные данные: 
* Очередь служебной шины 
* Раздел служебной шины 
* концентратор событий; 

В следующем примере мы добавим два поля `DeviceId` и `DeviceStatus` в метаданные. 
* Запрос: `select *, DeviceId, DeviceStatus from iotHubInput`
* Конфигурация выходных данных: `DeviceId,DeviceStatus`

![Столбцы свойств](./media/stream-analytics-define-outputs/10-stream-analytics-property-columns.png)

На следующем снимке экрана показаны свойства исходящих сообщений, проверенные в EventHub с помощью [обозревателя служебной шины](https://github.com/paolosalvatori/ServiceBusExplorer).

![Пользовательские свойства событий](./media/stream-analytics-define-outputs/09-stream-analytics-custom-properties.png)

## <a name="system-properties-for-service-bus-queue-and-topic-outputs"></a>Свойства системы для выходных данных очереди и раздела служебной шины 
Столбцы запросов можно присоединять как [системные свойства](https://docs.microsoft.com/dotnet/api/microsoft.servicebus.messaging.brokeredmessage?view=azure-dotnet#properties) к исходящим сообщениям очереди или раздела служебной шины. Эти столбцы не попадают в полезные данные, вместо этого соответствующее [системное свойство](https://docs.microsoft.com/dotnet/api/microsoft.servicebus.messaging.brokeredmessage?view=azure-dotnet#properties) BrokeredMessage заполняется значениями столбцов запроса.
Эти системные свойства поддерживаются — `MessageId, ContentType, Label, PartitionKey, ReplyTo, SessionId, CorrelationId, To, ForcePersistence, TimeToLive, ScheduledEnqueueTimeUtc`.
Строковые значения этих столбцов анализируются как соответствующие типы значений системных свойств, а все ошибки синтаксического анализа обрабатываются как ошибки данных.
Это поле предоставляется в виде формата объекта JSON. Ниже приведены сведения об этом формате.
* Заключено в фигурные скобки {}.
* Написано в парах "ключ — значение".
* Ключи и значения должны быть строками.
* Key — это имя системного свойства, а значение — имя столбца запроса.
* Ключи и значения разделяются двоеточием.
* Каждая пара "ключ-значение" отделяется запятыми.

В этом примере показано, как использовать это свойство —

* Запрос: `select *, column1, column2 INTO queueOutput FROM iotHubInput`
* Столбцы системных свойств: `{ "MessageId": "column1", "PartitionKey": "column2"}`

Это устанавливает `MessageId` в сообщениях очереди служебной шины с значениями `column1`, а PartitionKey — со значениями `column2`.

## <a name="partitioning"></a>Секционирование

В следующей таблице указаны поддержка секционирования и количество записей выходных данных для каждого типа данных:

| Тип выходных данных | Поддержка секционирования | Ключ секции  | Количество записей выходных данных |
| --- | --- | --- | --- |
| Хранилище озера данных Azure | Да | Используйте токены {Date} и {Time} в шаблоне префикса пути. Выберите формат даты, например гггг/мм/дд, дд/мм/гггг или мм-дд-гггг. В качестве формата времени используется HH. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| База данных SQL Azure | Да, необходимо включить. | На основе предложения PARTITION BY в запросе. | Если включен параметр наследования секционирования, следует за входным секционированием для [полностью параллелизуемые запросов](stream-analytics-scale-jobs.md). Дополнительные сведения о повышении производительности записи при загрузке данных в базу данных SQL Azure см. в статье [Azure Stream Analytics выходные данные в базу данных SQL Azure](stream-analytics-sql-output-perf.md). |
| Хранилище BLOB-объектов Azure | Да | Используйте токены {Date} и {Time} из полей событий в шаблоне пути. Выберите формат даты, например гггг/мм/дд, дд/мм/гггг или мм-дд-гггг. В качестве формата времени используется HH. Выходные данные большого двоичного объекта можно секционировать с помощью одного атрибута настраиваемого события {fieldname} или {datetime:\<specifier>}. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Центры событий Azure | Да | Да | Изменяется в зависимости от выравнивания секций.<br /> Если ключ секции для выходных данных концентратора событий одинаково согласован с вышестоящим (предыдущим) шагом запроса, количество модулей записи совпадает с количеством секций в выходных данных концентратора событий. Каждый модуль записи использует [класс EventHubSender](/dotnet/api/microsoft.servicebus.messaging.eventhubsender?view=azure-dotnet) для отправки событий в определенную секцию. <br /> Если ключ секции для выходных данных концентратора событий не согласован с вышестоящим (предыдущим) шагом запроса, число модулей записи совпадает с количеством секций в этом предыдущем шаге. Каждый модуль записи использует [класс сендбатчасинк](/dotnet/api/microsoft.servicebus.messaging.eventhubclient.sendasync?view=azure-dotnet) в **EventHubClient** для отправки событий во все выходные секции. |
| Power BI | Нет | Нет | Не применяется |
| табличное хранилище Azure; | Да | Любой выходной столбец.  | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Раздел служебной шины Azure | Да | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ секции — это уникальное целочисленное значение для каждой секции.| Совпадает с количеством секций в разделе выходных данных.  |
| Очередь служебной шины Azure | Да | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ секции — это уникальное целочисленное значение для каждой секции.| Совпадает с количеством секций в очереди выходных данных. |
| Azure Cosmos DB | Да | На основе предложения PARTITION BY в запросе. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Функции Azure | Да | На основе предложения PARTITION BY в запросе. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |

Количество модулей записи вывода также можно контролировать с помощью предложения `INTO <partition count>` (см. [в](https://docs.microsoft.com/stream-analytics-query/into-azure-stream-analytics#into-shard-count)) в запросе, что может оказаться полезным при достижении требуемой топологии задания. Если выходной адаптер не секционирован, нехватка данных в одном входном разделе приведет к задержке вплоть до установленного допустимого интервала. В таких случаях выходные данные объединяются в один модуль записи, что может вызвать узкие места в конвейере. Дополнительные сведения о политике с поздним получением см. в разделе [Azure Stream Analytics рекомендации по упорядочиванию событий](stream-analytics-out-of-order-and-late-events.md).

## <a name="output-batch-size"></a>Размер выходного пакета
Azure Stream Analytics использует пакеты с переменным размером для обработки событий и записи в выходные данные. Обычно обработчик Stream Analytics не записывает одно сообщение за раз и использует пакеты для повышения эффективности. Если интенсивность входящих и исходящих событий высока, Stream Analytics использует большие пакеты. При низкой интенсивности исходящего трафика используются пакеты меньшего размера, чтобы обеспечить низкую задержку.

В следующей таблице описаны некоторые рекомендации по пакетированию выходных данных.

| Тип выходных данных | Максимальный размер сообщения | Оптимизация размера пакета |
| :--- | :--- | :--- |
| Хранилище озера данных Azure | См. раздел [ограничения Data Lake Storage](../azure-subscription-service-limits.md#data-lake-store-limits). | Используйте до 4 МБ на операцию записи. |
| База данных SQL Azure | Можно настроить с помощью параметра max Batch Count. 10 000. максимум и 100 минимальных строк на одну операцию групповой вставки по умолчанию.<br />См. раздел [ограничения SQL Azure](../sql-database/sql-database-resource-limits.md). |  Каждый пакет первоначально вставляется с максимальным числом пакетов. Пакетная обработка разделяется на половину (до минимального количества пакетов), основанного на повторяемых ошибках SQL. |
| Хранилище BLOB-объектов Azure | См. раздел [ограничения хранилища Azure](../azure-subscription-service-limits.md#storage-limits). | Максимальный размер блока BLOB-объектов составляет 4 МБ.<br />Максимальное число Bock больших двоичных объектов — 50 000. |
| Центры событий Azure  | 256 КБ или 1 МБ на сообщение. <br />См. раздел [ограничения концентраторов событий](../event-hubs/event-hubs-quotas.md). |  Если секционирование ввода-вывода не согласовано, каждое событие упаковывается по отдельности в `EventData` и отправляется в пакете вплоть до максимального размера сообщения. Это также происходит, если используются [пользовательские свойства метаданных](#custom-metadata-properties-for-output) . <br /><br />  При согласовании секционирования ввода-вывода несколько событий упаковываются в один экземпляр `EventData`, до максимального размера сообщения и отправляется. |
| Power BI | См. раздел [ограничения API-интерфейса Power BI](https://msdn.microsoft.com/library/dn950053.aspx). |
| табличное хранилище Azure; | См. раздел [ограничения хранилища Azure](../azure-subscription-service-limits.md#storage-limits). | Значение по умолчанию — 100 сущностей на одну транзакцию. При необходимости можно настроить меньшее значение. |
| Очередь служебной шины Azure   | 256 КБ на сообщение для уровня "Стандартный", 1 МБ для уровня "Премиум".<br /> См. раздел [ограничения служебной шины](../service-bus-messaging/service-bus-quotas.md). | Использовать одно событие для каждого сообщения. |
| Раздел служебной шины Azure | 256 КБ на сообщение для уровня "Стандартный", 1 МБ для уровня "Премиум".<br /> См. раздел [ограничения служебной шины](../service-bus-messaging/service-bus-quotas.md). | Использовать одно событие для каждого сообщения. |
| Azure Cosmos DB   | См. раздел [ограничения Azure Cosmos DB](../azure-subscription-service-limits.md#azure-cosmos-db-limits). | Размер пакета и частота записи корректируются динамически на основе Azure Cosmos DB ответов. <br /> Отсутствуют предопределенные ограничения Stream Analytics. |
| Функции Azure   | | Размер пакета по умолчанию составляет 262 144 байт (256 КБ). <br /> Количество событий по умолчанию для каждого пакета составляет 100. <br /> Размер пакета можно настроить, его можно увеличить или уменьшить в [параметрах вывода](#azure-functions) Stream Analytics.

## <a name="next-steps"></a>Следующие шаги
> [!div class="nextstepaction"]
> 
> [Краткое руководство по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: https://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: https://go.microsoft.com/fwlink/?LinkId=517301
