---
title: Преобразование данных с помощью потока данных отображения
description: В этом уроке содержатся пошаговые инструкции по использованию Фабрики данных Azure для преобразования данных с помощью картографического потока данных
author: djpmsft
ms.author: daperlov
ms.reviewer: makromer
ms.service: data-factory
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 10/07/2019
ms.openlocfilehash: 917a8d6edf04d8a160c3a6a5ac59949623dfee5c
ms.sourcegitcommit: b80aafd2c71d7366838811e92bd234ddbab507b6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/16/2020
ms.locfileid: "81418683"
---
# <a name="transform-data-using-mapping-data-flows"></a>Преобразование данных с помощью картографических потоков данных

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

Если вы еще не работали с фабрикой данных Azure, ознакомьтесь со статьей [Введение в фабрику данных Azure](introduction.md).

В этом уроке вы будете использовать пользовательский интерфейс Azure Data Factory (UX) для создания конвейера, который копирует и преобразует данные из источника Azure Data Lake Storage (ADLS) Gen2 в уторение ADLS Gen2 с помощью картографического потока данных. Шаблон конфигурации в этом учебнике может быть расширен при преобразовании данных с помощью картографического потока данных

Вот какие шаги выполняются в этом руководстве:

> [!div class="checklist"]
> * Создали фабрику данных.
> * Создайте конвейер с действием потока данных.
> * Создайте поток картографических данных с четырьмя преобразованиями.
> * тестовый запуск конвейера;
> * Мониторинг активности потока данных

## <a name="prerequisites"></a>Предварительные требования
* **Подписка Azure**. Если у вас еще нет подписки Azure, создайте [бесплатную учетную запись](https://azure.microsoft.com/free/) Azure, прежде чем начинать работу.
* **Учетная запись хранения Azure**. Вы используете хранилище ADLS в качестве *источника* и хранилища *данных.* Если у вас нет учетной записи хранения, создайте ее, следуя действиям в [этом разделе](../storage/common/storage-account-create.md).

Файл, который мы преобразуем в этом учебнике MoviesDB.csv, который можно найти [здесь](https://raw.githubusercontent.com/djpmsft/adf-ready-demo/master/moviesDB.csv). Чтобы получить файл из GitHub, скопируйте содержимое текстовому редактору по вашему выбору, чтобы сохранить локально в виде файла .csv. Чтобы загрузить файл в учетную запись хранилища, [см.](../storage/blobs/storage-quickstart-blobs-portal.md) Примерами будут ссылки на контейнер под названием "выборочные данные".

## <a name="create-a-data-factory"></a>Создание фабрики данных

На этом этапе необходимо создать фабрику данных и открыть UX фабрики данных для создания конвейера на фабрике данных.

1. Откройте **Microsoft Edge** или **Google Chrome**. В настоящее время интерфейс Data Factory поддерживается только в веб-браузерах Microsoft Edge и Google Chrome.
2. В левом меню выберите Создать**Analytics** > **фабрику аналитики данных** **ресурсов:** > 

   ![Выбор фабрики данных в области "Создать"](./media/doc-common-process/new-azure-data-factory-menu.png)

3. На странице **Новая фабрика данных** в поле **Имя** введите **ADFTutorialDataFactory**.

   Название фабрики данных Azure должно быть *уникальным во всем мире.* Если вы увидите следующую ошибку касательно значения имени, введите другое имя фабрики данных. (Например, используйте yournameADFTutorialDataFactory.) Дополнительные сведения о правилах именования артефактов фабрики данных см. в статье [Фабрика данных Azure — правила именования](naming-rules.md).

     ![Новая фабрика данных](./media/doc-common-process/name-not-available-error.png)
4. Выберите **подписку** Azure, в рамках которой вы хотите создать фабрику данных.
5. Для **группы ресурсов** выполните одно из следующих действий:

    а. Выберите **Использовать существующую**и укажите существующую группу ресурсов в раскрывающемся списке.

    b. Выберите **Создать новую**и укажите имя группы ресурсов. 
         
    Сведения о группах ресурсов см. в статье [Общие сведения об Azure Resource Manager](../azure-resource-manager/management/overview.md). 
6. В качестве **версии** выберите **V2**.
7. В поле **Расположение** выберите расположение фабрики данных. В раскрывающемся списке отображаются только поддерживаемые расположения. Хранилища данных (например, база данных Azure Storage и База данных S'L) и вычисления (например, Azure HDInsight), используемые фабрикой данных, могут быть в других регионах.
8. Нажмите кнопку **создания**.
9. После завершения создания вы увидите уведомление в центре уведомлений. Нажмите кнопку **Перейти к ресурсу**, чтобы открыть страницу фабрики данных.
10. Выберите **Создание и мониторинг**, чтобы запустить на отдельной вкладке пользовательский интерфейс фабрики данных.

## <a name="create-a-pipeline-with-a-data-flow-activity"></a>Создание конвейера с помощью действия потока данных

На этом этапе вы создадите конвейер, содержащий активность потока данных.

1. На странице **Начало работы** выберите **Create pipeline** (Создать конвейер).

   ![Создание конвейера](./media/doc-common-process/get-started-page.png)

1. В **общей** вкладке для конвейера введите **TransformMovies** для **названия** конвейера.
1. В верхней панели завода сдвиньте **ползунк отладки Data Flow.** Режим Debug позволяет проводить интерактивное тестирование логики преобразования против живого кластера Spark. Кластеры потока данных нагреваются в течение 5-7 минут, и пользователям рекомендуется сначала включить отладку, если они планируют разработать Data Flow. Для получения дополнительной [Debug Mode](concepts-data-flow-debug-mode.md)информации см.

    ![Активность потока данных](media/tutorial-data-flow/dataflow1.png)
1. В панели **Действий** расширьте аккордеон **Move and Transform.** Перетащите и отбросьте активность **потока данных** с панели на холст конвейера.

    ![Активность потока данных](media/tutorial-data-flow/activity1.png)
1. Во всплывающем окне **Flow добавления данных** выберите **Создать новый поток данных,** а затем назовите поток данных **TransformMovies.** По завершении нажмите кнопку Готово.

    ![Активность потока данных](media/tutorial-data-flow/activity2.png)

## <a name="build-transformation-logic-in-the-data-flow-canvas"></a>Создание логики преобразования в холсте потока данных

Как только вы создадите поток данных, вы будете автоматически отправлены на полотно потока данных. На этом этапе вы создадете поток данных, который берет moviesDB.csv в хранилище ADLS и агрегирует среднюю оценку комедий с 1910 по 2000 год. Затем вы запишите этот файл обратно в хранилище ADLS.

1. В холсте потока данных добавьте источник, нажав на поле **Add Source.**

    ![Холст потока данных](media/tutorial-data-flow/dataflow2.png)
1. Назовите свой источник **MoviesDB**. Нажмите на **New,** чтобы создать новый набор исходных данных.

    ![Холст потока данных](media/tutorial-data-flow/dataflow3.png)
1. Выберите **Хранилище данных Azure Data Lake Gen2.** Нажмите кнопку "Продолжить".

    ![Dataset](media/tutorial-data-flow/dataset1.png)
1. Выберите **DelimitedText**. Нажмите кнопку "Продолжить".

    ![Dataset](media/tutorial-data-flow/dataset2.png)
1. Назовите свой набор данных **MoviesDB.** В связанном с ним сервисе выпадите, выберите **новое**.

    ![Dataset](media/tutorial-data-flow/dataset3.png)
1. На экране создания связанных с ними службы назовите службу **ADLS** gen2 и укажите метод проверки подлинности. Затем введите учетные данные соединения. В этом уроке мы используем ключ учетной записи для подключения к нашей учетной записи. Вы можете нажать **тестное соединение,** чтобы проверить, что ваши учетные данные были введены правильно. После завершения нажмите Создать.

    ![Связанные службы](media/tutorial-data-flow/ls1.png)
1. Как только вы вернетесь на экран создания набора данных, введите, где ваш файл находится под полем **пути файла.** В этом учебнике файл moviesDB.csv находится в контейнерных выборочных данных. Поскольку файл имеет заголовки, проверьте **первый ряд в качестве заголовка.** Выберите **из соединения/магазина** импортировать схему заголовка непосредственно из файла в хранилище. Когда закончите, нажмите кнопку ОК.

    ![Наборы данных](media/tutorial-data-flow/dataset4.png)
1. Если кластер отладки начался, перейдите на вкладку **«Предварительный просмотр данных»** преобразования источника и нажмите **«Обновление»,** чтобы получить снимок данных. Можно использовать предварительный просмотр данных для проверки правильной настройки преобразования.

    ![Холст потока данных](media/tutorial-data-flow/dataflow4.png)
1. Рядом с исходным узелом на холсте потока данных нажмите на значок plus, чтобы добавить новое преобразование. Первая трансформация, которая добавляется, это **фильтр.**

    ![Холст потока данных](media/tutorial-data-flow/dataflow5.png)
1. Назовите трансформацию фильтра **FilterYears.** Нажмите на поле выражения рядом с **фильтром,** чтобы открыть строитель выражения. Здесь вы укажете состояние фильтрации.

    ![Filter](media/tutorial-data-flow/filter1.png)
1. Строитель выражения потока данных позволяет интерактивно создавать выражения для использования в различных преобразованиях. Выражения могут включать встроенные функции, столбцы из схемы ввода и параметры, определяемые пользователем. Для получения дополнительной информации о [Data Flow expression builder](concepts-data-flow-expression-builder.md)том, как создавать выражения, см.

    В этом учебнике, вы хотите, чтобы фильтровать фильмы жанра комедии, которые вышли между 1910 и 2000 годах. Поскольку год в настоящее время является строкой, необходимо ```toInteger()``` преобразовать ее в целый ряд с помощью функции. Используйте больше или равна (>) и меньше, чем или равна (<) операторов для сравнения с буквальном значения 1910 года и 200-. Объедините эти выражения вместе с оператором и (&&). Выражение выходит как:

    ```toInteger(year) >= 1910 && toInteger(year) <= 2000```

    Чтобы найти, какие фильмы комедии, вы можете использовать функцию, ```rlike()``` чтобы найти шаблон "Комедия" в колонке жанров. Союз Rlike выражение с годом сравнения, чтобы получить:

    ```toInteger(year) >= 1910 && toInteger(year) <= 2000 && rlike(genres, 'Comedy')```

    Если кластер отладки активен, можно проверить свою логику, нажав **кнопку «Обновление»,** чтобы увидеть выход выражения по сравнению с используемыми входами. Существует несколько правильных ответов на то, как вы можете выполнить эту логику с помощью языка выражения потока данных.

    ![Filter](media/tutorial-data-flow/filter2.png)

    Нажмите **Сохранить и закончить,** как только вы закончите с выражением.

1. Получение **предварительного просмотра данных** для проверки правильной работы фильтра.

    ![Filter](media/tutorial-data-flow/filter3.png)
1. Следующее преобразование, добавленное к преобразованию, — это **агрегированное** преобразование под **модификатором Schema.**

    ![Статистическое](media/tutorial-data-flow/agg1.png)
1. Назовите свою совокупную трансформацию **AggregateComedyRatings.** В **группе за** вкладкой, выберите **год** от падения к группе агрегации к году фильм вышел.

    ![Статистическое](media/tutorial-data-flow/agg2.png)
1. Перейдите на вкладку **Агрегированные.** В левом текстовом окне назовите агрегированную колонку **AverageComedyRating**. Нажмите на правое поле выражения, чтобы ввести агрегированное выражение через строитель выражения.

    ![Статистическое](media/tutorial-data-flow/agg3.png)
1. Чтобы получить среднее **значение**рейтинга ```avg()``` столбца, используйте агрегированную функцию. Поскольку **рейтинг** является ```avg()``` строкой и принимает в числовом вход, мы ```toInteger()``` должны преобразовать значение в число через функцию. Это выражение выглядит следующим образом:

    ```avg(toInteger(Rating))```

    Нажмите **Сохранить и закончить,** когда сделано.

    ![Статистическое](media/tutorial-data-flow/agg4.png)
1. Перейдите на вкладку **«Предварительный просмотр данных»** для просмотра вывода преобразования. Обратите внимание только две колонки есть, **год** и **AverageComedyRating**.

    ![Статистическое](media/tutorial-data-flow/agg3.png)
1. Далее необходимо добавить преобразование **sink** под **destination.**

    ![Приемник](media/tutorial-data-flow/sink1.png)
1. Назовите **раковину.** Нажмите **Новый,** чтобы создать набор данных раковины.

    ![Приемник](media/tutorial-data-flow/sink2.png)
1. Выберите **Хранилище данных Azure Data Lake Gen2.** Нажмите кнопку "Продолжить".

    ![Dataset](media/tutorial-data-flow/dataset1.png)
1. Выберите **DelimitedText**. Нажмите кнопку "Продолжить".

    ![Dataset](media/tutorial-data-flow/dataset2.png)
1. Назовите свой набор данных sink **MoviesSink.** Для службы связанного с вами службы ADLS gen2, созданной на шаг 6. Введите папку вывода для записи данных. В этом учебнике мы пишем папку 'выход' в контейнере 'образ-данных'. Папка не должна существовать заранее и может быть динамически создана. Установите **первую строку в качестве заголовка** как верно и выберите **Нет** для **схемы импорта.** Нажмите кнопку «Готово».

    ![Приемник](media/tutorial-data-flow/sink3.png)

Теперь вы закончили строить свой поток данных. Вы готовы запустить его в конвейере.

## <a name="running-and-monitoring-the-data-flow"></a>Запуск и мониторинг потока данных

Перед публикацией можно отладить конвейер. На этом этапе вы запустите отладку конвейера потока данных. В то время как предварительный просмотр данных не записывает данные, отладка запуска будет записывать данные к месту вашего раковины.

1. Перейдите на холст конвейера. Нажмите **отладить,** чтобы вызвать отладку.

    ![Pipeline](media/tutorial-data-flow/pipeline1.png)
1. Отладка трубопровода действий Data Flow использует активный кластер отладки, но по-прежнему занимает по крайней мере минуту, чтобы инициализировать. Вы можете отслеживать прогресс через вкладку **Выход.** После того, как запуск успешно, нажмите на значок очки, чтобы открыть панель мониторинга.

    ![Pipeline](media/tutorial-data-flow/pipeline2.png)
1. В панели мониторинга можно увидеть количество строк и время, затраченное на каждом этапе преобразования.

    ![Наблюдение](media/tutorial-data-flow/pipeline3.png)
1. Нажмите на преобразование, чтобы получить подробную информацию о столбцах и разделении данных.

    ![Наблюдение](media/tutorial-data-flow/pipeline4.png)

Если вы следовали этому учебнику правильно, вы должны были написать 83 строки и 2 столбца в папку раковины. Вы можете проверить, что данные верны, проверив ваше хранилище капли.

## <a name="next-steps"></a>Дальнейшие действия

Конвейер в этом учебнике запускает поток данных, который агрегирует среднюю оценку комедий с 1910 по 2000 год и записывает данные в ADLS. Вы ознакомились с выполнением следующих задач:

> [!div class="checklist"]
> * Создали фабрику данных.
> * Создайте конвейер с действием потока данных.
> * Создайте поток картографических данных с четырьмя преобразованиями.
> * тестовый запуск конвейера;
> * Мониторинг активности потока данных

Подробнее о [языке выражения потока данных.](data-flow-expression-functions.md)
