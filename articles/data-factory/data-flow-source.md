---
title: Преобразование источника в потоке данных сопоставления
description: Узнайте, как настроить преобразование источника в потоке данных сопоставления.
author: kromerm
ms.author: makromer
manager: anandsub
ms.service: data-factory
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 09/06/2019
ms.openlocfilehash: 33a63b8a887594747aba03e19c107653e438853f
ms.sourcegitcommit: d6b68b907e5158b451239e4c09bb55eccb5fef89
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/20/2019
ms.locfileid: "74217734"
---
# <a name="source-transformation-for-mapping-data-flow"></a>Преобразование источника для потока данных сопоставления 

Преобразование «источник» настраивает источник данных для потока данных. При проектировании потоков данных первым шагом всегда будет настройка преобразования «источник». Чтобы добавить источник, щелкните поле **Добавить источник** на холсте потока данных.

Для каждого потока данных требуется по крайней мере одно преобразование источника, но при необходимости можно добавить столько источников, сколько необходимо для выполнения преобразований данных. Вы можете объединить эти источники вместе с преобразованием «соединение», «уточняющий запрос» или «объединение».

Каждое преобразование источника связано ровно с одним набором данных фабрики данных. Набор данных определяет форму и расположение данных, которые необходимо записать или прочитать. При использовании файлового набора данных можно использовать подстановочные знаки и списки файлов в источнике для работы с несколькими файлами за раз.

## <a name="supported-connectors-in-mapping-data-flow"></a>Поддерживаемые соединители в потоке данных сопоставления

Поток данных сопоставления соответствует подходу "извлечь", "Загрузка", "преобразование" (ELT) и работает с *промежуточными* наборами DataSet, которые находятся в Azure. В настоящее время в преобразовании источника можно использовать следующие наборы данных:
    
* Хранилище BLOB-объектов Azure (JSON, Avro, Text, Parquet)
* Azure Data Lake Storage 1-го поколения (JSON, Avro, Text, Parquet)
* Azure Data Lake Storage 2-го поколения (JSON, Avro, Text, Parquet)
* Хранилище данных SQL Azure
* База данных SQL Azure
* Azure CosmosDB

Фабрика данных Azure имеет доступ к более чем 80 собственным соединителям. Чтобы включить данные из других источников в потоке данных, используйте действие копирования для загрузки этих данных в одну из поддерживаемых промежуточных областей.

## <a name="source-settings"></a>Параметры источника

После добавления источника настройте его с помощью вкладки **Параметры источника** . Здесь можно выбрать или создать набор данных, на котором находятся исходные точки. Можно также выбрать схему и параметры выборки для данных.

![Вкладка "Параметры источника"](media/data-flow/source1.png "Вкладка "Параметры источника"")

Отклонение **схемы.** [смещение схемы](concepts-data-flow-schema-drift.md) — это способность фабрики данных в собственном коде управлять гибкими схемами в потоках данных без необходимости явно определять изменения столбцов.

* Установите флажок **Разрешить смещение схемы** , если исходные столбцы будут часто меняться. Этот параметр позволяет всем входящим полям источника проходить через преобразования в приемник.

* Выбор **типов столбцов с несмещенными типами** позволит фабрике данных обнаруживать и определять типы данных для каждого обнаруженного нового столбца. Если эта функция отключена, все смещенные столбцы будут иметь строковый тип.

**Проверить схему:** Если выбран параметр проверить схему, поток данных не будет выполняться, если входящие исходные данные не соответствуют определенной схеме набора данных.

**Пропустить число строк:** В поле пропустить число строк указывается, сколько строк следует игнорировать в начале набора данных.

**Выборка:** Включите выборку, чтобы ограничить количество строк в источнике. Используйте этот параметр при тестировании или выборки данных из источника для отладки.

**Многострочные строки:** Выберите многострочные строки, если исходный текстовый файл содержит строковые значения, охватывающие несколько строк, то есть символы новой строки в значении.

Чтобы проверить правильность настройки источника, включите режим отладки и получите предварительную версию данных. Дополнительные сведения см. в разделе [режим отладки](concepts-data-flow-debug-mode.md).

> [!NOTE]
> Когда режим отладки включен, конфигурация ограничения строк в параметрах отладки будет перезаписывать параметр выборки в источнике при предварительном просмотре данных.

## <a name="file-based-source-options"></a>Параметры источника на основе файлов

Если вы используете файловый набор данных, например хранилище BLOB-объектов Azure или Azure Data Lake Storage, вкладка **Параметры источника** позволяет управлять тем, как источник считывает файлы.

![Параметры источника](media/data-flow/sourceOPtions1.png "Параметры источника")

**Путь с подстановочными знаками:** Использование шаблона с подстановочными знаками даст указание потоку ADF перебрать каждую соответствующую папку и файл в одном преобразовании источника. Это эффективный способ обработки нескольких файлов в одном потоке. Добавьте несколько шаблонов сопоставления с подстановочными знаками с символом +, который появляется при наведении указателя мыши на существующий шаблон шаблона.

В исходном контейнере выберите ряд файлов, соответствующих шаблону. В наборе данных можно указать только контейнер. Путь к шаблону должен также включать путь к папке из корневой папки.

Примеры подстановочных знаков:

* ```*``` представляет любой набор символов
* ```**``` представляет рекурсивную вложенность каталога
* ```?``` заменяет один символ
* ```[]``` соответствует одному из символов в квадратных скобках

* ```/data/sales/**/*.csv``` получает все CSV-файлы в/Дата/Салес
* ```/data/sales/20??/**``` получает все файлы в 20 века
* ```/data/sales/2004/*/12/[XY]1?.csv``` получает все CSV-файлы в 2004 в декабре, начиная с префикса X или Y и заканчивая двузначным числом

**Корневой путь к разделу:** Если в источнике файлов имеются секционированные папки с форматом ```key=value``` (например, Year = 2019), то верхний уровень этого дерева папок секционирования можно назначить имени столбца в потоке данных потока.

Во-первых, задайте подстановочный знак, чтобы включить все пути, которые являются секционированными папками, а также конечные файлы, которые вы хотите прочитать.

![Параметры исходного файла секции](media/data-flow/partfile2.png "Параметр файла секционирования")

Для определения верхнего уровня структуры папок используйте параметр путь к корневому каталогу секции. При просмотре содержимого данных с помощью предварительного просмотра данных вы увидите, что ADF добавит разрешенные секции, найденные на каждом уровне папок.

![Корневой путь к разделу](media/data-flow/partfile1.png "Предварительный просмотр корневого пути к разделу")

**Список файлов:** Это набор файлов. Создайте текстовый файл, содержащий список файлов относительных путей для обработки. Наведите указатель на этот текстовый файл.

**Столбец для хранения имени файла:** Сохраните имя исходного файла в столбце данных. Введите имя нового столбца, чтобы сохранить строку имени файла.

**После завершения:** Выберите не выполнять никаких действий с исходным файлом после выполнения потока данных, удалите исходный файл или переместите исходный файл. Пути для перемещения являются относительными.

Чтобы переместить исходные файлы в другое расположение после обработки, сначала выберите "Переместить" для операции с файлом. Затем задайте каталог "from". Если вы не используете подстановочные знаки для своего пути, параметр "от" будет иметь ту же папку, что и исходная папка.

Если у вас есть исходный путь с подстановочным знаком, синтаксис будет выглядеть следующим образом:

```/data/sales/20??/**/*.csv```

Можно указать "от" в качестве

```/data/sales```

И "to" как

```/backup/priorSales```

В этом случае все файлы, источником которых является/дата/Салес, перемещаются в/Баккуп/приорсалес.

> [!NOTE]
> Операции с файлами выполняются только при запуске потока данных из запуска конвейера (Отладка или выполнение конвейера), в котором используется действие «выполнение потока данных» в конвейере. Операции с файлами *не* выполняются в режиме отладки потока данных.

**Фильтровать по дате последнего изменения:** Вы можете отфильтровать обрабатываемые файлы, указав диапазон дат последнего изменения. Все значения даты и времени указаны в формате UTC. 

### <a name="add-dynamic-content"></a>Добавление динамического содержимого

Все параметры источника могут быть заданы в виде выражений с помощью [языка выражений преобразования потока данных сопоставления](data-flow-expression-functions.md). Чтобы добавить динамическое содержимое, щелкните или наведите указатель мыши внутри полей на панели параметры. Щелкните гиперссылку, чтобы **добавить динамическое содержимое**. Запустится построитель выражений, где можно задать значения динамически с помощью выражений, статических литеральных значений или параметров.

![Параметры](media/data-flow/params6.png "parameters")

## <a name="sql-source-options"></a>Параметры источника SQL

Если источник находится в базе данных SQL или хранилище данных SQL, на вкладке **Параметры источника** доступны дополнительные параметры, относящиеся к SQL. 

**Входные данные:** Укажите, следует ли указывать источник в таблице (эквивалентно ```Select * from <table-name>```) или введите пользовательский SQL-запрос.

**Запрос**: при выборе запроса в поле ввода введите SQL-запрос для источника. Этот параметр переопределяет любую таблицу, выбранную в наборе данных. Предложения **ORDER BY** не поддерживаются здесь, но можно задать полную инструкцию SELECT FROM. Можно также использовать определяемые пользователем табличные функции. **SELECT * FROM удфжетдата ()** — это определяемая пользователем функция в SQL, возвращающая таблицу. Этот запрос приведет к созданию исходной таблицы, которую можно использовать в потоке данных. Использование запросов также является отличным способом сокращения количества строк для тестирования или поиска. Пример: ```Select * from MyTable where customerId > 1000 and customerId < 2000```

**Размер пакета**: введите размер пакета для фрагментов больших данных в операциях чтения.

**Уровень изоляции**: значение по умолчанию для источников SQL в потоке данных сопоставления — READ UNCOMMITTED. Здесь можно изменить уровень изоляции на одно из следующих значений:
* Чтение зафиксировано
* Незафиксированное чтение
* Повторяющаяся операция чтения
* Сериализует
* Нет (пропускать уровень изоляции)

![Уровень изоляции](media/data-flow/isolationlevel.png "Уровень изоляции")

## <a name="projection"></a>Проекция

Подобно схемам в наборах данных, проекция в источнике определяет столбцы данных, типы и форматы из исходных. Для большинства типов наборов данных, таких как SQL и Parquet, проекция в источнике исправлена в соответствии со схемой, определенной в наборе данных. Если исходные файлы не являются строго типизированными (например, плоскими CSV-файлами, а не файлами Parquet), можно определить типы данных для каждого поля в преобразовании «источник».

![Параметры на вкладке "Проекция"](media/data-flow/source3.png "Проекция")

Если в текстовом файле не определена схема, выберите команду **определить тип данных** , чтобы фабрика данных выработала выборку и вывести типы данных. Выберите параметр **определить формат по умолчанию** , чтобы автоматически определить форматы данных по умолчанию. 

Типы данных столбца можно изменить в некотором преобразовании «Производный столбец». Используйте преобразование SELECT, чтобы изменить имена столбцов.

### <a name="import-schema"></a>Импорт схемы

Для наборов данных, таких как Avro и CosmosDB, которые поддерживают сложные структуры, определения схем не должны существовать в наборе данных. Таким образом, вы сможете нажать кнопку "Импорт схемы" на вкладке "Проекция" для этих типов источников.

## <a name="cosmosdb-specific-settings"></a>Специальные параметры CosmosDB

При использовании CosmosDB в качестве типа источника необходимо учитывать несколько параметров.

* Включить системные столбцы: Если этот флажок установлен, ```id```, ```_ts```и другие системные столбцы будут включены в метаданные потока данных из CosmosDB. При обновлении коллекций важно включить этот параметр, чтобы можно было взять существующий идентификатор строки.
* Размер страницы: количество документов на страницу результата запроса. Значение по умолчанию — "-1", которое использует динамическую страницу службы до 1000.
* Пропускная способность. задайте необязательное значение для числа RUs, которое вы хотите применить к коллекции CosmosDB для каждого выполнения потока данных во время операции чтения. Минимум — 400.
* Предпочтительные регионы: для этого процесса можно выбрать предпочтительные регионы чтения.

## <a name="optimize-the-source-transformation"></a>Оптимизация преобразования источника

На вкладке **Оптимизация** для преобразования источник может отображаться тип **исходной** секции. Этот параметр доступен, только если источником является база данных SQL Azure. Это связано с тем, что фабрика данных пытается сделать подключения параллельными для выполнения больших запросов к источнику базы данных SQL.

![Параметры исходной секции](media/data-flow/sourcepart3.png "Секционирование")

Вам не нужно секционировать данные в источнике базы данных SQL, но секции полезны для больших запросов. Вы можете создать секцию на основе столбца или запроса.

### <a name="use-a-column-to-partition-data"></a>Использование столбца для секционирования данных

В исходной таблице выберите столбец для секционирования. Также задайте количество секций.

### <a name="use-a-query-to-partition-data"></a>Использование запроса для секционирования данных

Можно выбрать секционирование соединений на основе запроса. Введите содержимое предиката WHERE. Например, введите year > 1980.

Дополнительные сведения о оптимизации в потоке данных сопоставления см. на [вкладке "оптимизация"](concepts-data-flow-overview.md#optimize).

## <a name="next-steps"></a>Дополнительная информация

Начните создавать [Преобразование «Производный столбец](data-flow-derived-column.md) » и [Преобразование «выбор](data-flow-select.md)».
