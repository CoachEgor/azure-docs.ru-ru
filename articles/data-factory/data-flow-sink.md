---
title: Настройка преобразования приемника в функции потока данных сопоставления фабрики данных Azure
description: Узнайте, как настроить преобразование приемника в потоке данных сопоставления.
author: kromerm
ms.author: makromer
ms.service: data-factory
ms.topic: conceptual
ms.date: 02/03/2019
ms.openlocfilehash: be2ab5605f7fa60ebb78493f714648d458e82a6c
ms.sourcegitcommit: 11265f4ff9f8e727a0cbf2af20a8057f5923ccda
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/08/2019
ms.locfileid: "72029246"
---
# <a name="sink-transformation-for-a-data-flow"></a>Преобразование приемника для потока данных



После преобразования потока данных можно передать данные в целевой набор данных. В преобразовании «приемник» выберите определение набора данных для выходных данных назначения. Можно использовать столько преобразований приемника, сколько требуется для потока данных.

Чтобы учитывать отклонение схемы и изменения во входящих данных, следует отменять выходные данные в папку без определенной схемы в выходном наборе данных. Можно также учитывать изменения столбцов в источниках, выбрав параметр **Разрешить смещение схемы** в источнике. Затем автоматическое сопоставление все поля в приемнике.

![Параметры на вкладке "приемник", включая параметр "автоматическое отображение"](media/data-flow/sink1.png "приемник 1")

Чтобы включить в приемник все входящие поля, включите **Автоподбор**. Чтобы выбрать поля для приемника в назначении или изменить имена полей в назначении, отключите **автокарту**. Затем откройте вкладку **сопоставление** , чтобы сопоставить поля выходных данных.

![Параметры на вкладке "сопоставление"](media/data-flow/sink2.png "приемника 2")

## <a name="output"></a>Output 
Для хранилища BLOB-объектов Azure или типов приемников Data Lake Storage выводит преобразованные данные в папку. Spark создает секционированные файлы выходных данных на основе схемы секционирования, которую использует преобразование «приемник». 

Схему секционирования можно задать на вкладке « **Оптимизация** ». Если требуется объединить выходные данные в один файл фабрикой данных, выберите **один раздел**.

![Параметры на вкладке «Оптимизация» для](media/data-flow/opt001.png "приемника")

## <a name="field-mapping"></a>Сопоставление полей
На вкладке **сопоставление** преобразования «приемник» можно сопоставить входящие столбцы слева и места назначения справа. При передаче потоков данных в файлы фабрика данных всегда будет записывать новые файлы в папку. При сопоставлении с набором данных можно выбрать параметры операции таблицы базы данных для вставки, обновления, Upsert или удаления.

![Вкладка «Сопоставление »](media/data-flow/sink2.png "Приемники")

В таблице сопоставлений можно выполнить множественный выбор, чтобы связать несколько столбцов, отсоединить несколько столбцов или сопоставить несколько строк с одним и тем же именем столбца.

Чтобы всегда сопоставлять входящий набор полей целевому объекту, так как они и полностью принимают гибкие определения схемы, установите флажок **Разрешить смещение схемы**.

![Вкладка "сопоставление", отображающая поля, сопоставленные со столбцами в наборе данных](media/data-flow/multi1.png "несколько параметров")

Чтобы сбросить сопоставления столбцов, выберите **повторно сопоставить**.

Приемник ![вкладки приемника]—(media/data-flow/sink1.png "один")

Выберите **проверить схему** для сбоя приемника при изменении схемы.

Выберите **Очистить папку** , чтобы усечь содержимое папки приемника перед записью конечных файлов в эту целевую папку.

## <a name="rule-based-mapping"></a>Сопоставление на основе правил
При отключении автоматического сопоставления вы сможете добавить сопоставление на основе столбцов (фиксированное сопоставление) или сопоставление на основе правил. Сопоставление на основе правил позволит писать выражения с сопоставлением шаблонов. 

Сопоставление на ![основе правил](media/data-flow/rules4.png "с") учетом правил

При выборе сопоставления на основе правил вы указываете ADF, что необходимо оценить выражение сопоставления, чтобы оно соответствовало входящим правилам шаблона, и определить имена исходящих полей. Вы можете добавить любое сочетание сопоставлений полей и на основе правил. Затем имена полей создаются в среде выполнения ADF на основе входящих метаданных из источника. Имена созданных полей можно просмотреть во время отладки и с помощью панели предварительного просмотра данных.

Подробные сведения о сопоставлении шаблонов см. в [документации по шаблону столбцов](concepts-data-flow-column-pattern.md).

## <a name="file-name-options"></a>Параметры имени файла

Настройка именования файлов: 

   * **По умолчанию**: Разрешить Spark использовать имена файлов на основе значений по умолчанию для части.
   * **Шаблон**: Введите шаблон для выходных файлов. Например, **займы [n]** будут создавать loans1. csv, loans2. csv и т. д.
   * **На секцию**: Введите одно имя файла для каждой секции.
   * **Как данные в столбце**: Задайте в качестве выходного файла значение столбца.
   * **Вывод в один файл**: Этот параметр позволяет объединить секционированные выходные файлы в один именованный файл. Для использования этого параметра набор данных должен быть разрешен в имя папки. Кроме того, имейте в виду, что эта операция слияния может завершиться сбоем в зависимости от размера узла.

> [!NOTE]
> Файловые операции запускаются только при выполнении действия потока выполнение данных. Они не запускаются в режиме отладки потока данных.

## <a name="database-options"></a>Параметры базы данных

Выберите параметры базы данных:

![Вкладка "Параметры", отображающая параметры SQL для приемника SQL](media/data-flow/alter-row2.png "")

* **Метод обновления**: Значение по умолчанию — разрешить вставки. Снимите флажок **Разрешить вставку** , если требуется запретить вставку новых строк из источника. Чтобы обновить, Upsert или удалить строки, сначала добавьте преобразование «изменение строк» для добавления тегов к строкам для этих действий. 
* **Повторно создать таблицу**: Удалите или создайте целевую таблицу перед завершением потока данных.
* **Усечение таблицы**: Перед завершением потока данных удалите все строки из целевой таблицы.
* **Размер пакета**. Введите число для разделения операций записи на блоки. Используйте этот параметр для загрузки больших данных. 
* **Включить промежуточное хранение**: Используйте Polybase при загрузке хранилища данных Azure в качестве набора данных приемника.
* **Сценарии SQL pre и POST**: Введите многострочные скрипты SQL, которые будут выполняться до (Предварительная обработка) и после чего в базу данных-приемник записываются данные (после обработки).

скрипты(media/data-flow/prepost1.png "обработки SQL") ![, выполняемые до и после выполнения сценариев]обработки SQL

> [!NOTE]
> В потоке данных можно направить фабрику данных для создания нового определения таблицы в целевой базе данных. Чтобы создать определение таблицы, установите набор данных в преобразовании «приемник» с новым именем таблицы. В наборе данных SQL под именем таблицы выберите **изменить** и введите новое имя таблицы. Затем в преобразовании приемника включите параметр **Разрешить смещение схемы**. Задайте для параметра **импортировать схему** значение **нет**.

![Параметры набора данных SQL, в котором показано, где можно изменить имя таблицы](media/data-flow/dataset2.png "Схема SQL")

> [!NOTE]
> При обновлении или удалении строк в приемнике базы данных необходимо задать ключевой столбец. Этот параметр позволяет преобразованию «изменение строки» определить уникальную строку в библиотеке перемещения данных (DML).

## <a name="next-steps"></a>Следующие шаги
Теперь, когда вы создали поток данных, добавьте [в конвейер действие потока данных](concepts-data-flow-overview.md).
