---
title: Перенос данных из Amazon S3 в службу хранилища Azure с помощью фабрики данных Azure | Документация Майкрософт
description: Используйте фабрику данных Azure для переноса данных из Amazon S3 в службу хранилища Azure.
services: data-factory
documentationcenter: ''
author: dearandyxu
ms.author: yexu
ms.reviewer: ''
manager: ''
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.topic: conceptual
ms.date: 8/04/2019
ms.openlocfilehash: c56e6e004fe7f63725b5f6f4b9c71f60cc7b91ed
ms.sourcegitcommit: 3073581d81253558f89ef560ffdf71db7e0b592b
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/06/2019
ms.locfileid: "68829116"
---
# <a name="use-azure-data-factory-to-migrate-data-from-amazon-s3-to-azure-storage"></a>Перенос данных из Amazon S3 в службу хранилища Azure с помощью фабрики данных Azure 

Фабрика данных Azure предоставляет производительный, надежный и экономичный механизм переноса данных из Amazon S3 в хранилище BLOB-объектов Azure или Azure Data Lake Storage 2-го поколения.  Эта статья содержит следующие сведения для инженеров и разработчиков данных: 

> [!div class="checklist"]
> * Производительность 
> * Устойчивость к копированию
> * Безопасность сети
> * Архитектура высокого уровня решения 
> * Рекомендации по реализации  

## <a name="performance"></a>Производительность

ADF предлагает бессерверную архитектуру, которая обеспечивает параллелизм на разных уровнях, что позволяет разработчикам создавать конвейеры для полного использования пропускной способности сети, а также операций ввода-вывода и пропускной способности хранилища для максимального увеличения пропускной способности перемещения данных для вашей среды. 

Клиенты успешно выполнили миграцию петабайтов данных, состоящих из сотен миллионов файлов из Amazon S3 в хранилище BLOB-объектов Azure, с устойчивой пропускной способностью 2 Гбит/с и выше. 

![производительность](media/data-migration-guidance-s3-to-azure-storage/performance.png)

На рисунке выше показано, как можно достичь оптимальной скорости перемещения данных с помощью различных уровней параллелизма.
 
- Одно действие копирования может воспользоваться преимуществами масштабируемых вычислений: при использовании Azure Integration Runtime можно указать [до 256 диус](https://docs.microsoft.com/azure/data-factory/copy-activity-performance#data-integration-units) для каждого действия копирования на бессерверном сервере. При использовании автономного Integration Runtime можно вручную масштабировать компьютер или масштабировать его на несколько компьютеров ([до 4 узлов](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability)), и одно действие копирования будет секционировать набор файлов на всех узлах. 
- Одно действие копирования считывает из хранилища данных и выполняет запись в него с помощью нескольких потоков. 
- Поток управления ADF может запускать несколько операций копирования параллельно, например, с использованием [циклов for each](https://docs.microsoft.com/azure/data-factory/control-flow-for-each-activity). 

## <a name="resilience"></a>Устойчивость

В рамках одного действия копирования ADF имеет встроенный механизм повторных попыток, чтобы он мог управлять определенным уровнем временных сбоев в хранилищах данных или в базовой сети. 

При выполнении двоичного копирования из S3 в BLOB-объект и из S3 в ADLS 2-го поколения, ADF автоматически выполняет контрольные точки.  Если при выполнении действия копирования произошел сбой или истекло время ожидания, при последующей попытке (убедитесь, что число повторных попыток > 1) начинается с последней точки сбоя, а не с начала. 

## <a name="network-security"></a>Безопасность сети 

По умолчанию ADF передает данные из Amazon S3 в хранилище BLOB-объектов Azure или Azure Data Lake Storage 2-го поколения с помощью зашифрованного подключения по протоколу HTTPS.  HTTPS обеспечивает шифрование данных при передаче и предотвращает атаки перехвата и атак типа "злоумышленник в середине". 

Кроме того, если вы не хотите, чтобы данные передавались через общедоступный Интернет, вы можете повысить уровень безопасности, передавая данные через связь частного пиринга между AWS Direct Connect и Azure Express Route.  Сведения о том, как это можно сделать, см. в описании архитектуры решения. 

## <a name="solution-architecture"></a>Архитектура решения

Перенос данных через общедоступный Интернет:

![решение — архитектура — общедоступная — сеть](media/data-migration-guidance-s3-to-azure-storage/solution-architecture-public-network.png)

- В этой архитектуре данные безопасно передаются по протоколу HTTPS через общедоступный Интернет. 
- Как источник Amazon S3, так и целевое хранилище BLOB-объектов Azure или Azure Data Lake Storage 2-го поколения настроены для разрешения трафика со всех сетевых IP-адресов.  См. вторую архитектуру ниже, в которой можно ограничить сетевой доступ к определенному диапазону IP-адресов. 
- Вы можете легко масштабировать количество лошадиных ресурсов на сервере, чтобы полностью использовать пропускную способность сети и хранилища, чтобы можно было получить лучшую пропускную способность для вашей среды. 
- С помощью этой архитектуры можно достичь первоначальной миграции моментальных снимков и переноса разностных данных. 

Перенос данных по частной ссылке: 

![решение — архитектура — частный — сеть](media/data-migration-guidance-s3-to-azure-storage/solution-architecture-private-network.png)

- В этой архитектуре перенос данных выполняется через связь частного пиринга между AWS Direct Connect и Azure Express Route, так что данные никогда не проходят через общедоступный Интернет.  Для этого требуется использование AWS VPC и виртуальной сети Azure. 
- Для достижения этой архитектуры необходимо установить автономную среду выполнения интеграции ADF на виртуальной машине Windows в виртуальной сети Azure.  Вы можете вручную масштабировать собственные виртуальные машины с локальными ИНФРАКРАСными подключениями или масштабировать их на несколько виртуальных машин (до 4 узлов), чтобы полностью использовать операции ввода-вывода и пропускную способность сети и хранилища. 
- Если вы можете передавать данные по протоколу HTTPS, но вы хотите заблокировать сетевой доступ к исходному S3 для определенного диапазона IP-адресов, можно использовать разновидность этой архитектуры, удалив AWS VPC и заменив частный канал на HTTPS.  Вы хотите поддерживать виртуальную и локальную среду Azure на виртуальной машине Azure, чтобы можно было использовать статический IP-адрес с общедоступной маршрутизацией для целей список разрешений. 
- С помощью этой архитектуры можно достичь первоначальной миграции данных моментальных снимков и переноса разностных данных. 

## <a name="implementation-best-practices"></a>Рекомендации по реализации 

### <a name="authentication-and-credential-management"></a>Проверка подлинности и управление учетными данными 

- Для аутентификации в учетной записи Amazon S3 необходимо использовать [ключ доступа для учетной записи IAM](https://docs.microsoft.com/azure/data-factory/connector-amazon-simple-storage-service#linked-service-properties). 
- Для подключения к хранилищу BLOB-объектов Azure поддерживаются несколько типов проверки подлинности.  Настоятельно рекомендуется использовать [управляемые удостоверения для ресурсов Azure](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#managed-identity) , основанные на автоматически управляемой идентификации ADF в Azure AD. она позволяет настраивать конвейеры без предоставления учетных данных в определении связанной службы.  Кроме того, можно выполнить аутентификацию в хранилище BLOB-объектов Azure с помощью [субъекта-службы](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#service-principal-authentication), [подписанного URL-имени](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#shared-access-signature-authentication) или [ключа учетной записи хранения](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#account-key-authentication). 
- Для подключения к Azure Data Lake Storage 2-го поколения также поддерживаются несколько типов проверки подлинности.  Настоятельно рекомендуется использовать [управляемые удостоверения для ресурсов Azure](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#managed-identity) , хотя также можно использовать [субъект-службу](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#service-principal-authentication) или [ключ учетной записи хранения](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#account-key-authentication) . 
- Если вы не используете управляемые удостоверения для ресурсов Azure, настоятельно рекомендуется [хранить учетные данные в Azure Key Vault](https://docs.microsoft.com/azure/data-factory/store-credentials-in-key-vault) , чтобы упростить централизованное управление и вращение ключей без изменения связанных служб ADF.  Это также один из рекомендаций [по интеграции и откомпакт-диску](https://docs.microsoft.com/azure/data-factory/continuous-integration-deployment#best-practices-for-cicd). 

### <a name="initial-snapshot-data-migration"></a>Исходный моментальный снимок — перенос данных 

Рекомендуется использовать секцию данных, особенно при переносе более 10 ТБ данных.  Чтобы секционировать данные, используйте параметр "префикс" для фильтрации папок и файлов в Amazon S3 по имени, а затем каждое задание копирования ADF может копировать по одной секции за раз.  Для повышения пропускной способности можно одновременно запустить несколько заданий копирования ADF. 

Если какое-либо из заданий копирования завершилось сбоем из – за временной проблемы с сетью или хранилищем данных, можно повторно запустить задание копирования с ошибкой, чтобы снова загрузить эту секцию из AWS S3.  Все остальные задания копирования, загружают другие секции, не будут затронуты. 

### <a name="delta-data-migration"></a>Перенос разностных данных 

Наиболее эффективный способ обнаружения новых или измененных файлов из AWS S3 — использование соглашения об именовании с разделением по времени — если данные в AWS S3 были секционированы с данными временных срезов в имени файла или папки (например,/ИИИИ/мм/дд/филе.КСВ), то Ваш конвейер может легко найти файлы и папки для добавочного копирования. 

Кроме того, если данные в AWS S3 не разделены по времени, ADF может выделять новые или измененные файлы по их LastModifiedDate.   В этом случае ADF будет сканировать все файлы с AWS S3 и копировать только новый и обновленный файл, метка времени последнего изменения которого больше определенного значения.  Имейте в виду, что при наличии большого количества файлов в S3 сканирование исходного файла может занять много времени независимо от того, сколько файлов соответствует условию фильтра.  В этом случае рекомендуется сначала секционировать данные, используя тот же префикс, что и для первоначальной миграции моментальных снимков, чтобы сканирование файла можно было выполнять параллельно.  

### <a name="for-scenarios-that-require-self-hosted-integration-runtime-on-azure-vm"></a>Сценарии, для которых требуется локальная среда выполнения интеграции на виртуальной машине Azure 

Независимо от того, выполняется ли миграция данных по частной ссылке или требуется разрешить конкретный диапазон IP-адресов в брандмауэре Amazon S3, необходимо установить локальную среду выполнения интеграции на виртуальной машине Windows Azure. 

- Рекомендуемая конфигурация для запуска каждой виртуальной машины Azure — Standard_D32s_v3 с 32 виртуальных ЦП и 128 ГБ памяти.  Вы можете отслеживать использование ЦП и памяти виртуальной машины IR во время переноса данных, чтобы узнать, нужно ли дополнительно масштабировать виртуальную машину для повышения производительности или уменьшения масштаба виртуальной машины, чтобы сэкономить затраты. 
- Кроме того, можно выполнить горизонтальное масштабирование, связав до 4 узлов виртуальных машин с одним локальным IR.  Одно задание копирования, выполняемое для автономной среды IR, автоматически разсекционует набор файлов и будет использовать все узлы виртуальных машин для параллельного копирования файлов.  Для обеспечения высокой доступности рекомендуется начинать с 2 узлов виртуальных машин, чтобы избежать единой точки отказа во время переноса данных. 

### <a name="rate-limiting"></a>Ограничение частоты 

Рекомендуется провести оценку производительности с помощью репрезентативного примера набора данных, чтобы можно было определить соответствующий размер секции. 

Начните с одного раздела и одного действия копирования с параметром Диу по умолчанию.  Постепенно увеличивайте значение параметра Диу, пока не будет достигнуто ограничение пропускной способности сети или операций ввода-вывода или пропускной способности хранилища данных, либо достигнуто максимальное 256 Диу, разрешенное для одного действия копирования. 

Затем постепенно увеличивайте количество одновременных операций копирования, пока не будут достигнуты ограничения вашей среды. 

При возникновении ошибок регулирования, о которых сообщило действие копирования ADF, уменьшите значение параметра Concurrency или Диу в ADF или увеличьте ограничения пропускной способности и операций ввода-вывода для сети и хранилищ данных.  

### <a name="estimating-price"></a>Оценка цены 

> [!NOTE]
> Это гипотетический пример цены.  Реальная цена зависит от фактической пропускной способности в вашей среде.

Рассмотрим следующий конвейер, созданный для переноса данных из S3 в хранилище BLOB-объектов Azure: 

![цены — конвейер](media/data-migration-guidance-s3-to-azure-storage/pricing-pipeline.png)

Предположим, что мы предполагаем следующее: 

- Общий объем данных составляет 2 ПБ 
- Перенос данных по протоколу HTTPS с помощью первой архитектуры решения 
- 2 PB делится на 1 КБ, и каждая копия перемещается в одну секцию 
- Для каждой копии настраивается Диу = 256 и достигается пропускная способность 1 Гбит/с. 
- Параметр параллелизма ForEach имеет значение 2, а суммарная пропускная способность — 2 Гбит/с 
- В итоге для завершения миграции потребуется 292 часов 

Ниже приведена Оценочная цена, основанная на указанных выше допущениях. 

![цены — таблица](media/data-migration-guidance-s3-to-azure-storage/pricing-table.png)


### <a name="additional-references"></a>Дополнительные ссылки 
- [Соединитель простой службы хранилища Amazon](https://docs.microsoft.com/azure/data-factory/connector-amazon-simple-storage-service)
- [Соединитель хранилища BLOB-объектов Azure](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage)
- [Copy data to or from Azure Data Lake Storage Gen2 Preview using Azure Data Factory (Preview)](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage) (Копирование данных в Azure Data Lake Storage Gen2 (предварительная версия) или из него с помощью фабрики данных Azure)
- [Краткое руководств по настройке производительности действий копирования](https://docs.microsoft.com/azure/data-factory/copy-activity-performance)
- [Создание и Настройка автономных Integration Runtime](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime)
- [Высокая доступность и масштабируемость локальной среды выполнения интеграции](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability)
- [Вопросы безопасности перемещения данных](https://docs.microsoft.com/azure/data-factory/data-movement-security-considerations)
- [Хранение учетных данных в Azure Key Vault](https://docs.microsoft.com/azure/data-factory/store-credentials-in-key-vault)
- [Копировать файл добавочно в соответствии с временным секционированным именем файла](https://docs.microsoft.com/azure/data-factory/tutorial-incremental-copy-partitioned-file-name-copy-data-tool)
- [Копировать новые и измененные файлы на основе LastModifiedDate](https://docs.microsoft.com/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool)
- [Страница с ценами на ADF](https://azure.microsoft.com/pricing/details/data-factory/data-pipeline/)

## <a name="next-steps"></a>Следующие шаги

- [Копирование файлов из нескольких контейнеров с помощью фабрики данных Azure](solution-template-copy-files-multiple-containers.md)