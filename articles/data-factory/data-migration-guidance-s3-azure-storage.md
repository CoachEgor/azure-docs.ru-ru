---
title: Миграция данных из Amazon S3 в службу хранилища Azure
description: Используйте фабрику данных Azure для переноса данных из Amazon S3 в Хранилище Azure.
services: data-factory
ms.author: yexu
author: dearandyxu
ms.reviewer: ''
manager: shwang
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 8/04/2019
ms.openlocfilehash: 3f40ad7346219b48a38ade38b2a75ddf71940875
ms.sourcegitcommit: b80aafd2c71d7366838811e92bd234ddbab507b6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/16/2020
ms.locfileid: "81416419"
---
# <a name="use-azure-data-factory-to-migrate-data-from-amazon-s3-to-azure-storage"></a>Используйте фабрику данных Azure для переноса данных из Amazon S3 в хранилище Azure 

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

Azure Data Factory предоставляет эффективный, надежный и экономичный механизм для переноса данных в масштабе с Amazon S3 в Azure Blob Storage или Azure Data Lake Storage Gen2.  В этой статье содержится следующая информация для разработчиков данных и разработчиков: 

> [!div class="checklist"]
> * Производительность 
> * Устойчивость копирования
> * Безопасность сети
> * Архитектура решений высокого уровня 
> * Внедрение передовой практики  

## <a name="performance"></a>Производительность

ADF предлагает бессерверную архитектуру, которая позволяет параллелизм на различных уровнях, что позволяет разработчикам создавать конвейеры, чтобы в полной мере использовать пропускную способность сети, а также память IOPS и пропускную способность для максимизации пропускной способности движения данных для вашей среды. 

Клиенты успешно мигрировали с петабайтами данных, состоящих из сотен миллионов файлов с Amazon S3 в Azure Blob Storage, с постоянной пропускной стоимостью 2 GBpи и выше. 

![производительность](media/data-migration-guidance-s3-to-azure-storage/performance.png)

На рисунке выше показано, как можно достичь больших скоростей движения данных через различные уровни параллелизма:
 
- Одноразовая копия может использовать масштабируемые вычислительные ресурсы: при использовании Azure Integration Runtime можно указать [до 256 DI-образных емких](https://docs.microsoft.com/azure/data-factory/copy-activity-performance#data-integration-units) данных для каждой деятельности копирования без сервера; при использовании автономной интеграции Runtime, вы можете вручную масштабировать машину или масштабировать до нескольких машин[(до 4 узлов),](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability)и одна копия деятельности будет разделить его файл, установленный на всех узлов. 
- Одноразовая копия действия читается и записывается в хранилище данных с помощью нескольких потоков. 
- Поток управления ADF может запускать несколько действий копирования параллельно, например, используя [для каждого цикла.](https://docs.microsoft.com/azure/data-factory/control-flow-for-each-activity) 

## <a name="resilience"></a>Устойчивость

В рамках выполнения одной копирования ADF имеет встроенный механизм повторной работы, чтобы он мог обрабатывать определенный уровень временных сбоев в хранилищах данных или в базовой сети. 

При копировании двоичного копирования от S3 до Blob и от S3 до ADLS Gen2 ADF автоматически выполняет контрольные точки.  Если запуск действия копирования не удалось или приурочено, при последующей повторной попытке копия возобновляется с последней точки сбоя вместо начала. 

## <a name="network-security"></a>Безопасность сети 

По умолчанию ADF передает данные с Amazon S3 в Azure Blob Storage или Azure Data Lake Storage Gen2 с помощью зашифрованного соединения по протоколу HTTPS.  HTTPS обеспечивает шифрование данных в пути и предотвращает подслушивание и атаки «человек в середине». 

Кроме того, если вы не хотите, чтобы данные передавались через общедоступный Интернет, вы можете достичь более высокой безопасности, передавая данные по частной пиринговой связи между AWS Direct Connect и Azure Express Route.  Обратитесь к архитектуре решения ниже о том, как это может быть достигнуто. 

## <a name="solution-architecture"></a>Архитектура решения

Миграция данных через общедоступный Интернет:

![решение-архитектура-общественная сеть](media/data-migration-guidance-s3-to-azure-storage/solution-architecture-public-network.png)

- В этой архитектуре данные передаются надежно с помощью HTTPS через общедоступный Интернет. 
- Как источник Amazon S3, так и пункт назначения Azure Blob Storage или Azure Data Lake Storage Gen2 настроены для обеспечения трафика со всех IP-адресов сети.  Обратитесь ко второй архитектуре ниже о том, как можно ограничить доступ к сети к определенному диапазону IP. 
- Вы можете легко увеличить количество лошадиных сил без сервера, чтобы в полной мере использовать вашу сеть и пропускную способность хранения, так что вы можете получить лучшую пропускную способность для вашей среды. 
- С помощью этой архитектуры можно достичь как первоначальной миграции моментальных снимков, так и миграции дельта-данных. 

Миграция данных по частной ссылке: 

![решение-архитектура-частная сеть](media/data-migration-guidance-s3-to-azure-storage/solution-architecture-private-network.png)

- В этой архитектуре миграция данных осуществляется через приватную одноранговую связь между AWS Direct Connect и Azure Express Route таким образом, чтобы данные никогда не пересекаются через общедоступный Интернет.  Это требует использования AWS VPC и виртуальной сети Azure. 
- Для достижения этой архитектуры необходимо установить самоорганизованную интеграцию ADF на Windows VM в виртуальной сети Azure.  Вы можете вручную масштабировать свои самоуправляемые ИК-вМ или масштабировать до нескольких VMS (до 4 узлов), чтобы в полной мере использовать вашу сеть и память IOPS/пропускную способность. 
- Если передача данных по HTTPS допустима, но вы хотите заблокировать доступ к сети исходного S3 в определенном диапазоне IP, вы можете принять вариацию этой архитектуры, удалив AWS VPC и заменив частную ссылку на HTTPS.  Вы хотите сохранить Azure Virtual и самостоятельно размещенных ИК на Azure VM, так что вы можете иметь статический публично rutable IP для белого списка цели. 
- С помощью этой архитектуры можно достичь как первоначальной миграции данных моментального снимка, так и миграции дельта- данных. 

## <a name="implementation-best-practices"></a>Внедрение передовой практики 

### <a name="authentication-and-credential-management"></a>Аутентификация и управление учетных данных 

- Чтобы проверить подлинность учетной записи Amazon S3, необходимо использовать [ключ доступа для учетной записи IAM.](https://docs.microsoft.com/azure/data-factory/connector-amazon-simple-storage-service#linked-service-properties) 
- Для подключения к хранилищу Azure Blob поддерживается несколько типов аутентификации.  Настоятельно рекомендуется использовать [управляемые идентификаторы для ресурсов Azure:](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#managed-identity) построенный поверх автоматической идентификации ADF в Azure AD, он позволяет настраивать конвейеры без предоставления учетных данных в определении Linked Service.  Кроме того, можно проверить подлинность в Хранилище Azure Blob, используя [принцип обслуживания,](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#service-principal-authentication) [общую подпись доступа](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#shared-access-signature-authentication)или ключ [учетной записи хранилища.](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#account-key-authentication) 
- Несколько типов аутентификации также поддерживаются для подключения к Azure Data Lake Storage Gen2.  Настоятельно рекомендуется использовать [управляемые идентификаторы для ресурсов Azure,](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#managed-identity) хотя также можно использовать [основной учетный](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#service-principal-authentication) записи службы или [ключ учетной записи.](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#account-key-authentication) 
- При неиспользовании управляемых идентификаторов для ресурсов Azure рекомендуется [хранить учетные данные в Azure Key Vault,](https://docs.microsoft.com/azure/data-factory/store-credentials-in-key-vault) чтобы упростить централизованное управление и изменение ключей без изменения связанных с ADF служб.  Это также одна из [лучших практик для CI/ CD](https://docs.microsoft.com/azure/data-factory/continuous-integration-deployment#best-practices-for-cicd). 

### <a name="initial-snapshot-data-migration"></a>Первоначальная миграция данных моментального снимка 

Раздел данных рекомендуется особенно при миграции более 100 ТБ данных.  Чтобы развести данные, используйте настройку "префикс" для фильтрации папок и файлов в Amazon S3 по имени, а затем каждое задание копии ADF может копировать одну перегородку за один раз.  Вы можете запустить несколько заданий копирования ADF одновременно для лучшей пропускной всей входной. 

Если какая-либо из заданий копирования не сбой из-за сети или хранения переходных проблем, вы можете перезапустить неудавшую работу копирования для перезагрузки этого конкретного раздела снова от AWS S3.  Все остальные задания копирования, загружаемые другими разделами, не будут затронуты. 

### <a name="delta-data-migration"></a>Миграция данных Delta 

Наиболее эффективный способ идентификации новых или измененных файлов из AWS S3 — это использование соглашения о временном разделении именования — когда ваши данные в AWS S3 были разделены во времени с информацией о срезах времени в файле или имени папки (например, /yyyy/mm/dd/file.csv), то ваш конвейер может легко определить, какие файлы/папки могут постепенно копировать. 

Кроме того, если ваши данные в AWS S3 не разделены во времени, ADF может идентифицировать новые или измененные файлы по их LastModifiedDate.   Как это работает, что ADF будет сканировать все файлы из AWS S3, и только копировать новый и обновленный файл, последний измененный метки времени больше, чем определенное значение.  Имейте в виду, что если у вас есть большое количество файлов в S3, первоначальное сканирование файлов может занять много времени, независимо от того, сколько файлов соответствует состоянию фильтра.  В этом случае сначала предлагается развести данные, используя ту же настройку "префикса" для первоначальной миграции моментального снимка, чтобы сканирование файлов происходило параллельно.  

### <a name="for-scenarios-that-require-self-hosted-integration-runtime-on-azure-vm"></a>Для сценариев, требующих самостоятельной интеграции времени выполнения на Azure VM 

Независимо от того, мигрируете ли вы данные по частной ссылке или вы хотите разрешить определенный диапазон IP-адресов в брандмауэре Amazon S3, необходимо установить самохлаженное время выполнения интеграции на Azure Windows VM. 

- Рекомендуемая конфигурация для каждого Azure VM Standard_D32s_v3 с 32 vCPU и 128-ГБ памяти.  Вы можете продолжать мониторинг процессора и использования памяти IR VM во время миграции данных, чтобы увидеть, если вам нужно дальнейшее масштабирование VM для повышения производительности или масштабирования VM, чтобы сэкономить затраты. 
- Вы также можете масштабироваться, связывая до 4 Узлов VM с одним самоуправляемым ИК.  Одноразовая работа, работающее против самостоятельно гостоуправляемого ИК, автоматически разведет набор файлов и использует все vM-узлы для параллельного копирования файлов.  При высокой доступности рекомендуется начинать с 2 узлов VM, чтобы избежать одной точки сбоя во время миграции данных. 

### <a name="rate-limiting"></a>Ограничение частоты 

В качестве наилучшей практики, провести производительность POC с репрезентативным набором данных образца, так что вы можете определить соответствующий размер раздела. 

Начните с одной раздела и одноразовой копии с настройками DIU по умолчанию.  Постепенно увеличивайте настройки DIU до тех пор, пока вы не достигнете предела пропускной способности вашей сети или ioPS/лимит амплуа ширины данных, или вы достигли максимума 256 DIU, разрешенного на одной активности копии. 

Далее, постепенно увеличивайте количество параллельных действий копирования, пока вы не достигнете пределов вашей среды. 

При возникновении ошибок регулирования, сообщаемых активностью копирования ADF, либо уменьшите настройки параллелизма или DIU в ADF, либо рассмотрите возможность увеличения предельных масштабов пропускной способности/IOPS сети и хранилищ данных.  

### <a name="estimating-price"></a>Оценка цены 

> [!NOTE]
> Это гипотетический пример ценообразования.  Фактические цены зависят от фактической пропускной выхаживаемостью в вашей среде.

Рассмотрим следующий конвейер, построенный для миграции данных из S3 в Хранилище Azure Blob: 

![ценообразование-конвейер](media/data-migration-guidance-s3-to-azure-storage/pricing-pipeline.png)

Предположим следующее: 

- Общий объем данных составляет 2 ПБ 
- Миграция данных по HTTPS с использованием архитектуры первого решения 
- 2 ПБ делится на 1 K перегородки и каждая копия перемещает один раздел 
- Каждая копия настроена с DIU-256 и достигает пропускной четыржей 1 GBps 
- ForEach параллел- установлен на 2, а совокупная пропускная стоимость составляет 2 GBps 
- В общей сложности для завершения 

Вот оценочная цена, основанная на вышеуказанных предположениях: 

![таблица ценообразования](media/data-migration-guidance-s3-to-azure-storage/pricing-table.png)

### <a name="additional-references"></a>Дополнительные ссылки 
- [Разъем службы простого хранения Amazon](https://docs.microsoft.com/azure/data-factory/connector-amazon-simple-storage-service)
- [Соединитель хранилища BLOB-объектов Azure](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage)
- [Copy data to or from Azure Data Lake Storage Gen2 Preview using Azure Data Factory (Preview)](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage) (Копирование данных в Azure Data Lake Storage Gen2 (предварительная версия) или из него с помощью фабрики данных Azure)
- [Копирование руководства по настройке производительности](https://docs.microsoft.com/azure/data-factory/copy-activity-performance)
- [Создание и настройка автономной интеграции Runtime](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime)
- [Самохонугированная интеграция времени выполнения HA и масштабируемость](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability)
- [Вопросы безопасности при перемещении данных](https://docs.microsoft.com/azure/data-factory/data-movement-security-considerations)
- [Хранение учетных данных в Azure Key Vault](https://docs.microsoft.com/azure/data-factory/store-credentials-in-key-vault)
- [Копирование файла постепенно на основе времени разделенного имени файла](https://docs.microsoft.com/azure/data-factory/tutorial-incremental-copy-partitioned-file-name-copy-data-tool)
- [Копирование новых и измененных файлов на основе LastModifiedDate](https://docs.microsoft.com/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool)
- [Страница ценообразования ADF](https://azure.microsoft.com/pricing/details/data-factory/data-pipeline/)

## <a name="template"></a>Шаблон

Вот [шаблон,](solution-template-migration-s3-azure.md) чтобы начать с мигрировать петабайт данных, состоящий из сотен миллионов файлов из Amazon S3 в Azure Data Lake Storage Gen2.

## <a name="next-steps"></a>Дальнейшие действия

- [Копирование файлов из нескольких контейнеров с помощью Фабрики данных Azure](solution-template-copy-files-multiple-containers.md)
