---
title: Инструмент данных для копирования новых и обновленных файлов постепенно
description: Создайте фабрику данных Azure, а затем используйте инструмент Copy Data для постепенной загрузки новых файлов на основе LastModifiedDate.
services: data-factory
author: dearandyxu
ms.author: yexu
ms.reviewer: ''
manager: ''
ms.service: data-factory
ms.workload: data-services
ms.devlang: na
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 3/18/2020
ms.openlocfilehash: 743f9dd8f7998178a75d716f4da22efee2b3bc79
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "80131079"
---
# <a name="incrementally-copy-new-and-changed-files-based-on-lastmodifieddate-by-using-the-copy-data-tool"></a>Постепенно копируйте новые и измененные файлы на основе LastModifiedDate с помощью инструмента Copy Data

В этом уроке вы будете использовать портал Azure для создания фабрики данных. Затем вы будете использовать инструмент Copy Data для создания конвейера, который постепенно копирует только новые и измененные файлы, основываясь на их **lastModifiedDate** от хранилища Azure Blob до хранения Azure Blob.

Поступая таким образом, ADF будет сканировать все файлы из исходного магазина, применять фильтр файлов на их LastModifiedDate, и скопировать новый и обновленный файл только с последнего раза в магазин назначения.  Пожалуйста, обратите внимание, что если вы позволите ADF сканировать огромное количество файлов, но только скопировать несколько файлов к месту назначения, вы все равно ожидаете длительность из-за сканирования файлов занимает много времени, а также.   

> [!NOTE]
> Если вы еще не работали с фабрикой данных Azure, ознакомьтесь со статьей [Введение в фабрику данных Azure](introduction.md).

В этом учебнике рассматриваются следующие задачи:

> [!div class="checklist"]
> * Создали фабрику данных.
> * Создание конвейера с помощью средства копирования данных.
> * Мониторинг конвейера и выполнения действий.

## <a name="prerequisites"></a>Предварительные требования

* **Подписка Azure:** Если у вас нет подписки Нацу, создайте [бесплатную учетную запись](https://azure.microsoft.com/free/) перед началом.
* **Учетная запись хранения Azure**: Используйте хранилище Blob в качестве _хранилища исходных_ данных и _данных раковины._ Если у вас нет учетной записи хранения Azure, см. инструкции по [ее созданию](../storage/common/storage-account-create.md).

### <a name="create-two-containers-in-blob-storage"></a>Создание двух контейнеров в хранилище Blob

Подготовьте хранилище Blob для учебника, выполнив эти шаги.

1. Создайте источник названного **источника**контейнера. Для выполнения этой задачи можно использовать различные инструменты, например [Azure Storage Explorer.](https://storageexplorer.com/)

2. Создайте контейнер с именем **назначения.**

## <a name="create-a-data-factory"></a>Создание фабрики данных

1. В левом меню выберите **Создать ресурс** > **Данные и фабрику** > **аналитических данных:**

   ![Выбор фабрики данных в области "Создать"](./media/doc-common-process/new-azure-data-factory-menu.png)

2. На странице **Новая фабрика данных** в поле **Имя** введите **ADFTutorialDataFactory**.

   Имя фабрики данных должно быть _глобально уникальным_. Вы можете получить следующее сообщение об ошибке.

   ![Сообщение об ошибке фабрики данных](./media/doc-common-process/name-not-available-error.png)

   Если вы увидите следующую ошибку касательно значения имени, введите другое имя фабрики данных. Например,**_ваше_имя_****ADFTutorialDataFactory**. Правила именования артефактов службы "Фабрика данных" см. в [этой](naming-rules.md) статье.
3. Выберите **подписку** Azure, в которой вы создадите новую фабрику данных.
4. Для **группы ресурсов** выполните одно из следующих действий:

    * Выберите **«Используйте существующую** и выберите существующую группу ресурсов» из списка выпадающих.

    * Выберите **Создать новое** и введите имя группы ресурсов. 
         
    Сведения о группах ресурсов см. в статье [Общие сведения об Azure Resource Manager](../azure-resource-manager/management/overview.md).

5. В **версии**, выберите **V2**.
6. В **соответствии с местоположением**выберите место для фабрики данных. В раскрывающемся списке отображаются только поддерживаемые расположения. Хранилища данных (например, база данных Azure Storage и База данных S'L) и вычисления (например, Azure HDInsight), которые использует фабрика данных, могут находиться в других местах и регионах.
8. Выберите **Создать**.
9. Когда создание завершится, откроется домашняя страница **Фабрика данных**.
10. Чтобы открыть пользовательский интерфейс (UI) Azure Data Factory на отдельной вкладке, выберите плитку **Author & Monitor.**

    ![Домашняя страница фабрики данных](./media/doc-common-process/data-factory-home-page.png)

## <a name="use-the-copy-data-tool-to-create-a-pipeline"></a>Создание конвейера с помощью средства копирования данных

1. На странице **«Давайте начнем»** выберите заголовок **данных копирования,** чтобы открыть инструмент Copy Data.

   ![Плитка средства копирования данных](./media/doc-common-process/get-started-page.png)

2. На странице **Properties** следующие шаги:

    а. Под **названием задачи**введите **DeltaCopyFromBlobPipeline**.

    b. В соответствии **с расписанием каденции задачи** или **задачами,** выберите **Выполнить регулярно по расписанию.**

    c. Под **типом триггера**, выберите **Tumbling окно**.

    d. Под **повторением,** введите **15 минут (ы)**.

    д) Нажмите кнопку **Далее**.

    Пользовательский интерфейс фабрики данных создаст конвейер с указанным именем задачи.

    ![Страница свойств](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/copy-data-tool-properties-page.png)

3. На странице **Исходное хранилище данных** сделайте следующее:

    а. Выберите **: Создайте новое соединение,** чтобы добавить соединение.

    b. В коллекции выберите **Хранилище BLOB-объектов Azure** и щелкните **Продолжить**.

    ![Страница исходного хранилища данных](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/source-data-store-page-select-blob.png)

    c. На странице **New Linked Service (Azure Blob Storage)** выберите учетную запись хранения из списка **имен учетной записи хранилища.** Тест подключение, а затем выберите **Создать**.

    d. Выберите вновь созданную связанную службу, а затем выберите **Следующий**.

   ![Страница исходного хранилища данных](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/source-data-store-page-select-linkedservice.png)

4. На странице **Choose the input file or folder** (Выбор входного файла или папки) выполните следующие действия:

    а. Просмотрите и выберите **исходную** папку, а затем выберите **выберите.**

    ![Выбор файла или папки входных данных](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/choose-input-file-folder.png)

    b. Под **поведением загрузки файлов**выберите **инкрементную нагрузку: LastModifiedDate**.

    c. Проверьте **двоичную копию** и выберите **Следующую**.

     ![Выбор файла или папки входных данных](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/check-binary-copy.png)

5. На странице **хранилища данных Destination** выберите созданный **вами AzureBlobStorage.** Это та же учетная запись хранения, что и хранилище исходных данных. Затем выберите **Следующий**.

6. На странице **Choose the output file or folder** (Выбор целевого файла или папки) выполните следующие действия:

    а. Просмотрите и выберите папку **назначения,** а затем выберите **выберите.**

    ![Выбор целевого файла или папки](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/choose-output-file-folder.png)

    b. Нажмите кнопку **Далее**.

7. На странице **Параметры** выберите **Далее**.

8. На странице **Резюме** просмотрите настройки, а затем выберите **следующий**.

    ![Страница "Сводка"](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/summary-page.png)

9. На **странице развертывания** выберите **Мониторинг**, чтобы отслеживать созданный конвейер (задачу).

    ![Страница развертывания](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/deployment-page.png)

10. Обратите внимание, что слева автоматически выбирается вкладка **Мониторинг**. Приложение переключается на вкладку **Монитор.** Вы видите состояние конвейера. Щелкните **Обновить**, чтобы обновить список. Нажмите на ссылку под **PIPELINE NAME,** чтобы просмотреть сведения о запуске активности или повторно запустить конвейер. 

    ![Обновить список и выбрать просмотр запусков действий](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/monitor-pipeline-runs1.png)

11. В конвейере есть только одно действие (активность копирования), так что вы видите только одну запись. Для получения подробной информации об операции копирования выберите **ссылку «Подробности»** (значок очков) под столбцом **ACTIVITY NAME.** Дополнительные сведения о свойствах см. в [обзоре действия копирования](copy-activity-overview.md). 

    ![Копирование деятельности находится в стадии разработки](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/monitor-pipeline-runs2.png)

    Поскольку в **исходном** контейнере в вашей учетной записи хранилища Blob нет файла, вы не увидите файл, скопированный в контейнер **назначения** в вашей учетной записи хранения Blob.

    ![Нет файла в исходном контейнере или контейнере назначения](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/monitor-pipeline-runs3.png)

12. Создайте пустой текстовый файл и назовите его **file1.txt.** Загрузите этот текстовый файл в **исходный** контейнер в учетной записи хранилища. Это можно сделать при помощи таких средств, как [обозреватель службы хранилища Azure](https://storageexplorer.com/).

    ![Создание file1.txt и загрузка в исходный контейнер](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/monitor-pipeline-runs3-1.png)

13. Чтобы вернуться к представлению **Pipeline Runs,** выберите **все запуски конвейеров**и дождитесь автоматического срабатывания одного и того же конвейера.  

    ![Выберите все запуски трубопроводов](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/monitor-pipeline-runs4.png)

14. Когда второй запуск конвейера завершается, выполните те же шаги, которые упоминались выше, чтобы просмотреть детали выполнения действия.  

    Вы увидите, что один файл (file1.txt) был скопирован из **исходного** контейнера в контейнер **назначения** вашего счета хранения Blob.

    ![File1.txt был скопирован из контейнера исходного кода в контейнер назначения](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/monitor-pipeline-runs6.png)

15. Создайте еще один пустой текстовый файл и назовите его **file2.txt.** Загрузите этот текстовый файл в **исходный** контейнер в вашей учетной записи хранения Blob.

16. Повторите шаги 13 и 14 для этого второго текстового файла. Вы увидите, что только новый файл (file2.txt) был скопирован из **исходного** контейнера в контейнер **назначения** вашего счета хранения в следующем запуске конвейера.  

    Вы также можете проверить это с помощью [Azure Storage Explorer](https://storageexplorer.com/) для сканирования файлов.

    ![Сканирование файлов с помощью Azure Storage Explorer](./media/tutorial-incremental-copy-lastmodified-copy-data-tool/monitor-pipeline-runs8.png)


## <a name="next-steps"></a>Дальнейшие действия
Перейти к следующему учебнику, чтобы узнать о преобразовании данных с помощью кластера Apache Spark на Azure:

> [!div class="nextstepaction"]
>[Преобразование данных в облаке с помощью кластера Apache Spark](tutorial-transform-data-spark-portal.md)
