---
title: Действие потока данных
description: Выполнение потоков данных внутри конвейера фабрики данных.
services: data-factory
documentationcenter: ''
author: kromerm
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.author: makromer
ms.date: 01/02/2020
ms.openlocfilehash: d0b9c59852175b91b4bf799a366ae5124fa0ae42
ms.sourcegitcommit: f788bc6bc524516f186386376ca6651ce80f334d
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/03/2020
ms.locfileid: "75644802"
---
# <a name="data-flow-activity-in-azure-data-factory"></a>Действие потока данных в фабрике данных Azure

Используйте действие потока данных для преобразования и перемещения данных посредством сопоставления потоков данных. Если вы не знакомы с потоками данных, см. раздел [Общие сведения о сопоставлении потока данных](concepts-data-flow-overview.md)

## <a name="syntax"></a>Синтаксис

```json
{
    "name": "MyDataFlowActivity",
    "type": "ExecuteDataFlow",
    "typeProperties": {
      "dataflow": {
         "referenceName": "MyDataFlow",
         "type": "DataFlowReference"
      },
      "compute": {
         "coreCount": 8,
         "computeType": "General"
      },
      "staging": {
          "linkedService": {
              "referenceName": "MyStagingLinkedService",
              "type": "LinkedServiceReference"
          },
          "folderPath": "my-container/my-folder"
      },
      "integrationRuntime": {
          "referenceName": "MyDataFlowIntegrationRuntime",
          "type": "IntegrationRuntimeReference"
      }
}

```

## <a name="type-properties"></a>Свойства типа

Свойство | Description | Допустимые значения | Обязательно для заполнения
-------- | ----------- | -------------- | --------
DataFlow | Ссылка на выполняемый поток данных | датафловреференце | Да
интегратионрунтиме | Среда вычислений, в которой выполняется поток данных. Если не указано, будет использоваться автоматическая разрешающая среда выполнения интеграции Azure. | интегратионрунтимереференце | Нет
COMPUTE. Корекаунт | Число ядер, используемых в кластере Spark. Можно указать, только если используется среда выполнения интеграции Azure с авторазрешением | 8, 16, 32, 48, 80, 144, 272 | Нет
COMPUTE. Компутетипе | Тип вычислений, используемых в кластере Spark. Можно указать, только если используется среда выполнения интеграции Azure с авторазрешением | "General", "Компутеоптимизед", "MemoryOptimized" | Нет
промежуточное хранение. linkedService | Если вы используете источник или приемник хранилища данных SQL, то учетная запись хранения, используемая для промежуточного хранения Polybase. | LinkedServiceReference | Только при считывании или записи потока данных в хранилище SQL
промежуточное хранение. folderPath | Если вы используете источник или приемник хранилища данных SQL, путь к папке в учетной записи хранения BLOB-объектов, используемый для промежуточного хранения Polybase. | String | Только при считывании или записи потока данных в хранилище SQL

![Выполнение потока данных](media/data-flow/activity-data-flow.png "Выполнение потока данных")

### <a name="data-flow-integration-runtime"></a>Среда выполнения интеграции потока данных

Выберите Integration Runtime, который будет использоваться для выполнения действия потока данных. По умолчанию в фабрике данных используется автоматическая разрешающая среда выполнения интеграции Azure с четырьмя рабочими ядрами и без срока жизни (TTL). Этот IR имеет тип вычислений общего назначения и работает в том же регионе, что и фабрика. Вы можете создавать собственные среды выполнения интеграции Azure, определяющие конкретные регионы, тип вычислений, количество ядер и TTL для выполнения действий потока данных.

Для выполнения конвейера кластером является кластер заданий, для запуска которого требуется несколько минут перед началом выполнения. Если TTL не указан, это время запуска необходимо для каждого выполнения конвейера. Если указать срок жизни, горячий пул кластера останется активным в течение времени, указанного после последнего выполнения, что приведет к сокращению времени запуска. Например, если срок жизни составляет 60 минут и поток данных будет выполняться один раз в час, пул кластера останется активным. Дополнительные сведения см. в разделе [Среда выполнения интеграции Azure](concepts-integration-runtime.md).

![Azure Integration Runtime](media/data-flow/ir-new.png "Azure Integration Runtime")

> [!NOTE]
> Выбор Integration Runtime в действии потока данных применяется только к *запущенным выполнениям* конвейера. Отладка конвейера с потоками данных выполняется в кластере, указанном в сеансе отладки.

### <a name="polybase"></a>PolyBase

Если вы используете хранилище данных SQL Azure в качестве приемника или источника, необходимо выбрать промежуточное расположение для пакетной загрузки Polybase. Polybase обеспечивает неполное пакетную загрузку, а не загружает данные построчно. Polybase радикально сокращает время загрузки в хранилище данных SQL.

## <a name="parameterizing-data-flows"></a>Параметризация потоков данных

### <a name="parameterized-datasets"></a>Параметризованные наборы данных

Если в потоке данных используются параметризованные DataSet, задайте значения параметров на вкладке **Параметры** .

![Выполнение параметров потока данных](media/data-flow/params.png "Параметры")

### <a name="parameterized-data-flows"></a>Параметризованные потоки данных

Если поток данных является параметризованным, задайте динамические значения параметров потока данных на вкладке **Параметры** . Для назначения динамических или литеральных значений параметров можно использовать язык выражений конвейера ADF (только для строковых типов) или язык выражений потока данных. Дополнительные сведения см. в разделе [Параметры потока данных](parameters-data-flow.md).

![Пример выполнения параметра потока данных](media/data-flow/parameter-example.png "Пример параметра")

### <a name="parameterized-compute-properties"></a>Параметризованные свойства вычислений.

Вы можете параметризовать количество ядер или тип вычислений, если вы используете автоматическую разрешающую среду выполнения интеграции Azure и указали значения для COMPUTE. Корекаунт и COMPUTE. Компутетипе.

![Пример выполнения параметра потока данных](media/data-flow/parameterize-compute.png "Пример параметра")

## <a name="pipeline-debug-of-data-flow-activity"></a>Отладка конвейера действия потока данных

Для выполнения конвейера отладки, выполняемого с действием потока данных, необходимо переключиться в режим отладки потока данных с помощью ползунка **отладки потока данных** на верхней панели. Режим отладки позволяет запускать поток данных в активном кластере Spark. Дополнительные сведения см. в разделе [режим отладки](concepts-data-flow-debug-mode.md).

![Кнопка "Отладка"](media/data-flow/debugbutton.png "Кнопка отладки")

Конвейер отладки выполняется для активного кластера отладки, а не для среды выполнения интеграции, указанной в параметрах действия потока данных. При запуске режима отладки можно выбрать среду вычислений для отладки.

## <a name="monitoring-the-data-flow-activity"></a>Наблюдение за действием потока данных

В действии потока данных предусмотрена специальная процедура мониторинга, позволяющая просматривать сведения о секционировании, времени этапа и преобразовании данных. Откройте панель "Мониторинг" с помощью значка очков в разделе " **действия**". Дополнительные сведения см. в разделе [наблюдение за потоками данных](concepts-data-flow-monitoring.md).

### <a name="use-data-flow-activity-results-in-a-subsequent-activity"></a>Использование действия потока данных приводит к последующему действию

Действие потока данных выводит метрики, касающиеся количества строк, записанных в каждый приемник, и строк, считываемых из каждого источника. Эти результаты возвращаются в разделе `output` результата выполнения действия. Возвращаемые метрики имеют формат ниже JSON.

``` json
{
    "runStatus": {
        "metrics": {
            "<your sink name1>": {
                "rowsWritten": <number of rows written>,
                "sinkProcessingTime": <sink processing time in ms>,
                "sources": {
                    "<your source name1>": {
                        "rowsRead": <number of rows read>
                    },
                    "<your source name2>": {
                        "rowsRead": <number of rows read>
                    },
                    ...
                }
            },
            "<your sink name2>": {
                ...
            },
            ...
        }
    }
}
```

Например, чтобы получить число строк, записанных в приемник с именем "sink1" в действии с именем "Датафловактивити", используйте `@activity('dataflowActivity').output.runStatus.metrics.sink1.rowsWritten`.

Чтобы получить число строк, считанных из источника с именем source1, который использовался в этом приемнике, используйте `@activity('dataflowActivity').output.runStatus.metrics.sink1.sources.source1.rowsRead`.

> [!NOTE]
> Если в приемнике записано ноль строк, он не будет отображаться в метриках. Существование можно проверить с помощью функции `contains`. Например, `contains(activity('dataflowActivity').output.runStatus.metrics, 'sink1')` проверит, были ли строки записаны в sink1.

## <a name="next-steps"></a>Дальнейшие действия

См. раздел действия потока управления, поддерживаемые фабрикой данных. 

- [Действие условия If](control-flow-if-condition-activity.md)
- [Действие выполнения конвейера](control-flow-execute-pipeline-activity.md)
- [Действие ForEach](control-flow-for-each-activity.md)
- [Действие получения метаданных](control-flow-get-metadata-activity.md)
- [Действие поиска](control-flow-lookup-activity.md)
- [Веб-действие](control-flow-web-activity.md)
- [Действие Until](control-flow-until-activity.md)
