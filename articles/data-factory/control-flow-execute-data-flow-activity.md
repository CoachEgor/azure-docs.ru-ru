---
title: Активность потока данных
description: Как выполнять потоки данных из конвейера фабрики данных.
services: data-factory
documentationcenter: ''
author: kromerm
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.author: makromer
ms.date: 03/16/2020
ms.openlocfilehash: 32088dd712cd0c70fc01de48add17a0b6a828dc8
ms.sourcegitcommit: b80aafd2c71d7366838811e92bd234ddbab507b6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/16/2020
ms.locfileid: "81415328"
---
# <a name="data-flow-activity-in-azure-data-factory"></a>Активность потока данных в фабрике данных Azure

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

Используйте деятельность Data Flow для преобразования и перемещения данных с помощью картографических потоков данных. Если вы новичок в потоках данных, [см.](concepts-data-flow-overview.md)

## <a name="syntax"></a>Синтаксис

```json
{
    "name": "MyDataFlowActivity",
    "type": "ExecuteDataFlow",
    "typeProperties": {
      "dataflow": {
         "referenceName": "MyDataFlow",
         "type": "DataFlowReference"
      },
      "compute": {
         "coreCount": 8,
         "computeType": "General"
      },
      "staging": {
          "linkedService": {
              "referenceName": "MyStagingLinkedService",
              "type": "LinkedServiceReference"
          },
          "folderPath": "my-container/my-folder"
      },
      "integrationRuntime": {
          "referenceName": "MyDataFlowIntegrationRuntime",
          "type": "IntegrationRuntimeReference"
      }
}

```

## <a name="type-properties"></a>Свойства типа

Свойство | Описание | Допустимые значения | Обязательно
-------- | ----------- | -------------- | --------
поток данных | Ссылка на выполняемый поток данных | DataFlowСправка | Да
интеграцияRuntime | Среда вычислений, в которой работает поток данных. Если не указано, будет использовано время автоматического разрешения интеграции Azure | ИнтеграцияRuntimeСправка | нет
compute.coreCount | Количество ядер, используемых в искровом кластере. Может быть указано только в том случае, если используется время автоматического разрешения azure Integration | 8, 16, 32, 48, 80, 144, 272 | нет
compute.computeType | Тип вычислений, используемых в искровом кластере. Может быть указано только в том случае, если используется время автоматического разрешения azure Integration | "Генерал", "Вычислительный оптимизированный", "ПамятьОпиликом" | нет
staging.linkedService | Если вы используете источник или раковину S'L DW, учетная запись хранилища, используемая для постановки PolyBase | LinkedServiceСправка | Только в том случае, если поток данных читает или записывает сяробвие в DW
staging.folderPath | Если вы используете источник или раковину S'L DW, путь папки в учетной записи хранения капли используется для постановки PolyBase | Строка | Только в том случае, если поток данных читает или записывает сяробвие в DW

![Выполнение потока данных](media/data-flow/activity-data-flow.png "Выполнение потока данных")

### <a name="dynamically-size-data-flow-compute-at-runtime"></a>Динамический размер потока данных вычисляют во время выполнения

Свойства Core Count и Compute Type можно настроить динамически, чтобы приспособиться к размеру входящих исходных данных во время выполнения. Используйте действия конвейера, такие как Lookup или Get Metadata, чтобы найти размер исходного набора данных. Затем используйте добавление динамического содержимого в свойства активности Потока данных.

![Динамический поток данных](media/data-flow/dyna1.png "Динамический поток данных")

[Вот краткий видео-учебник объясняя эту технику](https://www.youtube.com/watch?v=jWSkJdtiJNM)

### <a name="data-flow-integration-runtime"></a>Время выполнения интеграции потока данных

Выберите время выполнения интеграции для выполнения действия Data Flow. По умолчанию Data Factory будет использовать автоматическое разрешение времени запуска Azure Integration с четырьмя ядрами рабочего времени и отсутствием времени для жизни (TTL). Этот ИК имеет общий тип вычислений цели и работает в том же регионе, что и ваша фабрика. Можно создать свой собственный Runtimes интеграции Azure, которые определяют определенные области, тип вычислений, количество яедер и TTL для выполнения деятельности потока данных.

Для выполнения конвейера кластер представляет собой кластер задания, запуск которого занимает несколько минут перед запуском выполнения. Если TTL не указан, это время запуска требуется на каждом запуске конвейера. Если указать TTL, теплый кластерный пул будет оставаться активным в течение времени, указанного после последнего выполнения, что приведет к сокращению времени запуска. Например, если у вас есть TTL 60 минут и запустить поток данных на нем один раз в час, кластер пул будет оставаться активным. Для получения дополнительной [Azure integration runtime](concepts-integration-runtime.md)информации см.

![Запуск интеграции Azure](media/data-flow/ir-new.png "Запуск интеграции Azure")

> [!NOTE]
> Выбор времени выполнения интеграции в действии Потока данных применяется только к *срабатыванию выполнения* конвейера. Отладка конвейера потоками данных выполняется в кластере, указанном в сеансе отладки.

### <a name="polybase"></a>PolyBase

Если вы используете хранилище данных Azure S'L в качестве раковины или источника, необходимо выбрать место постановки для загрузки пакета PolyBase. PolyBase позволяет осуществлять пакетную загрузку оптом вместо загрузки данных. PolyBase резко сокращает время загрузки в S'L DW.

## <a name="parameterizing-data-flows"></a>Параметрыализация потоков данных

### <a name="parameterized-datasets"></a>Параметризированные наборы данных

Если поток данных использует параметризированные наборы данных, установите значения параметров во вкладке **"Настройки".**

![Выполнение параметров потока данных](media/data-flow/params.png "Параметры")

### <a name="parameterized-data-flows"></a>Параметризованные потоки данных

Если поток данных параметрывен, установите динамические значения параметров потока данных во вкладке **Параметры.** Для присвоения динамических или буквальных значений параметров можно использовать язык выражения конвейера ADF (только для типов строк), либо язык выражения потока данных. Для получения дополнительной [информации см.](parameters-data-flow.md)

![Выполнение примера параметра потока данных](media/data-flow/parameter-example.png "Пример параметра")

### <a name="parameterized-compute-properties"></a>Параметризированные вычислительные свойства.

Можно параметризировать количество ягов или вычислить, если использовать автоматическое разрешение времени выполнения интеграции Azure и указать значения для compute.coreCount и computeType.

![Выполнение примера параметра потока данных](media/data-flow/parameterize-compute.png "Пример параметра")

## <a name="pipeline-debug-of-data-flow-activity"></a>Отладка трубопровода активности потока данных

Для выполнения отладки конвейера с помощью действия потока данных необходимо включить режим отладки потока данных через ползунок **Data Flow Debug** на верхней панели. Режим Debug позволяет запускать поток данных в отношении активного кластера Spark. Для получения дополнительной [Debug Mode](concepts-data-flow-debug-mode.md)информации см.

![Кнопка отладки](media/data-flow/debugbutton.png "Кнопка отладки")

Конвейер отладки работает в отношении активного кластера отладки, а не среды времени выполнения интеграции, указанной в настройках активности потока данных. Вы можете выбрать среду вычислений отладки при запуске режима отладки.

## <a name="monitoring-the-data-flow-activity"></a>Мониторинг активности потока данных

Активность Data Flow имеет специальный опыт мониторинга, где можно просматривать информацию о разделении, времени этапа и линии данных. Откройте панель мониторинга через значок очков под **действием.** Для получения дополнительной информации [см.](concepts-data-flow-monitoring.md)

### <a name="use-data-flow-activity-results-in-a-subsequent-activity"></a>Результаты действий потока данных в последующем действии

Активность потока данных выводит метрики относительно количества строк, написанных на каждую раковину и строк, прочитанную из каждого источника. Эти результаты возвращаются в `output` разделе результата выполнения действия. Возвращается метрики в формате ниже json.

``` json
{
    "runStatus": {
        "metrics": {
            "<your sink name1>": {
                "rowsWritten": <number of rows written>,
                "sinkProcessingTime": <sink processing time in ms>,
                "sources": {
                    "<your source name1>": {
                        "rowsRead": <number of rows read>
                    },
                    "<your source name2>": {
                        "rowsRead": <number of rows read>
                    },
                    ...
                }
            },
            "<your sink name2>": {
                ...
            },
            ...
        }
    }
}
```

Например, чтобы добраться до количества строк, написанных на раковину под названием 'sink1' в действии, названном 'dataflowActivity', используйте. `@activity('dataflowActivity').output.runStatus.metrics.sink1.rowsWritten`

Чтобы получить количество строк, прочитанных из источника под названием 'source1', который использовался в этой раковине, используйте. `@activity('dataflowActivity').output.runStatus.metrics.sink1.sources.source1.rowsRead`

> [!NOTE]
> Если в раковине написано ноль строк, она не будет отображаться в метриках. Существование может быть `contains` проверено с помощью функции. Например, `contains(activity('dataflowActivity').output.runStatus.metrics, 'sink1')` проверит, были ли какие-либо строки написаны для погружения1.

## <a name="next-steps"></a>Дальнейшие действия

Просмотрите действия управления потоками, поддерживаемые Фабрикой данных: 

- [Действие условия If](control-flow-if-condition-activity.md)
- [Действие выполнения конвейера](control-flow-execute-pipeline-activity.md)
- [Действие For Each](control-flow-for-each-activity.md)
- [Действие получения метаданных](control-flow-get-metadata-activity.md)
- [Действие поиска](control-flow-lookup-activity.md)
- [Веб-действие](control-flow-web-activity.md)
- [Действие Until](control-flow-until-activity.md)
