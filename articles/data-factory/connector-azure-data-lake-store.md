---
title: Копирование данных в или из хранилища озер данных Azure Gen1
description: Узнайте, как копировать данные из поддерживаемых исходных хранилищ данных в Azure Data Lake Store или из Data Lake Store в поддерживаемые приемники хранилищ с помощью фабрики данных.
services: data-factory
ms.author: jingwang
author: linda33wj
manager: shwang
ms.reviewer: douglasl
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 03/12/2020
ms.openlocfilehash: a8ba8b212a504a8f8e4e29fbd50126189998e81a
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "80065468"
---
# <a name="copy-data-to-or-from-azure-data-lake-storage-gen1-using-azure-data-factory"></a>Копирование данных в или из хранилища озер Лазурного берега Gen1 с помощью Azure Data Factory

> [!div class="op_single_selector" title1="Выберите используемую версию фабрики данных Azure:"]
> * [Версия 1](v1/data-factory-azure-datalake-connector.md)
> * [Текущая версия](connector-azure-data-lake-store.md)

В этой статье описывается, как копировать данные в azure Data Lake Storage Gen1 и из нее. Дополнительные сведения о Фабрике данных Azure см. во [вводной статье](introduction.md).

## <a name="supported-capabilities"></a>Поддерживаемые возможности

Этот разъем хранения данных Azure Data Lake Gen1 поддерживается для следующих действий:

- [Копирование активности](copy-activity-overview.md) с [помощью поддерживаемой матрицы источника/раковины](copy-activity-overview.md) 
- [Картирование потока данных](concepts-data-flow-overview.md)
- [Активность поиска](control-flow-lookup-activity.md)
- [Действие получения метаданных в Фабрике данных Azure](control-flow-get-metadata-activity.md)
- [Удаление действия](delete-activity.md)

В частности, с помощью этого разъема вы можете:

- Копирование файлов с помощью одного из следующих методов проверки подлинности: основной службы или управляемых идентификаторов для ресурсов Azure.
- Копировать файлы как есть или разбирать или генерировать файлы с [поддерживаемыми форматами файлов и кодеков сжатия.](supported-file-formats-and-compression-codecs.md)
- [Сохранить AcLs](#preserve-acls-to-data-lake-storage-gen2) при копировании в Azure Data Lake Storage Gen2.

> [!IMPORTANT]
> Если вы копируете данные с помощью автономного времени выполнения интеграции, `<ADLS account name>.azuredatalakestore.net` `login.microsoftonline.com/<tenant>/oauth2/token` назначаем корпоративный брандмауэр, чтобы исходящий трафик в порт 443 и на нее. Последняя – это служба токенов безопасности Azure, с которой должна взаимодействовать среда выполнения интеграции для получения маркера доступа.

## <a name="get-started"></a>Начало работы

> [!TIP]
> Для просмотра способов использования разъема Azure Data Lake Store можно ознакомиться с [данными Load data в магазине Azure Data Lake Store.](load-azure-data-lake-store.md)

[!INCLUDE [data-factory-v2-connector-get-started](../../includes/data-factory-v2-connector-get-started.md)]

В следующих разделах приводится информация о свойствах, используемых для определения сущностей фабрики данных, специфичных для Azure Data Lake Store.

## <a name="linked-service-properties"></a>Свойства связанной службы

Для связанной службы Azure Data Lake Store поддерживаются следующие свойства:

| Свойство | Описание | Обязательно |
|:--- |:--- |:--- |
| type | Для свойства `type` следует задать значение **AzureDataLakeStore**. | Да |
| dataLakeStoreUri | Сведения об учетной записи Azure Data Lake Store. Эти данные принимают один из следующих форматов: `https://[accountname].azuredatalakestore.net/webhdfs/v1` или `adl://[accountname].azuredatalakestore.net/`. | Да |
| subscriptionId | Идентификатор подписки Azure, к которой принадлежит учетная запись Data Lake Store. | Необходимо для приемника |
| имя_группы_ресурсов | Имя группы ресурсов Azure, к которой принадлежит учетная запись Data Lake Store. | Необходимо для приемника |
| connectVia | [Время выполнения интеграции,](concepts-integration-runtime.md) используемое для подключения к хранилику данных. Вы можете использовать время выполнения интеграции Azure или самохозня, если ваш хранилище данных находится в частной сети. Если это свойство не указано, используется время выполнения интеграции Azure по умолчанию. |нет |

### <a name="use-service-principal-authentication"></a>Использование аутентификации субъекта-службы

Чтобы использовать основную аутентификацию службы, выполните следующие действия.

1. Зарегистрируйте приложение в каталоге Azure Active И предоставите ей доступ к магазину Data Lake Store. Подробные инструкции см. в статье [Аутентификация между службами в Data Lake Store с помощью Azure Active Directory](../data-lake-store/data-lake-store-authenticate-using-active-directory.md). Запишите следующие значения, которые используются для определения связанной службы:

    - Идентификатор приложения
    - Ключ приложения
    - Tenant ID

2. Предоставите главному сервису соответствующее разрешение. Смотрите примеры того, как работает разрешение в Data Lake Storage Gen1 из [управления доступом в Azure Data Lake Storage Gen1.](../data-lake-store/data-lake-store-access-control.md#common-scenarios-related-to-permissions)

    - **Как источник**: В **доступе к исследователю** > **Access**данных , предоставить по крайней мере **выполнить** разрешение для всех папок вверх по течению, включая корень, наряду с Разрешением **Чтения** для файлов для копирования. Вы можете выбрать добавление в **эту папку и все дочерние элементы** для свойства recursive и добавить в качестве **записи разрешения доступа и записи разрешения по умолчанию**. Нет никаких требований в управлении доступом на уровне учетной записи (IAM).
    - **Как раковина**: В **доступе исследователя** > **Access**данных , дарите хотя бы **разрешение исполнения** для ВСЕХ вверх по течению скоросшивателей включая корень, вместе с позволением **записывать** для скоросшивателя раковины. Вы можете выбрать добавление в **эту папку и все дочерние элементы** для свойства recursive и добавить в качестве **записи разрешения доступа и записи разрешения по умолчанию**. Если для копирования времени активации интеграции Azure (как источник, так и раковина находятся в облаке) в IAM, предоставите по крайней мере роль **читателя,** чтобы позволить Data Factory обнаружить область для Хранилища Data Lake Store. Если вы хотите избежать этой роли IAM, явным образом [создайте среду выполнения интеграции Azure](create-azure-integration-runtime.md#create-azure-ir) с расположением Data Lake Store. Например, если ваш магазин Data Lake Store находится в Западной Европе, создайте время выполнения интеграции Azure с набором местоположения в «Западная Европа». Связать их в хранилище Data Lake Store, как показано в следующем примере.

Поддерживаются следующие свойства:

| Свойство | Описание | Обязательно |
|:--- |:--- |:--- |
| servicePrincipalId | Укажите идентификатора клиента приложения. | Да |
| servicePrincipalKey | Укажите ключ приложения. Пометьте это поле как `SecureString`, чтобы безопасно хранить его в фабрике данных, или [добавьте ссылку на секрет, хранящийся в Azure Key Vault](store-credentials-in-key-vault.md). | Да |
| tenant | Укажите информацию о арендаторе, такую как доменное имя или идентификатор клиента, под которым находится приложение. Эти сведения можно получить, наведя указатель мыши на правый верхний угол страницы портала Azure. | Да |

**Примере:**

```json
{
    "name": "AzureDataLakeStoreLinkedService",
    "properties": {
        "type": "AzureDataLakeStore",
        "typeProperties": {
            "dataLakeStoreUri": "https://<accountname>.azuredatalakestore.net/webhdfs/v1",
            "servicePrincipalId": "<service principal id>",
            "servicePrincipalKey": {
                "type": "SecureString",
                "value": "<service principal key>"
            },
            "tenant": "<tenant info, e.g. microsoft.onmicrosoft.com>",
            "subscriptionId": "<subscription of ADLS>",
            "resourceGroupName": "<resource group of ADLS>"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
```

### <a name="use-managed-identities-for-azure-resources-authentication"></a><a name="managed-identity"></a> Использование аутентификации управляемых удостоверений для ресурсов Azure

Фабрика данных может быть связана с [управляемым удостоверением для ресурсов Azure](data-factory-service-identity.md), которое представляет эту фабрику данных. Эту управляемую идентификацию можно использовать непосредственно для проверки подлинности Data Lake Store, подобно использованию собственного принципала обслуживания. Оно разрешает назначенной фабрике обращаться к данным и копировать их из службы Data Lake Store и в нее.

Для использования управляемых идентификаторов для проверки подлинности ресурсов Azure выполните следующие действия.

1. [Извлекайте информацию о том, что фабрика данных управляет идентификационным кодом,](data-factory-service-identity.md#retrieve-managed-identity) копируя значение "Идентификатор атлиты служебного удостоверения", генерируемого вместе с вашей фабрикой.

2. Предоставить управляемый доступ к идентификационным данным в хранилище Data Lake Store. Смотрите примеры того, как работает разрешение в Data Lake Storage Gen1 из [управления доступом в Azure Data Lake Storage Gen1.](../data-lake-store/data-lake-store-access-control.md#common-scenarios-related-to-permissions)

    - **Как источник**: В **доступе к исследователю** > **Access**данных , предоставить по крайней мере **выполнить** разрешение для всех папок вверх по течению, включая корень, наряду с Разрешением **Чтения** для файлов для копирования. Вы можете выбрать добавление в **эту папку и все дочерние элементы** для свойства recursive и добавить в качестве **записи разрешения доступа и записи разрешения по умолчанию**. Нет никаких требований в управлении доступом на уровне учетной записи (IAM).
    - **Как раковина**: В **доступе исследователя** > **Access**данных , дарите хотя бы **разрешение исполнения** для ВСЕХ вверх по течению скоросшивателей включая корень, вместе с позволением **записывать** для скоросшивателя раковины. Вы можете выбрать добавление в **эту папку и все дочерние элементы** для свойства recursive и добавить в качестве **записи разрешения доступа и записи разрешения по умолчанию**. Если для копирования времени активации интеграции Azure (как источник, так и раковина находятся в облаке) в IAM, предоставите по крайней мере роль **читателя,** чтобы позволить Data Factory обнаружить область для Хранилища Data Lake Store. Если вы хотите избежать этой роли IAM, явным образом [создайте среду выполнения интеграции Azure](create-azure-integration-runtime.md#create-azure-ir) с расположением Data Lake Store. Связать их в хранилище Data Lake Store, как показано в следующем примере.

В фабрике данных Azure не нужно указывать ничего, кроме общих сведений о Data Lake Store в связанной службе.

**Примере:**

```json
{
    "name": "AzureDataLakeStoreLinkedService",
    "properties": {
        "type": "AzureDataLakeStore",
        "typeProperties": {
            "dataLakeStoreUri": "https://<accountname>.azuredatalakestore.net/webhdfs/v1",
            "subscriptionId": "<subscription of ADLS>",
            "resourceGroupName": "<resource group of ADLS>"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
```

## <a name="dataset-properties"></a>Свойства набора данных

Полный список разделов и свойств, доступных для определения наборов данных, см. в статье о [наборах данных](concepts-datasets-linked-services.md). 

[!INCLUDE [data-factory-v2-file-formats](../../includes/data-factory-v2-file-formats.md)] 

Следующие свойства поддерживаются для Azure Data `location` Lake Store Gen1 в настройках в наборе данных на основе формата:

| Свойство   | Описание                                                  | Обязательно |
| ---------- | ------------------------------------------------------------ | -------- |
| type       | Свойство типа, под `location` данными, должно быть установлено в **AzureDataLakeStoreLocation.** | Да      |
| folderPath | Путь к папке. Если вы хотите использовать подстановочный знак для фильтрации папок, пропустите эту настройку и укажите его в настройках источника активности. | нет       |
| fileName   | Имя файла под данной папкойPath. Если вы хотите использовать подстановочный знак для фильтрации файлов, пропустите эту настройку и укажите ее в настройках источника активности. | нет       |

**Примере:**

```json
{
    "name": "DelimitedTextDataset",
    "properties": {
        "type": "DelimitedText",
        "linkedServiceName": {
            "referenceName": "<ADLS Gen1 linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, auto retrieved during authoring > ],
        "typeProperties": {
            "location": {
                "type": "AzureDataLakeStoreLocation",
                "folderPath": "root/folder/subfolder"
            },
            "columnDelimiter": ",",
            "quoteChar": "\"",
            "firstRowAsHeader": true,
            "compressionCodec": "gzip"
        }
    }
}
```

## <a name="copy-activity-properties"></a>Свойства действия копирования

Полный список разделов и свойств, доступных для определения действий, см. в разделе [Конвейеры и действия в фабрике данных Azure](concepts-pipelines-activities.md). Этот раздел содержит список свойств, поддерживаемых источником и приемником Azure Data Lake Store.

### <a name="azure-data-lake-store-as-source"></a>Azure Data Lake Store в качестве источника

[!INCLUDE [data-factory-v2-file-formats](../../includes/data-factory-v2-file-formats.md)] 

Следующие свойства поддерживаются для Azure Data `storeSettings` Lake Store Gen1 в настройках в источнике копирования на основе формата:

| Свойство                 | Описание                                                  | Обязательно                                      |
| ------------------------ | ------------------------------------------------------------ | --------------------------------------------- |
| type                     | Свойство типа `storeSettings` под должны быть установлены на **AzureDataLakeStoreReadSettings.** | Да                                           |
| recursive                | Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. Когда рекурсивный установлен на истину и раковина файл на основе магазина, пустая папка или subfolder не скопированы или созданы в раковине. Разрешенные значения **верны** (по умолчанию) и **ложные.** | нет                                            |
| подстановочный знакFolderPath       | Путь папки с символами подстановочных знаков для фильтрации папок источника. <br>Разрешенные подстановочные знаки `*` (соответствует `?` нулю или более символов) и (соответствует нулю или одному символу). Используйте, `^` чтобы избежать, если ваше фактическое имя папки имеет подстановочный знак или этот побег символ внутри. <br>Дополнительные примеры приведены в разделе [Примеры фильтров папок и файлов](#folder-and-file-filter-examples). | нет                                            |
| подстановочный знакFileName         | Имя файла с символами подстановочных знаков под данной папкойPath/wildcardFolderPath для фильтрации исходных файлов. <br>Разрешенные подстановочные знаки `*` (соответствует `?` нулю или более символов) и (соответствует нулю или одному символу). Используйте, `^` чтобы избежать, если ваше фактическое имя папки имеет подстановочный знак или этот побег символ внутри. Дополнительные примеры приведены в разделе [Примеры фильтров папок и файлов](#folder-and-file-filter-examples). | Да, `fileName` если не указан в наборе данных |
| modifiedDatetimeStart    | Фильтр файлов на основе атрибута Последнее изменение. Файлы выбираются, если их последнее измененное `modifiedDatetimeStart` `modifiedDatetimeEnd`время находится в пределах временной промена между и . Время применяется к часовому поясу UTC в формате "2018-12-01T05:00:00". <br> Свойства могут быть NULL, что означает, что фильтр атрибута файла не применяется к набору данных. Когда `modifiedDatetimeStart` значение датируется `modifiedDatetimeEnd` датой, но является NULL, это означает, что выбраны файлы, последний измененный атрибут которых больше или равен значению времени даты. Когда `modifiedDatetimeEnd` значение времени дат, но `modifiedDatetimeStart` NULL, это означает файлы, чей последний измененный атрибут меньше, чем значение времени даты выбрано. | нет                                            |
| modifiedDatetimeEnd      | То же, что и выше.                                               | нет                                            |
| maxConcurrentConnections | Одновременное количество подключений для подключения к хранилищу. Указать только при ограничении одновременного подключения к хранилику данных. | нет                                            |

**Примере:**

```json
"activities":[
    {
        "name": "CopyFromADLSGen1",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Delimited text input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "DelimitedTextSource",
                "formatSettings":{
                    "type": "DelimitedTextReadSettings",
                    "skipLineCount": 10
                },
                "storeSettings":{
                    "type": "AzureDataLakeStoreReadSettings",
                    "recursive": true,
                    "wildcardFolderPath": "myfolder*A",
                    "wildcardFileName": "*.csv"
                }
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
```

### <a name="azure-data-lake-store-as-sink"></a>Azure Data Lake Store в качестве приемника

[!INCLUDE [data-factory-v2-file-formats](../../includes/data-factory-v2-file-formats.md)] 

Следующие свойства поддерживаются для Azure Data `storeSettings` Lake Store Gen1 в настройках в подбросе копий на основе формата:

| Свойство                 | Описание                                                  | Обязательно |
| ------------------------ | ------------------------------------------------------------ | -------- |
| type                     | Свойство типа `storeSettings` под должны быть установлены на **AzureDataLakeStoreWriteSettings.** | Да      |
| copyBehavior             | Определяет поведение копирования, когда источником являются файлы из файлового хранилища данных.<br/><br/>Допустимые значения:<br/><b>— PreserveHierarchy (по умолчанию)</b>. Сохраняет иерархию файлов в целевой папке. Относительный путь исходного файла в исходной папке идентичен относительному пути целевого файла в целевой папке.<br/><b>— FlattenHierarchy</b>. Все файлы из исходной папки размещаются на первом уровне в целевой папке. Целевые файлы имеют автоматически сформированные имена. <br/><b>— MergeFiles</b>. Объединяет все файлы из исходной папки в один файл. Если указано имя файла, то оно присваивается объединенному файлу. В противном случае присваивается автоматически созданное имя файла. | нет       |
| истечение срокадействияDateTime | Упогоняет время истечения срока действия письменных файлов. Время применяется к времени UTC в формате "2020-03-01T08:00:00". По умолчанию это NULL, что означает, что письменные файлы никогда не истекли. | нет |
| maxConcurrentConnections | Одновременное количество подключений к хранилику данных. Указать только при ограничении одновременного подключения к хранилику данных. | нет       |

**Примере:**

```json
"activities":[
    {
        "name": "CopyToADLSGen1",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<Parquet output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "ParquetSink",
                "storeSettings":{
                    "type": "AzureDataLakeStoreWriteSettings",
                    "copyBehavior": "PreserveHierarchy"
                }
            }
        }
    }
]
```

### <a name="folder-and-file-filter-examples"></a>Примеры фильтров папок и файлов

В этом разделе описываются результаты применения фильтров с подстановочными знаками к пути папки и имени файла.

| folderPath | fileName | recursive | Структура папки исхода и результат фильтра (файлы **жирным шрифтом** извлекаются)|
|:--- |:--- |:--- |:--- |
| `Folder*` | (Пустой, используйте по умолчанию) | false | ПапкаA<br/>&nbsp;&nbsp;&nbsp;&nbsp;**Файл1.csv**<br/>&nbsp;&nbsp;&nbsp;&nbsp;**Файл2.json**<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3.csv<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4.json<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5.csv<br/>Другая_папкаB<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл6.csv |
| `Folder*` | (Пустой, используйте по умолчанию) | Да | ПапкаA<br/>&nbsp;&nbsp;&nbsp;&nbsp;**Файл1.csv**<br/>&nbsp;&nbsp;&nbsp;&nbsp;**Файл2.json**<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Файл3.csv**<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Файл4.json**<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Файл5.csv**<br/>Другая_папкаB<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл6.csv |
| `Folder*` | `*.csv` | false | ПапкаA<br/>&nbsp;&nbsp;&nbsp;&nbsp;**Файл1.csv**<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2.json<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3.csv<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4.json<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5.csv<br/>Другая_папкаB<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл6.csv |
| `Folder*` | `*.csv` | Да | ПапкаA<br/>&nbsp;&nbsp;&nbsp;&nbsp;**Файл1.csv**<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2.json<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Файл3.csv**<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4.json<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Файл5.csv**<br/>Другая_папкаB<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл6.csv |

### <a name="examples-of-behavior-of-the-copy-operation"></a>Примеры поведения операции копирования

В данном разделе описываются результаты выполнения операции копирования при использовании различных сочетаний значений `recursive` и `copyBehavior`.

| recursive | copyBehavior | Структура папок источника | Результаты цели |
|:--- |:--- |:--- |:--- |
| Да |preserveHierarchy | Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 | Целевая "Папка1" создается с такой же структурой, как и исходная папка:<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 |
| Да |flattenHierarchy | Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 | Целевая папка "Папка1" создается со следующей структурой: <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;автоматически созданное имя для "Файл1"<br/>&nbsp;&nbsp;&nbsp;&nbsp;автоматически созданное имя для "Файл2"<br/>&nbsp;&nbsp;&nbsp;&nbsp;автоматически созданное имя для "Файл3"<br/>&nbsp;&nbsp;&nbsp;&nbsp;автоматически созданное имя для "Файл4"<br/>&nbsp;&nbsp;&nbsp;&nbsp;автоматически созданное имя для "Файл5" |
| Да |mergeFiles | Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 | Целевая папка "Папка1" создается со следующей структурой: <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1 - Файл2 - Файл3 - Файл4 - Содержимое файла5 объединены в один файл с автоматически генерируемым именем файла. |
| false |preserveHierarchy | Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 | Целевая папка "Папка1" создается со следующей структурой:<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/><br/>Subfolder1 с File3, File4 и File5 не подобраны. |
| false |flattenHierarchy | Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 | Целевая папка "Папка1" создается со следующей структурой:<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;автоматически созданное имя для "Файл1"<br/>&nbsp;&nbsp;&nbsp;&nbsp;автоматически созданное имя для "Файл2"<br/><br/>Subfolder1 с File3, File4 и File5 не подобраны. |
| false |mergeFiles | Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 | Целевая папка "Папка1" создается со следующей структурой:<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Содержимое file1 и File2 объединено в один файл с автоматически генерируемым именем файла. автоматически созданное имя для "Файл1"<br/><br/>Subfolder1 с File3, File4 и File5 не подобраны. |

## <a name="preserve-acls-to-data-lake-storage-gen2"></a>Сохранение AcLs для хранения данных озера Gen2

>[!TIP]
>Для копирования данных из Azure Data Lake Storage Gen1 в Gen2 в целом см. [Копии данных из Azure Data Lake Storage Gen1 до Gen2 с помощью фабрики данных Azure](load-azure-data-lake-storage-gen2-from-gen1.md) data Factory для прохождения и передового опыта.

Если вы хотите воспроизвести списки управления доступом (ACLs) вместе с файлами данных при обновлении с Data Lake Storage Gen1 до Data Lake Storage Gen2, [см.](copy-activity-preserve-metadata.md#preserve-acls)

## <a name="mapping-data-flow-properties"></a>Отображение свойств потока данных

При преобразовании данных в картографируем поток данных можно читать и писать файлы из Azure Data Lake Storage Gen1 в формате JSON, Avro, Delimited Text или Parquet. Для получения дополнительной информации см. [преобразование источника](data-flow-source.md) и преобразование [раковины](data-flow-sink.md) в функции потока данных отображения.

### <a name="source-transformation"></a>Преобразование источника

В исходной трансформации можно прочитать из контейнера, папки или отдельного файла в Azure Data Lake Storage Gen1. Вкладка **параметры «Источник»** позволяет управлять чтением файлов. 

![Варианты исхода](media/data-flow/sourceOptions1.png "Варианты исхода")

**Путь Wildcard:** Использование шаблона подстановочных знаков поручает ADF прослежять каждую соответствующую папку и файл в одном преобразовании Исходного кода. Это эффективный способ обработки нескольких файлов в одном потоке. Добавьте несколько шаблонов подстановочного знака, который появляется при нависшем над существующим шаблоном подстановочных знаков.

Из исходного контейнера выберите серию файлов, которые соответствуют шаблону. Только контейнер может быть указан в наборе данных. Таким образом, ваш путь подстановочного знака должен также включать путь папки из корневой папки.

Примеры Wildcard:

* ```*```Представляет любой набор символов
* ```**```Представляет собой рекурсивное вложение каталога
* ```?```Заменяет одного символа
* ```[]```Соответствует одному из символов в скобках

* ```/data/sales/**/*.csv```Получает все файлы csv под /данными/продажами
* ```/data/sales/20??/**/```Получает все файлы в 20-м веке
* ```/data/sales/*/*/*.csv```Получает csv файлы двух уровней под /данные / продажи
* ```/data/sales/2004/*/12/[XY]1?.csv```Получает все файлы csv в 2004 году в декабре, начиная с X или Y префиксированы двузначным числом

**Раздел Корневой путь:** Если в исходном исходном материале с ```key=value``` форматом (например, год-2019) есть раздельные папки с форматом (например, год 2019), то можно назначить верхний уровень этого дерева папки раздела к имени столбца в потоке данных.

Во-первых, установите подстановочный знак, чтобы включить все пути, которые разделены папки плюс лист файлы, которые вы хотите прочитать.

![Настройки исходного кода раздела](media/data-flow/partfile2.png "Настройка файла раздела")

Используйте настройки раздела Root Path, чтобы определить, что такое верхний уровень структуры папки. При просмотре содержимого данных с помощью предварительного просмотра данных можно увидеть, что ADF добавит разрешенные разделы, найденные в каждом из уровней папки.

![Путь корня раздела](media/data-flow/partfile1.png "Предварительный просмотр корневого пути раздела")

**Список файлов:** Это набор файлов. Создайте текстовый файл, включающий список относительного пути обработки файлов. Укажите на этот текстовый файл.

**Колонка для хранения имени файла:** Храните имя исходного файла в столбце в данных. Введите новое имя столбца здесь для хранения строки имени файла.

**После завершения:** Выберите ничего не делать с исходным файлом после запуска потока данных, удаления исходного файла или перемещения исходного файла. Пути для перемещения являются относительными.

Чтобы переместить исходные файлы в другое место после обработки, сначала выберите "Движение" для операции файлов. Затем установите "из" каталога. Если вы не используете какие-либо подстановочные знаки для вашего пути, то параметр "от" будет той же папкой, что и папка исходного кода.

Если у вас есть исходный путь с подстановочным знаком, ваш синтаксис будет выглядеть следующим образом:

```/data/sales/20??/**/*.csv```

Вы можете указать "от" как

```/data/sales```

И "к", как

```/backup/priorSales```

В этом случае все файлы, полученные в соответствии с /данными/продажами, перемещаются в /backup/priorSales.

> [!NOTE]
> Операции файлов выполняются только при запуске потока данных из запуска конвейера (отладка или выполнение конвейера), используюго действие потока данных в конвейере. Операции файлов *не* работают в режиме отладки потока данных.

**Фильтр по последним модифицированным:** Можно фильтровать файлы, которые вы обрабатываете, указывая диапазон дат, когда они были в последний раз изменены. Все даты-время в UTC. 

### <a name="sink-properties"></a>Свойства раковины

При преобразовании раковины можно написать в контейнер или папку в Azure Data Lake Storage Gen1. вкладка **«Настройки»** позволяет управлять написанием файлов.

![варианты раковины](media/data-flow/file-sink-settings.png "варианты раковины")

**Очистить папку:** Определяет, очищается ли папка назначения до их написания данных.

**Опция названия файла:** Определяет, как будут названы файлы назначения в папке назначения. Параметры имени файла:
   * **По умолчанию**: Разрешить Spark называть файлы на основе по умолчанию PART.
   * **Шаблон**: Введите шаблон, перечисляет выводные файлы на раздел. Например, **кредиты будут** создавать кредиты1.csv, loans2.csv и так далее.
   * **Для раздела**: Введите одно имя файла на раздел.
   * **Как данные в столбце:** Установите выходный файл на значение столбца. Путь относительно контейнера набора данных, а не папки назначения. Если в наборе данных есть путь папки, он будет перекрыт.
   * **Выход в один файл:** Объедините разделенные выходные файлы в один названный файл. Путь относительно папки набора данных. Пожалуйста, имейте в виду, что операция te merge может быть сбой в зависимости от размера узла. Этот параметр не рекомендуется для больших наборов данных.

**Цитата все:** Определяет, следует ли приложить все значения в кавычки

## <a name="lookup-activity-properties"></a>Свойства активности поиска

Чтобы узнать подробности о свойствах, проверьте [активность поиска.](control-flow-lookup-activity.md)

## <a name="getmetadata-activity-properties"></a>Свойства активности GetMetadata

Чтобы узнать подробности о свойствах, проверьте [активность GetMetadata](control-flow-get-metadata-activity.md) 

## <a name="delete-activity-properties"></a>Удаление свойств активности

Чтобы узнать подробности о свойствах, проверьте [действия Delete](delete-activity.md)

## <a name="legacy-models"></a>Устаревшие модели

>[!NOTE]
>Следующие модели по-прежнему поддерживаются как для обратной совместимости. Вы предложили использовать новую модель, упомянутую в вышеуказанных разделах в будущем, и ADF авторство UI переключился на генерацию новой модели.

### <a name="legacy-dataset-model"></a>Устаревшая модель набора данных

| Свойство | Описание | Обязательно |
|:--- |:--- |:--- |
| type | Свойство типа набора данных должно быть установлено на **AzureDataLakeStoreFile.** |Да |
| folderPath | Путь к папке в Data Lake Store. Если это свойство не указано, будет использоваться корневая папка. <br/><br/>Фильтр Wildcard поддерживается. Разрешенные подстановочные знаки `*` (соответствует `?` нулю или более символов) и (соответствует нулю или одному символу). Используйте, `^` чтобы избежать, если ваше фактическое имя папки имеет подстановочный знак или этот побег символ внутри. <br/><br/>Например: rootfolder/subfolder/. Дополнительные примеры приведены в разделе [Примеры фильтров папок и файлов](#folder-and-file-filter-examples). |нет |
| fileName | Фильтр имени или подстановочного знака для файлов под указанным "folderPath". Если этому свойству не присвоить значение, набор данных будет указывать на все файлы в папке. <br/><br/>Для фильтра разрешены `*` подстановочные знаки (совпадает с нулем или более символов) и `?` (совпадают с нулем или одним символом).<br/>Пример 1. `"fileName": "*.csv"`<br/>Пример 2. `"fileName": "???20180427.txt"`<br/>Используйте, `^` чтобы избежать, если ваше фактическое имя файла имеет подстановочный знак или этот побег символ внутри.<br/><br/>Когда fileName не указан для набора данных вывода и **сохраненияИерархия** не указана в поглотителе активности, активность копирования автоматически генерирует имя файла со следующим шаблоном:*«Данные». действия запустить ID GUID. (ГУИД, если ФлэттенИерархии). «формат, если настроен». «Сжатие при настройке»,* например, «Data.0a405f8a-93ff-4c6f-b3be-f696166f1df7a.txt.gz». Если копировать из таблика с помощью имени таблицы вместо запроса, шаблон имени —*«имя стола». формата. «Сжатие при настройке»,* например, «MyTable.csv». |нет |
| modifiedDatetimeStart | Фильтр файлов на основе атрибута Последнее изменение. Файлы выбираются, если их последнее измененное `modifiedDatetimeStart` `modifiedDatetimeEnd`время находится в пределах временной промена между и . Время применяется к часовому поясу UTC в формате "2018-12-01T05:00:00". <br/><br/> На общую производительность движения данных влияет включение этой настройки, когда требуется создать файловый фильтр с огромным количеством файлов. <br/><br/> Свойства могут быть NULL, что означает, что фильтр атрибута файла не применяется к набору данных. Когда `modifiedDatetimeStart` значение датируется `modifiedDatetimeEnd` датой, но является NULL, это означает, что выбраны файлы, последний измененный атрибут которых больше или равен значению времени даты. Когда `modifiedDatetimeEnd` значение времени дат, но `modifiedDatetimeStart` NULL, это означает файлы, чей последний измененный атрибут меньше, чем значение времени даты выбрано.| нет |
| modifiedDatetimeEnd | Фильтр файлов на основе атрибута Последнее изменение. Файлы выбираются, если их последнее измененное `modifiedDatetimeStart` `modifiedDatetimeEnd`время находится в пределах временной промена между и . Время применяется к часовому поясу UTC в формате "2018-12-01T05:00:00". <br/><br/> На общую производительность движения данных влияет включение этой настройки, когда требуется создать файловый фильтр с огромным количеством файлов. <br/><br/> Свойства могут быть NULL, что означает, что фильтр атрибута файла не применяется к набору данных. Когда `modifiedDatetimeStart` значение датируется `modifiedDatetimeEnd` датой, но является NULL, это означает, что выбраны файлы, последний измененный атрибут которых больше или равен значению времени даты. Когда `modifiedDatetimeEnd` значение времени дат, но `modifiedDatetimeStart` NULL, это означает файлы, чей последний измененный атрибут меньше, чем значение времени даты выбрано.| нет |
| format | Если вы хотите копировать файлы, как это происходит между файлохранилищами (двоичная копия), пропустите раздел формата как ввода, так и в определениях набора выходных данных.<br/><br/>Если нужно проанализировать или создать файлы определенного формата, поддерживаются следующие типы форматов файлов: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat** и **ParquetFormat**. Свойству **type** в разделе **format** необходимо присвоить одно из этих значений. Для получения дополнительной информации, см [Текст формата](supported-file-formats-and-compression-codecs-legacy.md#text-format), [Формат JSON](supported-file-formats-and-compression-codecs-legacy.md#json-format), [формат Avro](supported-file-formats-and-compression-codecs-legacy.md#avro-format), [orc формат](supported-file-formats-and-compression-codecs-legacy.md#orc-format), и формат [паркета](supported-file-formats-and-compression-codecs-legacy.md#parquet-format) разделов. |Нет (только для сценария двоичного копирования) |
| compression | Укажите тип и уровень сжатия данных. Дополнительные сведения см. в разделе [Поддержка сжатия](supported-file-formats-and-compression-codecs-legacy.md#compression-support).<br/>Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**.<br/>Поддерживаемые уровни: **Optimal** и **Fastest**. |нет |

>[!TIP]
>Чтобы скопировать все файлы в папке, укажите только **folderPath**.<br>Чтобы скопировать один файл с определенным именем, укажите **папкуPath** с частью папки и **fileName** с именем файла.<br>Чтобы скопировать подмножество файлов под папкой, укажите **папкуPath** с частью папки и **fileName** с фильтром подстановочных знаков. 

**Примере:**

```json
{
    "name": "ADLSDataset",
    "properties": {
        "type": "AzureDataLakeStoreFile",
        "linkedServiceName":{
            "referenceName": "<ADLS linked service name>",
            "type": "LinkedServiceReference"
        },
        "typeProperties": {
            "folderPath": "datalake/myfolder/",
            "fileName": "*",
            "modifiedDatetimeStart": "2018-12-01T05:00:00Z",
            "modifiedDatetimeEnd": "2018-12-01T06:00:00Z",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ",",
                "rowDelimiter": "\n"
            },
            "compression": {
                "type": "GZip",
                "level": "Optimal"
            }
        }
    }
}
```

### <a name="legacy-copy-activity-source-model"></a>Модель источника исходной деятельности устаревшей копирования

| Свойство | Описание | Обязательно |
|:--- |:--- |:--- |
| type | Свойство `type` источника активности копирования должно быть установлено на **AzureDataLakeStoreSource.** |Да |
| recursive | Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. Когда `recursive` устанавливается на истину и раковина файл на основе магазина, пустая папка или subfolder не скопированы или созданы в раковине. Разрешенные значения **верны** (по умолчанию) и **ложные.** | нет |
| maxConcurrentConnections | Одновременное количество подключений к хранилику данных. Указать только при ограничении одновременного подключения к хранилику данных. | нет |

**Примере:**

```json
"activities":[
    {
        "name": "CopyFromADLSGen1",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<ADLS Gen1 input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "AzureDataLakeStoreSource",
                "recursive": true
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
```

### <a name="legacy-copy-activity-sink-model"></a>Устаревшая модель погружения активности копирования

| Свойство | Описание | Обязательно |
|:--- |:--- |:--- |
| type | Свойство `type` поглотителя активности копирования должно быть установлено на **AzureDataLakeStoreSink.** |Да |
| copyBehavior | Определяет поведение копирования, когда источником являются файлы из файлового хранилища данных.<br/><br/>Допустимые значения:<br/><b>— PreserveHierarchy (по умолчанию)</b>. Сохраняет иерархию файлов в целевой папке. Относительный путь исходного файла в исходной папке идентичен относительному пути целевого файла в целевой папке.<br/><b>— FlattenHierarchy</b>. Все файлы из исходной папки размещаются на первом уровне в целевой папке. Целевые файлы имеют автоматически сформированные имена. <br/><b>— MergeFiles</b>. Объединяет все файлы из исходной папки в один файл. Если указано имя файла, то оно присваивается объединенному файлу. В противном случае имя файла создается автоматически. | нет |
| maxConcurrentConnections | Одновременное количество подключений к хранилику данных. Указать только при ограничении одновременного подключения к хранилику данных. | нет |

**Примере:**

```json
"activities":[
    {
        "name": "CopyToADLSGen1",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<ADLS Gen1 output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "AzureDataLakeStoreSink",
                "copyBehavior": "PreserveHierarchy"
            }
        }
    }
]
```

## <a name="next-steps"></a>Дальнейшие действия

В таблице [Поддерживаемые хранилища данных](copy-activity-overview.md#supported-data-stores-and-formats) приведен список хранилищ данных, которые поддерживаются в качестве источников и приемников для действия копирования в фабрике данных Azure.
