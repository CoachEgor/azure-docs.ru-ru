---
title: 'Фабрика данных Azure: Часто задаваемые вопросы '
description: Получите ответы на часто задаваемые вопросы о фабрике данных Azure.
services: data-factory
documentationcenter: ''
author: djpmsft
ms.author: daperlov
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 02/10/2020
ms.openlocfilehash: 34972e70039fef17161bdef66f64278cbabf908f
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "80130800"
---
# <a name="azure-data-factory-faq"></a>Часто задаваемые вопросы о фабрике данных Azure
Эта статья содержит ответы на часто задаваемые вопросы о фабрике данных Azure.  

## <a name="what-is-azure-data-factory"></a>Что такое фабрика данных Azure? 
Data Factory — это полностью управляемый облачный сервис ETL- интеграции данных, который автоматизирует движение и трансформацию данных. Как на фабрике сырье превращается в готовую продукцию с помощью оборудования, так и в фабриках данных Azure необработанные данные собираются и преобразовываются в готовые к использованию сведения с помощью специальных служб. 

Фабрика данных Azure позволяет создавать управляемые данными рабочие процессы для перемещения данных между локальными и облачными хранилищами данных. И вы можете обрабатывать и преобразовывать данные с помощью потоков данных. ADF также поддерживает внешние вычислительные двигатели для преобразований ручной работы, используя вычислительные сервисы, такие как Azure HDInsight, Azure Databricks и время выполнения интеграции серверов S'L Server (SSIS). 

С помощью фабрики данных вы можете выполнить обработку данных, используя облачную службу на основе Azure или собственную вычислительную среду с локальным размещением, например SSIS, SQL Server или Oracle. После создания конвейера, выполняемого необходимое действие, можно запланировать его периодические (часы, ежедневные или еженедельные, например), планирование временного окна или вызвать конвейер из события. Дополнительную информацию см. в статье [Общие сведения о службе фабрики данных Azure, службе интеграции данных в облаке](introduction.md).

### <a name="control-flows-and-scale"></a>Потоки управления и масштабирование 
Для поддержки разнообразных интеграционных потоков и шаблонов в современном хранилище данных Data Factory обеспечивает гибкое моделирование конвейера данных. Это влечет за собой парадигмы программирования полного управления потоками, которые включают условное выполнение, ветвление в конвейерах данных и возможность явного пропуска параметров внутри и между этими потоками. Поток управления также включает в себя преобразование данных через отправку активности в внешние двигатели исполнения и возможности потока данных, включая движение данных в масштабе, через активность Copy.

Фабрика данных позволяет создать любой поток для своего сценария интеграции данных и запускать его по запросу или постоянно по расписанию. Ниже приведены несколько общих потоков, которые эта модель поддерживает.   

- Потоки управления:
    - Действия могут быть прикованы друг к другу в последовательности в конвейере.
    - Действия могут быть разветвлены в конвейере.
    - Параметры
        - Параметры могут быть определены на уровне конвейера и аргументы могут быть переданы при вызове конвейера по требованию или с триггера.
        - Действия могут использовать аргументы, передаваемые в конвейер.
    - Пользовательское прохождение состояния:
        - Выходы активности, включая состояние, могут быть использованы в результате последующего действия в конвейере.
    - Контейнеры для петли:
        - Пределость будет итерироваться над определенной коллекцией действий в цикле. 
- Потоки на основе триггеров:
    - Конвейеры можно вызывать по требованию или в определенное время.
- Разностные потоки:
    - Параметры могут быть использованы для определения вашего высоководного знака для дельта копии при перемещении измерения или справочных таблиц из реляционных магазинов, как на территории, так и в облаке, для загрузки данных в озеро. 

Дополнительные сведения см. в статье [Ветвления и создание цепочки действий в конвейере фабрики данных](tutorial-control-flow.md).

### <a name="data-transformed-at-scale-with-code-free-pipelines"></a>Данные, преобразуемые в масштабе с помощью бескодированных трубопроводов
Новый браузерный инструментарий позволяет писать и развертывать конвейеры без кода с помощью современного интерактивного веб-интерфейса.

Для разработчиков визуальных данных и инженеров по обработке данных веб-uI Data Factory — это среда без кодов, которую вы будете использовать для создания конвейеров. Он полностью интегрирован с Visual Studio Online Git и обеспечивает интеграцию для CI/CD и итеративную разработку с вариантами отладки.

### <a name="rich-cross-platform-sdks-for-advanced-users"></a>Богатые кросс-платформенные SDK для продвинутых пользователей
Data Factory V2 предоставляет богатый набор SDK, которые могут быть использованы для авторизации, управления и мониторинга конвейеров с помощью любимого IDE, в том числе:
* Пакет SDK для Python
* PowerShell CLI
* Пакет SDK для C#

Пользователи также могут использовать документированные AAP REST для взаимодействия с Data Factory V2.

### <a name="iterative-development-and-debugging-by-using-visual-tools"></a>Итеративная разработка и отладка с помощью визуальных инструментов
Визуальные инструменты Azure Data Factory обеспечивают итеративную разработку и отладку. Вы можете создавать конвейеры и выполнять тесты, используя возможность **Debug** в холсте конвейера, не написав ни одной строки кода. Результаты тестовых запусков можно просмотреть в окне **вывода** холста конвейера. После успешного выполнения теста можно добавить в конвейер больше действий и продолжить отладку итеративным образом. Вы также можете отменить тестовые запуски после их выполнения. 

Перед выбором **Debug**не требуется публиковать изменения в службе фабрики данных. Это полезно в сценариях, где требуется убедиться, что новые дополнения или изменения будут работать, как ожидалось, прежде чем обновлять рабочие процессы фабрики данных в средах разработки, тестирования или производства. 

### <a name="ability-to-deploy-ssis-packages-to-azure"></a>Возможность развертывания пакетов SSIS в Azure 
Если вы хотите перемещать рабочие нагрузки служб SSIS, то создайте фабрику данных и подготовьте среду выполнения интеграции Azure SSIS. Время выполнения интеграции Azure-SSIS — это полностью управляемый кластер vMs-сообщений Azure, предназначенных для запуска пакетов SSIS в облаке. Пошаговые инструкции см. в руководстве [Развертывание пакетов служб интеграции SQL Server (SSIS) в Azure](tutorial-create-azure-ssis-runtime-portal.md). 
 
### <a name="sdks"></a>Пакеты SDK
Если вы продвинутый пользователь и ищете программный интерфейс, Data Factory предоставляет богатый набор SDK, которые можно использовать для авторства, управления или мониторинга конвейеров с помощью любимого IDE. Поддерживаются такие языки, как .NET, Python, PowerShell и REST.

### <a name="monitoring"></a>Наблюдение
Фабрики данных можно отслеживать с помощью PowerShell, пакета SDK и визуальных средств наблюдения в браузерном пользовательском интерфейсе. Вы можете эффективно и эффективно отслеживать и управлять пользовательскими потоками по требованию, триггерами и часовыми потоками. Отмените существующие задачи, увидеть сбои с первого взгляда, просверлить вниз, чтобы получить подробные сообщения об ошибках, и отладить проблемы, все из одного стекла без переключения контекста или навигации взад и вперед между экранами. 

### <a name="new-features-for-ssis-in-data-factory"></a>Новые функции для SSIS на фабрике данных
С момента первоначального публичного предварительного релиза в 2017 году Data Factory добавила следующие функции для SSIS:

-    Поддержка еще трех конфигураций/вариантов базы данных Azure S'L для размещения базы данных SSIS (SSISDB) проектов/пакетов:
-    База данных СЗЛ с конечными точками виртуального сетевого обслуживания
-    управляемый экземпляр
-    Эластичный пул
-    Поддержка виртуальной сети Azure Resource Manager поверх классической виртуальной сети, которая будет универсана в будущем, что позволяет вводить/присоединиться к моменту выполнения интеграции Azure-SSIS в виртуальную сеть, настроенную для базы данных СЗЛ с виртуальными конечными точками обслуживания сети/MI/on-premises. Для получения дополнительной [Join an Azure-SSIS integration runtime to a virtual network](join-azure-ssis-integration-runtime-virtual-network.md)информации см.
-    Поддержка функциональной проверки функционального каталога Azure (Azure AD) и проверки подлинности S'L для подключения к SSISDB, что позволяет аутентификации Azure AD с помощью управляемой идентификации Data Factory для ресурсов Azure
-    Поддержка для привлечения собственной лицензии на серверную ночку s'L Server для получения существенной экономии от опции Azure Hybrid Benefit
-    Поддержка корпоративного издания интеграции Azure-SSIS, которая позволяет использовать расширенные/премиум-функции, пользовательский интерфейс настройки для установки дополнительных компонентов/расширений и экосистему партнеров. Для получения дополнительной [Enterprise Edition, Custom Setup, and 3rd Party Extensibility for SSIS in ADF](https://blogs.msdn.microsoft.com/ssis/2018/04/27/enterprise-edition-custom-setup-and-3rd-party-extensibility-for-ssis-in-adf/)информации см. 
-    Более глубокая интеграция SSIS в фабрику данных позволяет вызывать/вызвать первоклассные действия пакета SSIS в конвейерах Data Factory и запланировать их с помощью SSMS. Для получения дополнительной [Modernize and extend your ETL/ELT workflows with SSIS activities in ADF pipelines](https://blogs.msdn.microsoft.com/ssis/2018/05/23/modernize-and-extend-your-etlelt-workflows-with-ssis-activities-in-adf-pipelines/)информации см.


## <a name="what-is-the-integration-runtime"></a>Что такое время выполнения интеграции?
Время выполнения интеграции — это вычислительная инфраструктура, которую использует Azure Data Factory для обеспечения следующих возможностей интеграции данных в различных сетевых средах:

- **Движение данных**: Для перемещения данных время выполнения интеграции перемещает данные между хранилищами данных источника и назначения, обеспечивая при этом поддержку встроенных разъемов, преобразования формата, картирования столбцов и выполнения и масштабируемой передачи данных.
- **Действия диспетчерской:** Для преобразования время выполнения интеграции обеспечивает возможность естественного выполнения пакетов SSIS.
- **Выполнение пакетов SSIS:** Время выполнения интеграции выполняется в среде управляемых вычислений Azure. Время выполнения интеграции также поддерживает деятельность по отправке и мониторингу трансформации на различных вычислительных службах, таких как Azure HDInsight, Azure Machine Learning, база данных S'L и S'L Server.

Можно развернуть один или несколько экземпляров времени выполнения интеграции по мере необходимости перемещения и преобразования данных. Время выполнения интеграции может работать в публичной сети Azure или в частной сети (в непредпомянении, виртуальной сети Azure или виртуальном частном облаке Amazon Web Services. 

Для получения дополнительной информации смотрите [время выполнения интеграции в Azure Data Factory.](concepts-integration-runtime.md)

## <a name="what-is-the-limit-on-the-number-of-integration-runtimes"></a>Что такое ограничение количества сред выполнения интеграции?
В фабрике данных нет жестких ограничений на количество экземпляров среды выполнения интеграции. Однако есть ограничение на число ядер виртуальной машины, которые среда выполнения интеграции может использовать для каждой подписки при выполнении пакетов служб SSIS. Дополнительные сведения см. в разделе [Ограничения фабрики данных](../azure-resource-manager/management/azure-subscription-service-limits.md#data-factory-limits).

## <a name="what-are-the-top-level-concepts-of-azure-data-factory"></a>Какие основные концепции в фабрике данных Azure?
В подписке Azure может быть один или несколько экземпляров фабрики данных Azure. Фабрика данных Azure содержит четыре ключевых компонента. Они образуют платформу, на которой можно создавать управляемые данными рабочие процессы, предусматривающие перемещение и преобразование данных.

### <a name="pipelines"></a>Конвейеры
Фабрика данных может иметь один или несколько конвейеров. Конвейер — это логическая группа действий, которые выполняют определенный блок задач. Действия в конвейере совместно выполняют задачу. Например, конвейер может включать группу действий, которые принимают данные из большого двоичного объекта Azure и выполняют запрос Hive в кластере HDInsight для секционирования данных. Преимуществом является то, что конвейер позволяет управлять группами действий, а не каждым отдельным действием. Вы можете связать вместе действия в конвейере, чтобы выполнять их последовательно, или выполнять их параллельно и независимо друг от друга.

### <a name="data-flows"></a>Потоки данных
Потоки данных являются объектами, которые вы создаете визуально в Data Factory, которые преобразуют данные в масштабе в бэкэнд-сервисах Spark. Вам не нужно понимать программирование или внутренностя Spark. Просто проектируйте намерение преобразования данных с помощью графиков (Картирование) или электронных таблиц (Wrangling).

### <a name="activities"></a>Действия
Действия представляют отдельные этапы обработки в конвейере. Например, можно использовать действие Copy для копирования данных из одного хранилища данных в другой. Точно так же можно использовать действие Hive, которое выполняет запрос Hive к кластеру Azure HDInsight для преобразования или анализа данных. Фабрика данных поддерживает три типа действий: действия перемещения данных, действия преобразования данных и действия управления.

### <a name="datasets"></a>Наборы данных
Наборы данных представляют структуры данных в хранилищах. Эти структуры указывают данные, необходимые для использования в действиях, разделяя их на входные и выходные. 

### <a name="linked-services"></a>Связанные службы
Связанные службы напоминают строки подключения, определяющие сведения о подключении, необходимые для подключения фабрики данных к внешним ресурсам. Подумайте об этом так: связанная служба определяет подключение к источнику данных, а набор данных представляет структуру данных. Например, связанная служба хранилища Azure определяет строку подключения для подключения к учетной записи хранения Azure. А набор данных Azure blob определяет контейнер blob и папку, содержащую данные.

Связанные службы используются в фабрике данных для двух целей:

- Для представления *хранилища данных*, включая, помимо прочего, локальный экземпляр SQL Server, экземпляр базы данных Oracle, общую папку и учетную запись хранилища BLOB-объектов Azure. Список поддерживаемых хранилищ данных см. в статье [Действие копирования в фабрике данных Azure](copy-activity-overview.md).
- Для представления *вычислительного ресурса*, в котором можно выполнить действие. Например, действие HDInsightHive выполняется в кластере Hadoop в HDInsight. Список поддерживаемых действий преобразования и вычислительных сред см. в статье [Преобразование данных в фабрике данных Azure](transform-data.md).

### <a name="triggers"></a>Триггеры
Триггеры обозначают единицу обработки, которая определяет время запуска для выполнения конвейера. Существует несколько типов триггеров для разных событий. 

### <a name="pipeline-runs"></a>Запуски конвейера
Запуск конвейера — это экземпляр выполнения конвейера. Запуск конвейера обычно выполняется путем передачи аргументов для параметров, определенных в конвейере. Вы можете передать аргументы вручную или в определении триггера.

### <a name="parameters"></a>Параметры
Параметры представляют собой пары "ключ — значение" в конфигурации только для чтения.Вы определяете параметры в конвейере и передаете для них аргументы во время выполнения из контекста запуска. Контекст запуска создается триггером или из конвейера, который выполняется вручную. Действия в конвейере используют значения параметров.

Набор данных — это сильно набранный параметр и сущность, которую можно повторно использовать или ссылаться. Деятельность может ссылаться на наборы данных, и она может потреблять свойства, определенные в определении набора данных.

Связанная служба также является строго типизированным параметром, который содержит сведения о подключении к хранилищу данных или среде вычислений. Это также сущность, которую можно использовать повторно или ссылаться.

### <a name="control-flows"></a>Потоки управления
Потоки управления выполняют оркестрацию действий в конвейере, которая включает цепочки действий в последовательности, ветвление и параметры, определяемые на уровне конвейера, а также аргументы, которые передаются во время вызова конвейера по запросу или из триггера. Потоки управления также включают пользовательские проходящей состояния и циклических контейнеров (т.е. foreach итераторы).


Дополнительные сведения о понятиях фабрики данных см. в следующих статьях:

- [Datasets and linked services in Azure Data Factory](concepts-datasets-linked-services.md) (Наборы данных и связанные службы в фабрике данных Azure)
- [Трубопроводы и мероприятия](concepts-pipelines-activities.md)
- [Среда выполнения интеграции в фабрике данных Azure](concepts-integration-runtime.md)

## <a name="what-is-the-pricing-model-for-data-factory"></a>Какая модель ценообразования применяется для фабрики данных?
Подробные сведения о ценах на фабрику данных Azure см. на [этой странице](https://azure.microsoft.com/pricing/details/data-factory/).

## <a name="how-can-i-stay-up-to-date-with-information-about-data-factory"></a>Как оставаться в курсе последних новостей о фабрике данных?
Чтобы узнавать о последних новостях о фабрике данных Azure, используйте следующие сайты:

- [Блог](https://azure.microsoft.com/blog/tag/azure-data-factory/)
- [Документация по фабрике данных Azure V2](/azure/data-factory)
- [Домашняя страница продукта](https://azure.microsoft.com/services/data-factory/)

## <a name="technical-deep-dive"></a>Подробное техническое руководство 

### <a name="how-can-i-schedule-a-pipeline"></a>Как запланировать конвейер? 
Для планирования конвейера можно использовать триггер планировщика или триггер по временному окну. Спусковой крючок использует график календаря настенных часов, который может периодически планировать конвейеры или в повторяющихся шаблонах на основе календаря (например, по понедельникам в 18:00 и четвергам в 21:00). Дополнительные сведения см. в статье [Выполнение конвейера и триггеры в фабрике данных Azure](concepts-pipeline-execution-triggers.md).

### <a name="can-i-pass-parameters-to-a-pipeline-run"></a>Можно ли передать параметры в выполнение конвейера?
Да, параметры представляют собой первоклассную концепцию верхнего уровня в Data Factory. Вы можете определить параметры на уровне конвейера и передать аргументы при выполнении конвейера, запускаемого по требованию или с помощью триггера.  

### <a name="can-i-define-default-values-for-the-pipeline-parameters"></a>Можно ли определить значения по умолчанию для параметров конвейера? 
Да. Вы можете определить значения по умолчанию для параметров в конвейерах. 

### <a name="can-an-activity-in-a-pipeline-consume-arguments-that-are-passed-to-a-pipeline-run"></a>Может ли действие в конвейере использовать аргументы, передаваемые в конвейер? 
Да. Каждое действие в рамках конвейера может использовать значение параметра, переданное в конвейер и запущенное с помощью конструкции `@parameter`. 

### <a name="can-an-activity-output-property-be-consumed-in-another-activity"></a>Может ли свойство из выходных данных действия использоваться в другом действии? 
Да. Выходные данные действия могут использоваться в последующем действии. Для этого применяется конструкция `@activity`.
 
### <a name="how-do-i-gracefully-handle-null-values-in-an-activity-output"></a>Как корректно обрабатывать значения null в выходных данных действия? 
Для корректной обработки значений null в выражениях можно использовать конструкцию `@coalesce`. 

## <a name="mapping-data-flows"></a>Сопоставление потоков данных

### <a name="i-need-help-troubleshooting-my-data-flow-logic-what-info-do-i-need-to-provide-to-get-help"></a>Мне нужна помощь в устранении логики потока данных. Какую информацию мне нужно предоставить, чтобы получить помощь?

Когда корпорация Майкрософт предоставляет помощь или устранение неполадок с потоками данных, пожалуйста, предоставьте скрипт потока данных. Это скрипт с кодом с графика потока данных. С uI ADF откройте поток данных, а затем нажмите кнопку "Сценарий" в правом верхнем углу. Копировать и вставить этот скрипт или сохранить его в текстовом файле.

### <a name="how-do-i-access-data-by-using-the-other-90-dataset-types-in-data-factory"></a>Как получить доступ к данным, используя остальные 90 типов наборов данных в Data Factory?

Функция картирования потоков данных в настоящее время позволяет Лазурной базе данных S'L, Лазурный s'L хранилище данных, делимитированные текстовые файлы из azure Blob хранения или Azure Data Lake Storage Gen2, и Parquet файлы из Blob хранения или хранения данных озера Gen2 родообразно для источника и раковины. 

Используйте действие Copy для постановки данных из любого другого разъема, а затем выполните действие потока данных для преобразования данных после их постановки. Например, конвейер сначала копирует сярплю в хранилище Blob, а затем в потоке данных будет использоваться набор данных в источнике для преобразования этих данных.

### <a name="is-the-self-hosted-integration-runtime-available-for-data-flows"></a>Доступно ли автономное время выполнения интеграции для потоков данных?

Самоходной ИК — это построение конвейера ADF, которое можно использовать с помощью copy Activity для получения или перемещения данных в и из источников данных и поглотителей на основе ВМ. Этап данных сначала с копией, затем поток данных для преобразования, а затем последующую копию, если вам нужно переместить, что преобразованные данные обратно в магазин на прем.

### <a name="does-the-data-flow-compute-engine-serve-multiple-tenants"></a>Обслуживает ли механизм потока данных нескольких арендаторов?
Кластеры никогда не являются общими. Мы гарантируем изоляцию для каждого задания, работаемого в производственных запусках. В случае отладки один человек получает один кластер, и все отладки будут идти в тот кластер, который инициируется этим пользователем.

## <a name="wrangling-data-flows"></a>Потоки данных по вине

### <a name="what-are-the-supported-regions-for-wrangling-data-flow"></a>Каковы поддерживаемые регионы для потока данных?

Поток данных в настоящее время поддерживается на фабриках данных, созданных в следующих регионах:

* Восточная Австралия
* Центральная Канада
* Центральная Индия
* Восточная часть США
* восточная часть США 2
* Восточная Япония
* Северная Европа
* Юго-Восточная Азия
* Центрально-южная часть США
* южная часть Соединенного Королевства
* центрально-западная часть США
* Западная Европа
* западная часть США
* западная часть США 2

### <a name="what-are-the-limitations-and-constraints-with-wrangling-data-flow"></a>Каковы ограничения и ограничения, связанные с потоком данных?

Имена наборов данных могут содержать только альфа-числовые символы. Поддерживаются следующие хранилища данных:

* DelimitedText набор данных в хранилище Azure Blob с помощью аутентификации ключей учетной записи
* DelimitedText набор данных в Azure Data Lake Storage 2 с помощью ключа учетной записи или основной аутентификации службы
* DelimitedText набор данных в Azure Data Lake Storage gen1 с использованием основной аутентификации службы
* База данных и хранилище данных Azure S'L с использованием проверки подлинности кв.м. Ниже приведены поддерживаемые типы S'L. Отсутствует PolyBase или постановочная поддержка хранилища данных.

В настоящее время интеграция подключенных служб Key Vault не поддерживается в потоках данных.

### <a name="what-is-the-difference-between-mapping-and-wrangling-data-flows"></a>В чем разница между картографированием и потоками данных?

Потоки карт данных обеспечивают способ преобразования данных в масштабе без необходимости кодирования. Задания преобразования данных можно спроектировать в холсте потока данных, построив ряд преобразований. Начните с произвольным количеством исходных преобразований, за которыми следуют шаги преобразования данных. Завершите поток данных с раковиной, чтобы приземлиться ваши результаты в пункт назначения. Картирование потока данных отлично подходит для картирования и преобразования данных как с известными, так и с неизвестными схемами в раковинах и источниках.

Потоки данных для обработки данных позволяют выполнять гибкую подготовку и разведку данных с помощью редактора гибридного приложения Power Query Online в масштабе с помощью выполнения искры. С ростом данных озер иногда нужно просто изучить набор данных или создать набор данных в озере. Вы не отображение к известной цели. Потоки данных для обработки данных используются для менее формальных и основанных на моделях сценариев аналитики.

### <a name="what-is-the-difference-between-power-platform-dataflows-and-wrangling-data-flows"></a>В чем разница между потоками данных Power Platform и потоками данных?

Power Platform Dataflows позволяет пользователям импортировать и преобразовывать данные из широкого спектра источников данных в Общую службу данных и озеро данных Azure для создания приложений PowerApps, отчетов Power BI или автоматизации потока. Power Platform Dataflows использует установленный опыт подготовки данных Power Query, аналогичный Power BI и Excel. Power Platform Dataflows также позволяют легко использовать повторно в организации и автоматически обрабатывать оркестровку (например, автоматически обновлять потоки данных, которые зависят от другого потока данных при обновлении первого потока).

Azure Data Factory (ADF) — это управляемая служба интеграции данных, которая позволяет инженерам по обработке данных и интегратору данных гражданина создавать сложные гибридные процессы преобразования экстракта-трансформации (ETL) и процесс преобразования экстракта-нагрузки (ELT). Поток данных в ADF предоставляет пользователям свободную от кода среду, которая упрощает подготовку данных в облаке и масштабируется до любого размера данных без необходимости управления инфраструктурой. Для подготовки и формирования данных используется технология подготовки данных Power Query (также используемая в потоках данных Power Platform, Excel, Power BI). Построенные для решения всех сложностей и масштабирования проблем интеграции больших данных, споры потоков данных позволяют пользователям быстро готовить данные в масштабе с помощью выполнения искры. Пользователи могут создавать устойчивые конвейеры данных в доступной визуальной среде с помощью нашего браузерного интерфейса и позволяют ADF обрабатывать сложности выполнения Spark. Создавайте расписания для конвейеров и отслеживайте выполнение потоков данных с портала мониторинга ADF. Легко управлять sL-аналитиками доступности данных с помощью мониторинга и предупреждений о доступности ADF и использования встроенных возможностей непрерывной интеграции и развертывания для сохранения и управления потоками в управляемой среде. Установите оповещения и планы выполнения представления, чтобы проверить, что ваша логика выполняется по плану при настройке потоков данных.

### <a name="supported-sql-types"></a>Поддерживаемые типы S'L

Поток данных Wrangling поддерживает следующие типы данных в S'L. Вы получите ошибку проверки для использования типа данных, который не поддерживается.

* short
* double
* real
* FLOAT
* char
* nchar
* varchar
* nvarchar
* Целое число
* INT
* bit
* Логическое
* smallint
* tinyint
* BIGINT
* long
* text
* Дата
* DATETIME
* datetime2
* smalldatetime
* TIMESTAMP
* UNIQUEIDENTIFIER
* Xml

Другие типы данных будут поддерживаться в будущем.

## <a name="next-steps"></a>Дальнейшие действия
Пошаговые инструкции по созданию фабрики данных см. в следующих руководствах:

- [Быстрый запуск: Создание фабрики данных](quickstart-create-data-factory-dot-net.md)
- [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL Azure с помощью фабрики данных Azure](tutorial-copy-data-dot-net.md)
