---
title: Обработка строк ошибок с картографическими потоками данных в Azure Data Factory
description: Узнайте, как обрабатывать ошибки усечения S'L на фабрике данных Azure, используя потоки карт.
services: data-factory
author: kromerm
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 10/28/2019
ms.author: makromer
ms.openlocfilehash: 4f65421a6457d4bf4d438ce9d035d46476829da2
ms.sourcegitcommit: b80aafd2c71d7366838811e92bd234ddbab507b6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/16/2020
ms.locfileid: "81414358"
---
# <a name="handle-sql-truncation-error-rows-in-data-factory-mapping-data-flows"></a>Обработка строк усечения S'L в потоках картографических данных Data

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

Распространенным сценарием на фабрике данных при использовании картографических потоков данных является запись преобразованных данных в базу данных Azure S'L. В этом случае обычное условие ошибки, которое необходимо предотвратить, возможно усечение столбца. Выполните следующие действия, чтобы обеспечить журнал столбцов, которые не вписываются в столбец целевой строки, что позволяет потоку данных продолжаться в этих сценариях.

## <a name="scenario"></a>Сценарий

1. У нас есть целевая таблица ```nvarchar(5)``` баз данных Azure S'L с столбцом под названием "имя".

2. Внутри нашего потока данных, мы хотим, чтобы карта названия фильмов из нашего раковины, что целевой "имя" колонки.

    ![Поток данных о фильмах 1](media/data-flow/error4.png)
    
3. Проблема в том, что название фильма не все помещается в столбец раковины, который может содержать только 5 символов. При выполнении этого потока данных, вы получите ошибку, как этот:```"Job failed due to reason: DF-SYS-01 at Sink 'WriteToDatabase': java.sql.BatchUpdateException: String or binary data would be truncated. java.sql.BatchUpdateException: String or binary data would be truncated."```

## <a name="how-to-design-around-this-condition"></a>Как спроектировать вокруг этого условия

1. В этом сценарии максимальная длина столбца "имя" составляет пять символов. Итак, давайте добавим условное преобразование сплита, которое позволит нам регистрировать строки с "названиями", которые длиннее пяти символов, а также позволяя остальным строкам, которые могут поместиться в это пространство, писать в базу данных.

    ![условное разбиение](media/data-flow/error1.png)

2. Это условное преобразование разделения определяет максимальную длину "названия" в пять. Любая строка, которая меньше или равна пяти, войдет в ```GoodRows``` поток. Любая строка, превышающее ```BadRows``` пять строк, войдет в поток.

3. Теперь нам нужно войти строки, которые не удалось. Добавьте преобразование ```BadRows``` раковины в поток для ведения журнала. Здесь мы будем "авто-карта" все поля, так что у нас есть журнал полной записи транзакции. Это вывод файла CSV с текстовым разграничением в один файл в Blob Storage. Мы назовем файл журнала "badrows.csv".

    ![Плохие строки](media/data-flow/error3.png)
    
4. Завершенный поток данных показан ниже. Теперь мы можем разделить строки ошибок, чтобы избежать ошибок усечения S'L и поместить эти записи в файл журнала. Между тем, успешные строки могут продолжать писать в нашу целевую базу данных.

    ![полный поток данных](media/data-flow/error2.png)

## <a name="next-steps"></a>Дальнейшие действия

* Создайте остальную часть логики потока данных с помощью [преобразования потоков данных](concepts-data-flow-overview.md)для картирования.
