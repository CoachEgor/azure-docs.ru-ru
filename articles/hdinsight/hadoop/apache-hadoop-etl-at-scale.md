---
title: Извлечение, преобразование и загрузка (ETL) в масштабе в Azure HDInsight
description: Узнайте, как в HDInsight используются средства извлечения, преобразования и загрузки с Apache Hadoop.
author: ashishthaps
ms.author: ashishth
ms.reviewer: jasonh
ms.service: hdinsight
ms.topic: conceptual
ms.custom: hdinsightactive,seoapr2020
ms.date: 04/28/2020
ms.openlocfilehash: ee989ccbb2e441256bec71781c538c7761fc7b88
ms.sourcegitcommit: 58faa9fcbd62f3ac37ff0a65ab9357a01051a64f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/28/2020
ms.locfileid: "82232246"
---
# <a name="extract-transform-and-load-etl-at-scale"></a>Извлечение, преобразование и загрузка (ETL) в масштабе

Извлечение, преобразование и загрузка (ETL) — это процесс, с помощью которого данные получаются из различных источников. Собрано в стандартном расположении, очищено и обработано. В конечном итоге загружается в хранилище данных, из которого можно выполнить запрос. Прежние ETL-процессы дают возможность импортировать данные, очищать их на месте, а затем сохранять в реляционный обработчик данных. В HDInsight широкое разнообразие компонентов Apache Hadoop среды поддерживает ETL в масштабе.

Использование HDInsight в процессах ETL можно кратко обозначить с помощью такого конвейера:

![Обзор ETL в HDInsight в масштабе](./media/apache-hadoop-etl-at-scale/hdinsight-etl-at-scale-overview.png)

В разделах ниже рассматриваются все этапы ETL и их связанные компоненты.

## <a name="orchestration"></a>Оркестрация

Оркестрация охватывает все этапы конвейера ETL. Задания ETL в HDInsight часто включают в себя несколько совместно функционирующих различных продуктов.  Так, Hive можно использовать, чтобы очистить одну часть данных, а Pig — другую.  Фабрику данных Azure можно использовать для загрузки данных в базу данных SQL Azure из Azure Data Lake Store.

Оркестрация необходима для запуска соответствующего задания в соответствующее время.

### <a name="apache-oozie"></a>Apache Oozie

Apache Oozie — это система координации рабочих процессов, которая управляет заданиями Hadoop. Oozie работает в кластере HDInsight и интегрирована со стеком Hadoop. Эта система поддерживает задания Hadoop для Apache Hadoop MapReduce, Apache Pig, Apache Hive и Apache Sqoop. Ее также можно использовать для планирования относящихся к системе заданий, например Java-программ и сценариев оболочки.

Дополнительные сведения см. в статье [использование Apache Oozie с Apache Hadoop для определения и запуска рабочего процесса в HDInsight](../hdinsight-use-oozie-linux-mac.md). См. также [эксплуатацию конвейер данных](../hdinsight-operationalize-data-pipeline.md).

### <a name="azure-data-factory"></a>Фабрика данных Azure

Фабрика данных Azure предоставляет возможности оркестрации в форме "платформа как услуга". Это облачная служба интеграции данных, которая позволяет создавать управляемые данными рабочие процессы в облаке. Рабочие процессы для оркестрации и автоматизации перемещения и преобразования данных.

Используя фабрику данных Azure, можно выполнять такие задачи:

1. Создавать и включать в расписание управляемые данными рабочие процессы (конвейеры), которые принимают данные из разнородных хранилищ данных.
2. Обработка и преобразование данных с помощью служб вычислений, таких как Azure HDInsight Hadoop. Или Spark, Azure Data Lake Analytics, пакетная служба Azure и Машинное обучение Azure.
3. публиковать выходные данные в хранилища данных (например, хранилище данных SQL Azure) для использования приложениями бизнес-аналитики.

Дополнительные сведения о фабрике данных Azure см. в [этой статье](../../data-factory/introduction.md).

## <a name="ingest-file-storage-and-result-storage"></a>Хранилище файлов приема и хранилище результатов

Исходные файлы данных обычно загружаются в расположение в службе хранилища Azure или Azure Data Lake Storage. Файлы могут иметь любой формат, но обычно это плоские файлы, такие как CSV.

### <a name="azure-storage"></a>Хранилище Azure

Хранилище Azure имеет определенные целевые объекты адаптируемости. См. раздел [целевые показатели масштабируемости и производительности для хранилища BLOB-объектов](../../storage/blobs/scalability-targets.md). Для большинства аналитических узлов эта служба лучше всего масштабируется при использовании множества небольших файлов.  Служба хранилища Azure гарантирует ту же производительность, независимо от того, как размер файлов (при условии, что они находятся в пределах ограничений).  Эта гарантия означает, что вы можете хранить терабайты и по-прежнему получать согласованную производительность независимо от того, используете ли вы подмножество данных или все данные.

Служба хранилища Azure предоставляет несколько различных типов больших двоичных объектов.  *Добавочный большой двоичный объект* — оптимальный вариант для хранения веб-журналов или данных датчиков.  

Несколько больших двоичных объектов могут распределяться между несколькими серверами для масштабирования доступа к ним. Но один большой двоичный объект может обслуживаться только одним сервером. Хотя большие двоичные объекты могут быть логически сгруппированы в контейнеры, это не отражается на их распределении по разделам.

Служба хранилища Azure также содержит слой API WebHDFS для хранилища BLOB-объектов.  Все службы в HDInsight могут обращаться к файлам в хранилище BLOB-объектов Azure для очистки данных и обработки данных. Аналогично тому, как эти службы будут использовать систему распределенных файлов Hadoop (HDFS).

Прием данных в службу хранилища Azure выполняется с помощью PowerShell, SDK службы хранилища Azure или AZCopy.

### <a name="azure-data-lake-storage"></a>Azure Data Lake Storage

Azure Data Lake Storage (ADLS) — это управляемый, масштабируемый репозиторий. Репозиторий для аналитических данных, совместимых с HDFS.  ADLS использует парадигму проектирования, похожую на HDFS. ADLS обеспечивает неограниченную адаптируемость для общей емкости и размера отдельных файлов. ADLS хорошо подходит при работе с большими файлами, так как большой файл может храниться на нескольких узлах.  Секционирование данных в ADLS выполняется в фоновом режиме.  Это хранилище обеспечивает колоссальную пропускную способность: более тысячи исполнителей могут одновременно запускать аналитические задания, которые эффективно считывают и записывают тысячи терабайтов данных.

Данные обычно принимаются в ADLS с помощью фабрики данных Azure. Или ADLS SDK, AdlCopy Service, Apache DistCp или Apache Sqoop.  Выбор конкретных служб значительно зависит от расположения данных.  Если данные находятся в существующем кластере Hadoop, можно использовать Apache DistCp, службу AdlCopy или фабрику данных Azure.  Для данных в хранилище BLOB-объектов Azure вы можете использовать пакет SDK для Azure Data Lake Storage .NET, Azure PowerShell или фабрику данных Azure.

ADLS также оптимизирована для приема событий с помощью концентратора событий Azure или Apache Storm.

#### <a name="considerations-for-both-storage-options"></a>Рекомендации для двух вариантов хранения

Для отправки наборов данных в терабайтах задержки сети могут стать серьезной проблемой, особенно если источником данных является локальное расположение.  В таких ситуациях будут уместны следующие варианты:

* Azure ExpressRoute. Позволяет создавать частные подключения между центрами обработки данных Azure и локальной инфраструктурой. Такое подключение обеспечивает надежный вариант передачи больших объемов данных. Дополнительные сведения см. в [техническом обзоре ExpressRoute](../../expressroute/expressroute-introduction.md).

* "Автономная" передача данных. Вы можете использовать [службу импорта и экспорта Azure](../../storage/common/storage-import-export-service.md) для доставки жестких дисков с данными в центр обработки данных Azure. Данные сначала будут отправлены в хранилище BLOB-объектов Azure. Затем с помощью Фабрики данных Azure или инструмента AdlCopy можно скопировать данные из больших двоичных объектов службы хранилища Azure в Data Lake Storage.

### <a name="azure-sql-data-warehouse"></a>Хранилище данных SQL Azure

Хранилище данных SQL Azure отлично подходит для хранения подготовленных результатов.  Azure HDInsight можно использовать для этих служб в хранилище данных SQL Azure.

Хранилище данных SQL Azure (SQL DW) представляет собой реляционное хранилище базы данных, оптимизированное для аналитических рабочих нагрузок.  Масштабирование этого хранилища выполняется на основе секционированных таблиц.  Таблицы можно секционировать на нескольких узлах.  Узлы хранилища данных SQL Azure выбираются во время создания таблицы.  Затем их можно масштабировать, но для такого активного процесса может потребоваться перемещение данных. Дополнительные сведения см. в статье [хранилище данных SQL — Управление вычислениями](../../synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md).

### <a name="apache-hbase"></a>Apache HBase

Apache HBase представляет собой хранилище данных типа "ключ — значение", доступное в Azure HDInsight.  Apache HBase — это база данных NoSQL с открытым кодом, созданная на основе Hadoop и смоделированная после Google BigTable. HBase обеспечивает выполнение произвольного доступа и строгую согласованность для больших объемов неструктурированных и полуструктурированных данных. Данные в базе данных, не имеющие схемы, упорядоченные по семействам столбцов.

Данные хранятся в строках таблицы, данные в строке группируются по семейству столбцов. HBase — это безсхемная база данных. Столбцы и типы данных, которые хранятся в них, не нужно определять перед их использованием. Открытый код линейно масштабируется, чтобы обрабатывать петабайты данных на тысячах узлов. HBase может полагаться на избыточность данных, пакетную обработку и другие возможности, предоставляемые распределенными приложениями в среде Hadoop.

Это оптимальное место назначения для данных датчика и журнала для последующего анализа.

Адаптируемость HBase зависит от количества узлов в кластере HDInsight.

### <a name="azure-sql-database-and-azure-database"></a>База данных SQL Azure и Azure

Azure предлагает три различные реляционные базы данных по модели "платформа как услуга" (PAAS).

* [База данных SQL Azure](../../sql-database/sql-database-technical-overview.md) представляет собой реализацию Microsoft SQL Server. Дополнительные сведения о производительности см. в статье [Настройка производительности в Базе данных SQL Azure](../../sql-database/sql-database-performance-guidance.md).
* [База данных Azure для MySQL](../../mysql/overview.md) представляет собой реализацию Oracle MySQL.
* [База данных Azure для PostgreSQL](../../postgresql/quickstart-create-server-database-portal.md) представляет собой реализацию PostgreSQL.

Эти продукты масштабируются вверх, а это значит, что они масштабируются путем добавления дополнительных ресурсов ЦП и памяти.  Для них также можно использовать диски категории "Премиум", чтобы повысить производительность ввода-вывода.

## <a name="azure-analysis-services"></a>Службы Azure Analysis Services

Azure Analysis Services (AAS) — это модуль аналитических данных, используемый в службе поддержки решений и бизнес-аналитики. КОНСУЛЬТАНТы предоставляют аналитические данные для бизнес-отчетов и клиентских приложений, таких как Power BI. Кроме того, Excel, Reporting Services отчеты и другие средства визуализации данных.

Кубы анализа можно масштабировать, изменяя уровни для каждого отдельного куба.  Дополнительные сведения см. на странице [цен на службы Azure Analysis Services](https://azure.microsoft.com/pricing/details/analysis-services/).

## <a name="extract-and-load"></a>Извлечение и загрузка

Если данные находятся в Azure, можно использовать несколько служб для их извлечения и загрузки в другие продукты.  HDInsight поддерживает средства Sqoop и Flume.

### <a name="apache-sqoop"></a>Apache Sqoop

Apache Sqoop — это средство для эффективной передачи данных между структурированными, полуструктурированными и неструктурированными источниками данных.

Sqoop использует MapReduce для импорта и экспорта данных, чтобы обеспечить параллельное выполнение операций и отказоустойчивость.

### <a name="apache-flume"></a>Apache Flume

`Apache Flume`— это распределенная, надежная и доступная служба для эффективного сбора, агрегирования и перемещения больших объемов данных журнала. Flume имеет гибкую архитектуру на основе потоковых потоков данных. Это надежная отказоустойчивая служба с настраиваемыми механизмами для обеспечения надежности, а также механизмами отработки отказа и восстановления. Flume использует простую модель данных с возможностью расширения, позволяющую использовать интерактивное приложение аналитики.

Apache Flume нельзя использовать с Azure HDInsight.  Локально установленная версия Hadoop позволяет использовать Flume для отправки данных в Azure Storage Blobs или Azure Data Lake Storage.  Дополнительные сведения см. в записи блога об [использовании Apache Flume с HDInsight](https://web.archive.org/web/20190217104751/https://blogs.msdn.microsoft.com/bigdatasupport/2014/03/18/using-apache-flume-with-hdinsight/).

## <a name="transform"></a>Преобразование

Когда данные находятся в выбранном расположении, их необходимо очистить, объединить или подготовить для определенного шаблона использования.  Hive, Pig и Spark SQL — оптимальные варианты для такой работы.  Все они поддерживаются в HDInsight.

## <a name="next-steps"></a>Следующие шаги

* [Использование Apache Hive как средства для извлечения, преобразования и загрузки](apache-hadoop-using-apache-hive-as-an-etl-tool.md)
* [Использование Azure Data Lake Storage 2-го поколения с кластерами Azure HDInsight](../hdinsight-hadoop-use-data-lake-storage-gen2.md)
* [Перемещение данных из базы данных SQL Azure в Apache Hive таблицу](./apache-hadoop-use-sqoop-mac-linux.md)
