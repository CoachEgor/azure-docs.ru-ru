---
title: Оптимизация заданий Spark для повышения производительности в Azure HDInsight
description: Показать общие стратегии для наилучшей производительности кластеров Apache Spark в Azure HDInsight.
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: jasonh
ms.service: hdinsight
ms.topic: conceptual
ms.custom: hdinsightactive
ms.date: 04/17/2020
ms.openlocfilehash: 5012b5abf12beadbcb18f21fe2fe6ebfb076598a
ms.sourcegitcommit: eefb0f30426a138366a9d405dacdb61330df65e7
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/17/2020
ms.locfileid: "81617960"
---
# <a name="optimize-apache-spark-jobs-in-hdinsight"></a>Оптимизация рабочих мест Apache Spark в HDInsight

Узнайте, как оптимизировать конфигурацию кластера Apache Spark для определенной рабочей нагрузки.  Наиболее распространенной проблемой является давление памяти из-за неправильной конфигурации (например, исполнителей неправильного размера). Кроме того, длительные операции и задачи, которые приводят к декартовые операции. Ускорить выполнение заданий можно с помощью соответствующего кэширования и разрешения [неравномерного распределения данных](#optimize-joins-and-shuffles). Для наилучшей производительности отслеживайте и проверяйте длительные и ресурсоемкие выполнения задания Spark. Информацию о начале работы с Apache Spark на HDInsight можно получить [с помощью портала Azure.](apache-spark-jupyter-spark-sql-use-portal.md)

В разделах ниже описаны общие рекомендации и распространенные методы оптимизации задания Spark.

## <a name="choose-the-data-abstraction"></a>Выбор абстракции данных

Ранее версии Spark использовали RDD для абстрактных данных, Spark 1.3 и 1.6 представили DataFrames и DataSets, соответственно. Рассмотрим следующие относительные характеристики:

* **DataFrames**
    * Оптимальный вариант в большинстве случаев.
    * Предоставляет оптимизацию запросов через Catalyst.
    * Комплексное создание кода.
    * Прямой доступ к памяти.
    * Низкие накладные расходы при сборке мусора.
    * Не настолько удобны для разработчиков, как наборы данных, так как отсутствуют проверки со временем компиляции или программирование на основе объекта домена.
* **Наборов данных**
    * Подходят для использования в сложных конвейерах ETL, где допустимо влияние производительности.
    * Не подходят для использования в статистических функциях, где весомо влияние производительности.
    * Предоставляет оптимизацию запросов через Catalyst.
    * Удобны для разработчиков, так как обеспечивают программирование на основе объекта домена и проверки со временем компиляции.
    * Увеличивают нагрузку при десериализации и сериализации.
    * Высокие накладные расходы при сборке мусора.
    * Разбивают комплексное создание кода на этапы.
* **Устойчивые распределенные наборы данных (RDD)**
    * Вам не нужно использовать RDD, если вам не нужно создавать новый пользовательский RDD.
    * Отсутствует оптимизация запросов через Catalyst.
    * Отсутствует комплексное создание кода.
    * Высокие накладные расходы при сборке мусора.
    * Необходимо использовать устаревшие API-интерфейсы Spark 1.x.

## <a name="use-optimal-data-format"></a>Использование оптимального формата данных

Spark поддерживает многие форматы, такие как CSV, JSON, XML, PARQUET, ORC и AVRO. С помощью внешних источников данных его можно расширить для поддержки большего количества форматов. Дополнительные сведения см. на странице [пакетов Apache Spark](https://spark-packages.org).

Лучший формат для повышения производительности — PARQUET со *сжатием Snappy*, который является стандартным форматом в кластере Spark 2.x. В формате PARQUET данные хранятся в столбцах. Этот формат высоко оптимизирован в Spark.

## <a name="select-default-storage"></a>Выбор хранилища по умолчанию

При создании нового кластера Spark можно выбрать хранилище Azure Blob Storage или Azure Data Lake Storage в качестве хранилища по умолчанию. Оба варианта дают преимущество долгосрочного хранения переходных кластеров. Таким образом, ваши данные не удаляются автоматически при удалении кластера. Вы можете повторно создать промежуточный кластер и по-прежнему иметь доступ к данным.

| Тип хранилища данных | Файловая система | Speed | Промежуточный | Варианты использования |
| --- | --- | --- | --- | --- |
| хранилище BLOB-объектов Azure | **wasb:**//url/ | **Standard Edition** | Да | Промежуточный кластер |
| Хранение Azure Blob (безопасно) | **wasbs:**//url/ | **Standard Edition** | Да | Промежуточный кластер |
| Azure Data Lake Storage 2-го поколения| **abfs:**//url/ | **Более быстрая** | Да | Промежуточный кластер |
| Лазурное хранилище озер данных Gen 1| **adl:**//url/ | **Более быстрая** | Да | Промежуточный кластер |
| Локальная система HDFS | **hdfs:**//url/ | **Самая быстрая** | нет | Интерактивный постоянно доступный кластер |

Полное описание параметров хранения данных приведено в описании [см.](../hdinsight-hadoop-compare-storage-options.md)

## <a name="use-the-cache"></a>Использование кэша

Spark обеспечивает собственные механизмы кэширования, которые можно использовать с помощью различных методов, например `.persist()`, `.cache()` и `CACHE TABLE`. Это родное кэширование эффективно с небольшими наборами данных и в конвейерах ETL, где необходимо кэшировать промежуточные результаты. Однако в настоящее время родное кэширование Spark не работает хорошо с разделиванием, так как кэшированная таблица не хранит данные раздела. Более универсальным и надежным способом кэширования является *кэширование на уровне хранилища*.

* Встроенное кэширование Spark (не рекомендуется)
    * Подходит для небольших наборов данных.
    * Не работает с разделами, которые могут измениться в будущих релизах Spark.

* Кэширование на уровне хранилища (рекомендуется)
    * Может быть реализован на HDInsight с помощью функции [IO Кэш.](apache-spark-improve-performance-iocache.md)
    * Использует кэширование SSD и в памяти.

* Локальная система HDFS (рекомендуется)
    * Путь `hdfs://mycluster`.
    * Использует кэширование SSD.
    * Кэшированные данные будут потеряны при удалении кластера, что требует перестроения кэша.

## <a name="use-memory-efficiently"></a>Эффективное использование памяти

Spark работает путем размещения данных в памяти. Таким образом, управление ресурсами памяти является ключевым аспектом оптимизации выполнения заданий Spark.  Есть несколько методов, которые можно применить для эффективного использования памяти кластера.

* В рамках стратегии секционирования рекомендуется выбирать небольшие секции данных и учитывать размер данных, типы и распределение.
* Рассмотрим более новую, более эффективную, [`Kryo data serialization`](https://github.com/EsotericSoftware/kryo)а не сериализацию Java по умолчанию.
* Рекомендуется использовать YARN, так как можно разделить `spark-submit` по пакету.
* Отслеживайте и настраивайте параметры конфигурации Spark.

Для справки структура памяти Spark и некоторые основные параметры памяти исполнителя показаны на рисунке ниже.

### <a name="spark-memory-considerations"></a>Рекомендации по использованию памяти Spark

Если вы используете Apache Hadoop YARN, то YARN контролирует память, используемую всеми контейнерами на каждом узлах Spark.  На схеме ниже показаны ключевые объекты и их связи.

![Управление памятью Spark в YARN](./media/apache-spark-perf/apache-yarn-spark-memory.png)

При получении сообщений о нехватке памяти сделайте следующее:

* Просмотрите операции перемешивания при управлении группами обеспечения доступности баз данных. Ограничьте их путем снижения на стороне сопоставления, выполните предварительное секционирование (или разбиение на группы) исходных данных, увеличьте объем операций перемешивания для отдельных процессов и сократите объем отправляемых данных.
* Выберите `ReduceByKey` с фиксированным объемом памяти, а не `GroupByKey`, который обеспечивает статистические функции, управление окнами и другие возможности, но включает неограниченный объем памяти.
* Выберите `TreeReduce`, который в основном обрабатывает исполнителей или секции, а не `Reduce`, который в основном обрабатывает драйвер.
* Используйте DataFrames, а не низкоуровневые объекты RDD.
* Создайте типы ComplexTypes, инкапсулирующие действия, такие как "Первые N", различные статистические функции или операции управления окнами.

Дополнительные действия по устранению неполадок см. [исключения OutOfMemoryError для Apache Spark в Azure HDInsight.](apache-spark-troubleshoot-outofmemory.md)

## <a name="optimize-data-serialization"></a>Оптимизация сериализации данных

Так как задания кластера Spark можно распределить, соответствующая сериализация данных представляет собой важный шаг для повышения производительности.  Есть два варианта сериализации данных Spark:

* Сериализация Java, используемая по умолчанию.
* `Kryo`сериализация является новым форматом и может привести к более быстрой и компактной сериализации, чем Java.  `Kryo`требует, чтобы вы зарегистрировали классы в вашей программе, и она еще не поддерживает все serializable типов.

## <a name="use-bucketing"></a>Использование группирования

Ведра похожи на раздел данных. Но каждое ведро может содержать набор значений столбцов, а не только одно. Этот метод хорошо работает для раздела на больших (в миллионах или более) значениях, таких как идентификаторы продукта. Контейнер определяется хэшированием ключа контейнера строки. Таблицы в контейнерах предлагают уникальную оптимизацию, так как в них хранятся метаданные о способах группирования и сортировки.

Ниже приведены некоторые расширенные функции группирования.

* Оптимизация запросов на основе группирования метасведений.
* Оптимизированные статистические функции.
* Оптимизированные соединения.

Вы можете одновременно использовать секционирование и группирование.

## <a name="optimize-joins-and-shuffles"></a>Оптимизация операций соединения и перемешивания

Если у вас есть медленные задания на Join или Shuffle, причиной, вероятно, является *перекос данных.* Перекос данных — это асимметрия в данных о работе. Например, задание карты может занять 20 секунд. Но запуск задания, где данные соединены или перетасовываются занимает несколько часов. Для устранения неравномерного распределения данных необходимо использовать строку случайных данных для целого ключа или *изолированную строку случайных данных* только для некоторого подмножества ключей. Если вы используете изолированную соль, вы должны дополнительно фильтровать, чтобы изолировать подмножество соленых ключей в соединениях карты. Другой вариант — создать столбец группы и выполнить предварительную статистическую обработку сначала в группах.

На скорость выполнения операции соединения также влияет тип этой операции. По умолчанию кластер Spark использует тип соединения `SortMerge`. Этот тип соединения лучше всего подходит для больших наборов данных. Но в противном случае вычислительно дорого, потому что он должен сначала сортировать левую и правую стороны данных, прежде чем их слияния.

Соединение `Broadcast` лучше всего подходит для небольших наборов данных, или в случаях, когда одна сторона соединения значительно меньше другой. Этот тип соединения оповещает все исполнители, поэтому в целом требует большего объема памяти для такой операции передачи.

Тип соединения в конфигурации можно изменить, задав `spark.sql.autoBroadcastJoinThreshold`, или можно задать подсказку по соединению с помощью API-интерфейсов DataFrame (`dataframe.join(broadcast(df2))`).

```scala
// Option 1
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 1*1024*1024*1024)

// Option 2
val df1 = spark.table("FactTableA")
val df2 = spark.table("dimMP")
df1.join(broadcast(df2), Seq("PK")).
    createOrReplaceTempView("V_JOIN")

sql("SELECT col1, col2 FROM V_JOIN")
```

Если вы используете заведонные столы, то `Merge` у вас есть третий тип соединения, соединение. В соединении `SortMerge` правильно предварительно секционированный и отсортированный набор данных пропустит дорогостоящий этап сортировки.

Порядок соединений имеет значение, особенно в более сложных запросах. Начните с наиболее часто используемых соединений. Кроме того, по возможности перемещайте соединения, увеличивающие количество строк после статистической обработки.

Чтобы управлять параллелизмом для декартовых соединений, можно добавить вложенные структуры, оконные окна и, возможно, пропустить один или несколько шагов в Spark Job.

## <a name="customize-cluster-configuration"></a>Настройка конфигурации кластера

В зависимости от рабочей нагрузки кластера Spark можно определить, что конфигурация Spark по умолчанию приведет к более оптимизированной выполнению заданий Spark.  Проверьте тест с примерными рабочими нагрузками для проверки любых конфигураций кластеров, не по умолчанию.

Ниже приведены некоторые общие параметры, которые можно изменить:

|Параметр |Описание |
|---|---|
|--нум-исполнители|Устанавливает соответствующее количество исполнителей.|
|--исполнитель-ядра|Устанавливает количество ядер для каждого исполнителя. Обычно используются исполнители среднего размера, так как некоторые процессы используют доступную память.|
|--Исполнитель-память|Устанавливает размер памяти для каждого исполнителя, который контролирует размер кучи на YARN. Оставьте некоторую память для выполнения накладных расходов.|

### <a name="select-the-correct-executor-size"></a>Выбор правильного размера исполнителя

При выборе конфигурации исполнителя предусмотрите лимит переполнения памяти при сборке мусора Java.

* Факторы, которые стоит учесть, чтобы выбрать исполнитель меньшего размера:
    1. Уменьшите размер кучи до 32 ГБ, чтобы лимит переполнения памяти при сборке мусора не превышал 10 %.
    2. Уменьшите количество ядер, чтобы лимит переполнения памяти при сборке мусора не превышал 10 %.

* Факторы, которые стоит учесть, чтобы выбрать исполнитель большего размера:
    1. Уменьшите лимит переполнения памяти при обмене данными между исполнителями.
    2. Уменьшите количество открытых подключений между исполнителями (N2) в больших кластерах (более 100 исполнителей).
    3. Увеличьте размер кучи для обработки задач с интенсивным потреблением ресурсов памяти.
    4. (Необязательно.) Уменьшите лимит переполнения памяти каждого исполнителя.
    5. Дополнительно: Увеличьте использование и параллелизм, переподписав процессор.

Как правило, при выборе размера исполнителя:

1. Начните с 30 ГБ на каждый исполнитель и распределите доступные ядра компьютера.
2. Увеличьте количество ядер исполнителя для больших кластеров (более 100 исполнителей).
3. Изменение размера на основе как пробных запусков, так и предыдущих факторов, таких как накладные расходы GC.

При запуске одновременных запросов следует учитывать:

1. Начните с 30 ГБ на каждый исполнитель и для всех ядер компьютера.
2. Создайте несколько параллельных приложений Spark, увеличив число назначенных ЦП (уменьшение задержки приблизительно на 30 %).
3. Распределите запросы между параллельными приложениями.
4. Изменение размера на основе как пробных запусков, так и предыдущих факторов, таких как накладные расходы GC.

Для получения дополнительной информации об использовании Ambari для настройки исполнителей, [см.](apache-spark-settings.md#configuring-spark-executors)

Мониторинг производительности запроса для выбросов или других проблем с производительностью, при просмотре представления временной шкалы. Кроме того, график S'L, статистика работ, и так далее. Для получения информации о отладке заданий Spark с использованием YARN и сервера Spark History [см. Задания Debug Apache Spark, работающие на Azure HDInsight.](apache-spark-job-debugging.md) Советы по использованию сервера YARN Timeline Server можно узнать в [журналах приложений Access Apache Hadoop YARN.](../hdinsight-hadoop-access-yarn-app-logs-linux.md)

Иногда один или несколько исполнителей работают медленнее по сравнению с другими, и выполнение задач занимает гораздо больше времени. Эта медлительность часто происходит на больших кластерах (> 30 узлов). В этом случае разделите работу на большое число задач, чтобы планировщик мог компенсировать их медленное выполнение. Например, создайте как минимум в два раза больше задач по сравнению с количеством ядер исполнителя в приложении. Можно также включить упреждающее выполнение задач с помощью `conf: spark.speculation = true`.

## <a name="optimize-job-execution"></a>Оптимизация выполнения задания

* При необходимости выполните кэширование, например при повторном использовании данных.
* Передайте переменные во все исполнители. Переменные сериализуются только один раз, за счет чего поиск ускоряется.
* Используйте пул потока в драйвере, что ускорит выполнение нескольких задач.

Регулярно отслеживайте выполняющиеся задания для обнаружения проблем с производительностью. Если необходимо получить дополнительные сведения об определенных проблемах, ознакомьтесь с одним из следующих инструментов профилирования.

* [Intel PAL Tool](https://github.com/intel-hadoop/PAT) отслеживает использование процессора, хранения и пропускной способности сети.
* [Oracle Java 8 Mission Control](https://www.oracle.com/technetwork/java/javaseproducts/mission-control/java-mission-control-1998576.html) — формирует профили Spark и кода исполнителя.

Ключевым аспектом производительности запроса Spark 2.x является механизм Tungsten, который зависит от комплексного создания кода. В некоторых случаях комплексное создание кода можно отключить. Например, если в статистическом выражении используется неизменяемый тип (`string`), вместо `HashAggregate` появится `SortAggregate`. Например, для повышения производительности запустите команду ниже, а затем повторно включите создание кода:

```sql
MAX(AMOUNT) -> MAX(cast(AMOUNT as DOUBLE))
```

## <a name="next-steps"></a>Дальнейшие действия

* [Отладка заданий Apache Spark в Azure HDInsight](apache-spark-job-debugging.md)
* [Управление ресурсами для кластера Apache Spark в HDInsight](apache-spark-resource-manager.md)
* [Настройка параметров Apache Spark](apache-spark-settings.md)
* [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html) (Настройка Spark)
* [How to Actually Tune Your Spark Jobs So They Work](https://www.slideshare.net/ilganeli/how-to-actually-tune-your-spark-jobs-so-they-work) (О настройке заданий Spark для их оптимальной работы)
* [`Kryo Serialization`](https://github.com/EsotericSoftware/kryo)
