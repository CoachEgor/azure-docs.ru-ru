---
title: Настройка параметров Spark для Azure HDInsight
description: Просмотр и Настройка параметров Apache Spark для кластера Azure HDInsight
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive
ms.topic: conceptual
ms.date: 06/17/2019
ms.openlocfilehash: 2d369af7c11473d811677f33f9112d41260fcecf
ms.sourcegitcommit: 97605f3e7ff9b6f74e81f327edd19aefe79135d2
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/06/2019
ms.locfileid: "70736018"
---
# <a name="configure-apache-spark-settings"></a>Настройка параметров Apache Spark

Кластер HDInsight Spark включает в себя установку библиотеки [Apache Spark](https://spark.apache.org/).  Каждый кластер HDInsight содержит параметры конфигурации по умолчанию для всех установленных служб, в том числе для Spark.  Ключевым аспектом управления кластером Apache HDInsight Hadoop является мониторинг рабочей нагрузки, включая задания Spark. Так вы можете убедиться, что все задания выполняются надлежащим образом. Чтобы обеспечить оптимальное выполнение заданий Spark, при определении варианта оптимизации логической конфигурации кластера рассмотрите его физическую конфигурацию.

По умолчанию кластер HDInsight Apache Spark содержит следующие узлы: три узла [Apache ZooKeeper](https://zookeeper.apache.org/), два головных узла и один или несколько рабочих узлов:

![Архитектура Spark HDInsight](./media/apache-spark-settings/spark-hdinsight-arch.png)

Количество и размеры виртуальных машин для узлов кластера HDInsight также могут повлиять на конфигурацию Spark. Нестандартные значения конфигурации HDInsight часто требуют нестандартных значений конфигурации Spark. При создании кластера HDInsight Spark отображаются предлагаемые размеры виртуальной машины для каждого из компонентов. Сейчас [размерами виртуальных машин Azure с Linux, оптимизированными для памяти,](../../virtual-machines/linux/sizes-memory.md) являются экземпляры D12 версии 2 или выше.

## <a name="apache-spark-versions"></a>Версии Apache Spark

Используйте наиболее подходящую версию Spark для кластера.  Служба HDInsight включает в себя несколько версий Spark и HDInsight.  Каждая версия Spark содержит набор стандартных параметров кластера.  

Ниже приведены разные версии Spark. Вы можете выбрать одну из них для создания кластера. См. полный список [версий и компонентов HDInsight](https://docs.microsoft.com/azure/hdinsight/hdinsight-component-versioning)


> [!NOTE]  
> Версия по умолчанию Apache Spark в службе HDInsight может измениться без предварительного уведомления. Если используется зависимость версии, корпорация Майкрософт рекомендует указать конкретную версию при создании кластеров с использованием пакета SDK для .NET, Azure PowerShell и классического Azure CLI.

В Apache Spark системная конфигурация находится в трех расположениях.

* Свойства Spark управляют большинством параметров приложения. Их можно установить с помощью объекта `SparkConf` или свойств системы Java.
* Переменные среды можно использовать для установки параметров (например, IP-адреса) отдельного компьютера на каждом узле с помощью скрипта `conf/spark-env.sh`.
* Ведение журнала можно настроить с помощью `log4j.properties`.

Кластер Spark любой версии содержит параметры конфигурации по умолчанию.  Вы можете изменить их, используя настраиваемый файл конфигурации Spark.  Ниже приведен пример такого файла.

```
    spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.GzipCodec
    spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776
    spark.hadoop.parquet.block.size 1099511627776
    spark.sql.files.maxPartitionBytes 1099511627776
    spark.sql.files.openCostInBytes 1099511627776
```

В примере выше переопределяется несколько значений по умолчанию для пяти параметров конфигурации Spark.  К ним относятся кодек сжатия, минимальный размер разделения для заданий MapReduce в Apache Hadoop, размер блоков Parquet, а также раздел Spar SQL и значения по умолчанию для размеров открытых файлов.  Мы выбрали эти изменения конфигурации, так как они лучше подходят для используемых данных и заданий (в этом примере геномные данные).

---

## <a name="view-cluster-configuration-settings"></a>Просмотр параметров конфигурации кластера

Прежде чем выполнять оптимизацию производительности кластера, проверьте текущие параметры конфигурации кластера HDInsight. Запустите панель мониторинга HDInsight на портале Azure, щелкнув ссылку **Панель мониторинга** в области кластера Spark. Войдите с помощью имени пользователя и пароля администратора кластера.

Отобразится веб-интерфейс Apache Ambari с представлением ключевых метрик использования ресурсов кластера.  На панели мониторинга Ambari отображается конфигурация Apache Spark и другие установленные службы. На панели мониторинга есть вкладка **Config History** (История конфигураций). Здесь содержится информация о конфигурации всех установленных служб, включая Spark.

Чтобы просмотреть значения конфигурации для Apache Spark, выберите **Config History** (История конфигураций), а затем — **Spark2**.  Перейдите на вкладку **Configs** (Конфигурации), а затем в списке служб щелкните ссылку `Spark` (или `Spark2` в зависимости от вашей версии).  Отобразится список значений конфигурации для вашего кластера:

![Конфигурации Spark](./media/apache-spark-settings/spark-config.png)

Чтобы просмотреть и изменить отдельные значения конфигурации Spark, щелкните ссылку со словом "Spark" в названии.  Конфигурации для Spark содержат как пользовательские, так и расширенные значения конфигурации в следующих категориях:

* Custom Spark2-defaults;
* Custom Spark2-metrics-properties;
* Advanced Spark2-defaults;
* Advanced Spark2-env;
* Advanced spark2-hive-site-override;

Если вы создали нестандартный набор значений конфигурации, можно также просмотреть историю обновлений конфигурации.  В истории конфигурации можно увидеть, какая нестандартная конфигурация имеет оптимальную производительность.

> [!NOTE]  
> Чтобы увидеть (но не изменять) общие параметры конфигурации кластера Spark, выберите вкладку **Environment** (Среда) в интерфейсе верхнего уровня **Spark Job UI** (Пользовательский интерфейс задания Spark).

## <a name="configuring-spark-executors"></a>Настройка исполнителей Spark

На следующей схеме показаны ключевые объекты Spark. К ним относятся программа драйвера и связанный с ней контекст Spark, а также диспетчер кластера и его рабочие узлы *n*.  Каждый рабочий узел включает в себя исполнитель, кэш и экземпляры задач *n*.

![Объекты кластера](./media/apache-spark-settings/spark-arch.png)

Задания Spark используют рабочие ресурсы, в частности память, поэтому для исполнителей рабочего узла необходимо настроить значения конфигурации Spark.

Существует три ключевых параметра — `spark.executor.instances`, `spark.executor.cores` и `spark.executor.memory`. Их часто изменяют, чтобы настроить конфигурации Spark в соответствии с требованиями приложения. Исполнитель — это процесс, запущенный для приложения Spark. Он выполняется на рабочем узле и отвечает за выполнение задач этого приложения. Число исполнителей по умолчанию и размеры исполнителя для каждого кластера определяются с учетом числа рабочих узлов и размера каждого рабочего узла. Эти значения хранятся в файле `spark-defaults.conf` на головных узлах кластера.  Вы можете отредактировать эти значения в запущенном кластере, щелкнув ссылку **Custom spark-defaults** (Настраиваемые значения по умолчанию Spark) в веб-интерфейсе Ambari.  После внесения изменений пользовательский интерфейс запрашивает **перезапуск** всех затронутых служб.

> [!NOTE]  
> Эти три параметра конфигурации можно настроить на уровне кластера (для всех приложений, работающих в кластере) или для каждого отдельного приложения.

Другим источником информации о ресурсах, используемых исполнителями Spark, является пользовательский интерфейс приложения Spark.  В пользовательском интерфейсе Spark выберите вкладку **Executors** (Исполнители), чтобы отобразить сводные и подробные представления конфигурации и ресурсов, потребляемых исполнителями.  Эти представления могут помочь определить, для чего следует изменить значения по умолчанию исполнителей Spark: для всего кластера или для определенного набора заданий.

![Исполнители Spark](./media/apache-spark-settings/spark-executors.png)

Кроме того, можно использовать REST API Ambari для проверки параметров конфигурации кластера HDInsight и Spark программным способом.  Дополнительные сведения см. в [справочнике по API Apache Ambari на сайте GitHub](https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md).

В зависимости от рабочей нагрузки Spark может выясниться, что нестандартная конфигурация Spark оптимизирует выполнение задания.  Необходимо выполнить тестирование производительности примеров рабочих нагрузок, чтобы проверить все нестандартные конфигурации кластера.  Можно настроить такие общие параметры:

* `--num-executors` задает количество исполнителей.
* `--executor-cores` задает количество ядер для каждого исполнителя. Мы советуем выбирать исполнителей среднего размера, так как некоторые процессы используют доступную память.
* `--executor-memory` управляет размером памяти (размером кучи) каждого исполнителя в [Apache Hadoop YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html). Вам нужно оставить определенный объем памяти для выполнения служебных программ.

Ниже приведен пример двух рабочих узлов с разными значениями конфигурации.

![Конфигурации двух узлов](./media/apache-spark-settings/executor-config.png)

Ниже приведен список ключевых параметров памяти исполнителя Spark.

* `spark.executor.memory` определяет общий объем памяти, доступный для исполнителя.
* `spark.storage.memoryFraction` (по умолчанию ~ 60 %) определяет объем памяти, доступный для хранения устойчивых распределенных наборов данных (RDD).
* `spark.shuffle.memoryFraction` (по умолчанию ~ 20 %) определяет объем памяти, зарезервированный для перемешивания.
* `spark.storage.unrollFraction` и `spark.storage.safetyFraction` (30 % от общей памяти). Эти значения используются внутри Spark и не должны меняться.

YARN управляет максимальным объемом памяти, используемой контейнерами на всех узлах Spark. На следующей схеме показаны отношения между объектами конфигурации YARN и Spark для каждого узла.

![Управление памятью Spark в YARN](./media/apache-spark-settings/yarn-spark-memory.png)

## <a name="change-parameters-for-an-application-running-in-jupyter-notebook"></a>Изменение параметров для приложения, запущенного в записной книжке Jupyter

Кластеры Spark в HDInsight включают в себя несколько компонентов по умолчанию. Каждый из этих компонентов содержит значения конфигурации по умолчанию, которые можно переопределить при необходимости.

* Ядро Spark, Spark SQL, потоковые API-интерфейсы Spark, GraphX и Apache Spark MLlib.
* Anaconda — диспетчер пакетов Python.
* [Apache Livy](https://livy.incubator.apache.org/) — удаленная отправка заданий Spark в кластер Azure HDInsight с помощью Apache Spark REST API.
* Записные книжки [Jupyter](https://jupyter.org/) и [Apache Zeppelin](https://zeppelin.apache.org/) — интерактивный браузерный интерфейс для взаимодействия с кластером Spark.
* Драйвер ODBC. Соединяет кластеры Spark в HDInsight со средствами бизнес-аналитики, такими как Microsoft Power BI и Tableau.

Чтобы изменить конфигурацию для приложений, запущенных в записной книжке Jupyter, можно использовать команду `%%configure`. Эти изменения конфигурации будут применены к заданиям Spark, запущенным из экземпляра записной книжки. Необходимо вносить такие изменения на раннем этапе выполнения приложения, перед запуском первой ячейки кода. Измененная конфигурация будет применена к сеансу Livy, когда он будет создан.

> [!NOTE]  
> Чтобы изменить конфигурацию на более позднем этапе выполнения приложения, используйте параметр `-f` (принудительное выполнение). Но при этом будут потеряны все результаты, полученные в приложении.

В следующем коде показано, как изменить конфигурацию для приложения, работающего в записной книжке Jupyter.

```
    %%configure
    {"executorMemory": "3072M", "executorCores": 4, "numExecutors":10}
```

## <a name="conclusion"></a>Заключение

Существует ряд основных параметров конфигурации, которые необходимо отслеживать и настраивать, чтобы задания Spark выполнялись прогнозируемым и эффективным образом. Эти параметры помогают определить лучшую конфигурацию кластера Spark для конкретных рабочих нагрузок.  Вам также необходимо следить за выполнением длительных или ресурсоемких выполнений заданий Spark.  Наиболее распространенной проблемой является нехватка памяти. Причинами этой проблемы можно назвать неправильные конфигурации (в частности, неправильные размеры исполнителей), длительные операции и задачи, которые приводят к декартовым операциям.

## <a name="next-steps"></a>Следующие шаги

* [Компоненты и версии Apache Hadoop, доступные в HDInsight](../hdinsight-component-versioning.md)
* [Управление ресурсами для кластера Apache Spark в HDInsight](apache-spark-resource-manager.md)
* [Установка кластеров в HDInsight с использованием Apache Hadoop, Apache Spark, Apache Kafka и других технологий](../hdinsight-hadoop-provision-linux-clusters.md)
* [Сведения о конфигурации Spark](https://spark.apache.org/docs/latest/configuration.html)
* [Выполнение Apache Spark в Apache Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html)
