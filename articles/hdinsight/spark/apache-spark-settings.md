---
title: Настройка параметров Spark для Azure HDInsight
description: Как просмотреть и настроить настройки Apache Spark для кластера Azure HDInsight
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: jasonh
ms.service: hdinsight
ms.topic: conceptual
ms.custom: hdinsightactive
ms.date: 04/15/2020
ms.openlocfilehash: e13390067f8767e8c07b9c189264444e6d999a7a
ms.sourcegitcommit: b80aafd2c71d7366838811e92bd234ddbab507b6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/16/2020
ms.locfileid: "81411299"
---
# <a name="configure-apache-spark-settings"></a>Настройка параметров Apache Spark

Кластер HDInsight Spark включает в себя установку библиотеки [Apache Spark](https://spark.apache.org/).  Каждый кластер HDInsight содержит параметры конфигурации по умолчанию для всех установленных служб, в том числе для Spark.  Ключевым аспектом управления кластером HDInsight Apache Hadoop является мониторинг рабочей нагрузки, включая Spark Jobs. Чтобы лучше всего запускать задания Spark, учитывайте конфигурацию физического кластера при определении логической конфигурации кластера.

По умолчанию кластер HDInsight Apache Spark содержит следующие узлы: три узла [Apache ZooKeeper](https://zookeeper.apache.org/), два головных узла и один или несколько рабочих узлов:

![Архитектура Spark HDInsight](./media/apache-spark-settings/spark-hdinsight-arch.png)

Количество VMs и VM размеров для узлов в кластере HDInsight может повлиять на конфигурацию Spark. Нестандартные значения конфигурации HDInsight часто требуют нестандартных значений конфигурации Spark. При создании кластера HDInsight Spark показаны предлагаемые размеры VM для каждого из компонентов. Сейчас [размерами виртуальных машин Azure с Linux, оптимизированными для памяти,](../../virtual-machines/linux/sizes-memory.md) являются экземпляры D12 версии 2 или выше.

## <a name="apache-spark-versions"></a>Версии Apache Spark

Используйте наиболее подходящую версию Spark для кластера.  Служба HDInsight включает в себя несколько версий Spark и HDInsight.  Каждая версия Spark содержит набор стандартных параметров кластера.  

Ниже приведены разные версии Spark. Вы можете выбрать одну из них для создания кластера. Чтобы увидеть полный список, [HDInsight компоненты и версии](https://docs.microsoft.com/azure/hdinsight/hdinsight-component-versioning).

> [!NOTE]  
> Версия по умолчанию Apache Spark в службе HDInsight может измениться без предварительного уведомления. Если используется зависимость версии, корпорация Майкрософт рекомендует указать конкретную версию при создании кластеров с использованием пакета SDK для .NET, Azure PowerShell и классического Azure CLI.

В Apache Spark системная конфигурация находится в трех расположениях.

* Свойства Spark управляют большинством параметров приложения. Их можно установить с помощью объекта `SparkConf` или свойств системы Java.
* Переменные среды можно использовать для установки параметров (например, IP-адреса) отдельного компьютера на каждом узле с помощью скрипта `conf/spark-env.sh`.
* Ведение журнала можно настроить с помощью `log4j.properties`.

Кластер Spark любой версии содержит параметры конфигурации по умолчанию.  Вы можете изменить их, используя настраиваемый файл конфигурации Spark.  Ниже приведен пример такого файла.

```
spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.GzipCodec
spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776
spark.hadoop.parquet.block.size 1099511627776
spark.sql.files.maxPartitionBytes 1099511627776
spark.sql.files.openCostInBytes 1099511627776
```

В примере выше переопределяется несколько значений по умолчанию для пяти параметров конфигурации Spark.  Эти значения являются кодек сжатия, Apache Hadoop MapReduce разделить минимальный размер и паркетный блок размеров. Кроме того, раздел Spar S'L и открытые размеры файла значения по умолчанию.  Эти изменения конфигурации выбираются потому, что связанные данные и задания (в данном примере геномные данные) имеют особые характеристики. Эти характеристики будут лучше использовать эти пользовательские настройки конфигурации.

---

## <a name="view-cluster-configuration-settings"></a>Просмотр параметров конфигурации кластера

Проверьте текущие настройки конфигурации кластера HDInsight перед оптимизацией производительности кластера. Запустите панель мониторинга HDInsight на портале Azure, щелкнув ссылку **Панель мониторинга** в области кластера Spark. Вопинье с именем пользователя и паролем администратора кластера.

Отосевается веб-uI Apache Ambari с панелью мониторинга ключевых показателей использования ресурсов кластера.  На панели мониторинга Ambari показана конфигурация Apache Spark и другие установленные службы. Панель мониторинга включает вкладку **Config History,** где просматривается информация для установленных служб, включая Spark.

Чтобы просмотреть значения конфигурации для Apache Spark, выберите **Config History** (История конфигураций), а затем — **Spark2**.  Перейдите на вкладку **Configs** (Конфигурации), а затем в списке служб щелкните ссылку `Spark` (или `Spark2` в зависимости от вашей версии).  Отобразится список значений конфигурации для вашего кластера:

![Конфигурации Spark](./media/apache-spark-settings/spark-configurations.png)

Чтобы увидеть и изменить отдельные значения конфигурации Spark, выберите любую ссылку с "искра" в названии.  Конфигурации для Spark содержат как пользовательские, так и расширенные значения конфигурации в следующих категориях:

* Custom Spark2-defaults;
* Custom Spark2-metrics-properties;
* Advanced Spark2-defaults;
* Advanced Spark2-env;
* Advanced spark2-hive-site-override;

При создании набора значений конфигурации без значения конфигурации, история обновления видна.  В истории конфигурации можно увидеть, какая нестандартная конфигурация имеет оптимальную производительность.

> [!NOTE]  
> Чтобы увидеть (но не изменять) общие параметры конфигурации кластера Spark, выберите вкладку **Environment** (Среда) в интерфейсе верхнего уровня **Spark Job UI** (Пользовательский интерфейс задания Spark).

## <a name="configuring-spark-executors"></a>Настройка исполнителей Spark

На следующей схеме показаны ключевые объекты Spark. К ним относятся программа драйвера и связанный с ней контекст Spark, а также диспетчер кластера и его рабочие узлы *n*.  Каждый рабочий узел включает в себя исполнитель, кэш и экземпляры задач *n*.

![Объекты кластера](./media/apache-spark-settings/hdi-spark-architecture.png)

Задания Spark используют рабочие ресурсы, в частности память, поэтому для исполнителей рабочего узла необходимо настроить значения конфигурации Spark.

Существует три ключевых параметра — `spark.executor.instances`, `spark.executor.cores` и `spark.executor.memory`. Их часто изменяют, чтобы настроить конфигурации Spark в соответствии с требованиями приложения. Исполнитель — это процесс, запущенный для приложения Spark. Он выполняется на рабочем узле и отвечает за выполнение задач этого приложения. Количество узлов и размер узлов рабочего определяют количество исполнителей и размеры исполнителей. Эти значения хранятся `spark-defaults.conf` в на узлах головных узлов кластера.  Эти значения можно отсеивать в запущенном кластере, выбрав **пользовательские искры-по умолчанию** в пользовательском пользовательском пользовательском пользовательском пользовательском пользовательском пользовательском пользовательском варианте.  После внесения изменений пользовательский интерфейс запрашивает **перезапуск** всех затронутых служб.

> [!NOTE]  
> Эти три параметра конфигурации можно настроить на уровне кластера (для всех приложений, работающих в кластере) или для каждого отдельного приложения.

Другим источником информации о ресурсах, используемых исполнителями Spark, является uI-uI Spark Application.  В ui **Исполнители** отображают резюме и подробные представления конфигурации и потребляемых ресурсов.  Определите, следует ли изменять значения исполнителей для всего кластера или определенный набор выполнения задания.

![Исполнители Spark](./media/apache-spark-settings/apache-spark-executors.png)

Или вы можете использовать Ambari REST API для программной проверки параметров конфигурации кластера HDInsight и Spark.  Дополнительные сведения см. в [справочнике по API Apache Ambari на сайте GitHub](https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md).

В зависимости от рабочей нагрузки Spark может выясниться, что нестандартная конфигурация Spark оптимизирует выполнение задания.  Проверьте тест с примерными рабочими нагрузками для проверки любых конфигураций кластеров, не по умолчанию.  Можно настроить такие общие параметры:

|Параметр |Описание|
|---|---|
|--нум-исполнители|Устанавливает количество исполнителей.|
|--исполнитель-ядра|Устанавливает количество ядер для каждого исполнителя. Мы советуем выбирать исполнителей среднего размера, так как некоторые процессы используют доступную память.|
|--Исполнитель-память|Контролирует размер памяти (размер кучи) каждого исполнителя на [Apache Hadoop YARN,](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)и вам нужно оставить некоторую память для выполнения накладных расходов.|

Ниже приведен пример двух рабочих узлов с разными значениями конфигурации.

![Конфигурации двух узлов](./media/apache-spark-settings/executor-configuration.png)

Ниже приведен список ключевых параметров памяти исполнителя Spark.

|Параметр |Описание|
|---|---|
|spark.executor.memory.|Определяет общий объем памяти, доступной для исполнителя.|
|spark.storage.memoryФракция| (по умолчанию ~ 60 %) определяет объем памяти, доступный для хранения устойчивых распределенных наборов данных (RDD).|
|spark.shuffle.memoryФракция| (по умолчанию ~ 20 %) определяет объем памяти, зарезервированный для перемешивания.|
|spark.storage.unrollФракция и spark.storage.safetyФракция|(всего 30% от общей памяти) - эти значения используются внутри Spark и не должны быть изменены.|

YARN управляет максимальным объемом памяти, используемой контейнерами на всех узлах Spark. На следующей схеме показаны отношения между объектами конфигурации YARN и Spark для каждого узла.

![Управление памятью Spark в YARN](./media/apache-spark-settings/hdi-yarn-spark-memory.png)

## <a name="change-parameters-for-an-application-running-in-jupyter-notebook"></a>Изменение параметров для приложения, запущенного в записной книжке Jupyter

Кластеры Spark в HDInsight включают в себя несколько компонентов по умолчанию. Каждый из этих компонентов содержит значения конфигурации по умолчанию, которые можно переопределить при необходимости.

|Компонент |Описание|
|---|---|
|Искра Ядро|Spark Core, Spark S'L, Spark потокового AIS, GraphX, и Apache Spark MLlib.|
|Anaconda|Менеджер пакетов python.|
|[Apache Livy](https://livy.incubator.apache.org/)|API Apache Spark REST, используемый для отправки удаленных заданий в кластер HDInsight Spark.|
|[Ноутбуки Jupyter](https://jupyter.org/) и [Apache Цеппелин](https://zeppelin.apache.org/)|Интерактивный интерфейс браузера для взаимодействия с кластером Spark.|
|Драйвер ODBC|Подключает кластеры Spark в HDInsight к инструментам бизнес-аналитики (BI), таким как Microsoft Power BI и Tableau.|

Чтобы изменить конфигурацию для приложений, запущенных в записной книжке Jupyter, можно использовать команду `%%configure`. Эти изменения конфигурации будут применены к заданиям Spark, запущенным из экземпляра записной книжки. Вносить такие изменения в начале приложения, прежде чем запустить первую ячейку кода. Измененная конфигурация будет применена к сеансу Livy, когда он будет создан.

> [!NOTE]  
> Чтобы изменить конфигурацию на более позднем этапе выполнения приложения, используйте параметр `-f` (принудительное выполнение). Но при этом будут потеряны все результаты, полученные в приложении.

В следующем коде показано, как изменить конфигурацию для приложения, работающего в записной книжке Jupyter.

```
%%configure
{"executorMemory": "3072M", "executorCores": 4, "numExecutors":10}
```

## <a name="conclusion"></a>Заключение

Мониторинг параметров конфигурации ядра, чтобы обеспечить работу задания Spark предсказуемым и выполняющим. Эти параметры помогают определить лучшую конфигурацию кластера Spark для конкретных рабочих нагрузок.  Вам также необходимо следить за выполнением длительных и потребляющих ресурсов выполнения задания Spark.  Наиболее распространенные проблемы сосредоточены вокруг давления памяти из неправильной конфигурации, таких как неправильно гостевые исполнители. Кроме того, длительные операции и задачи, которые приводят к декартовые операции.

## <a name="next-steps"></a>Дальнейшие действия

* [Компоненты и версии Apache Hadoop, доступные в HDInsight](../hdinsight-component-versioning.md)
* [Управление ресурсами для кластера Apache Spark в HDInsight](apache-spark-resource-manager.md)
* [Установка кластеров в HDInsight с использованием Apache Hadoop, Apache Spark, Apache Kafka и других технологий](../hdinsight-hadoop-provision-linux-clusters.md)
* [Конфигурация Apache Spark](https://spark.apache.org/docs/latest/configuration.html)
* [Выполнение Apache Spark в Apache Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html)
