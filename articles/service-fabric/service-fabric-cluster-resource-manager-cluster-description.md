---
title: Описание кластера с помощью диспетчера кластерных ресурсов | Документация Майкрософт
description: Описание кластера Service Fabric путем указания доменов сбоя, доменов обновления, свойств узлов и емкости узлов для диспетчера кластерных ресурсов.
services: service-fabric
documentationcenter: .net
author: masnider
manager: chackdan
editor: ''
ms.assetid: 55f8ab37-9399-4c9a-9e6c-d2d859de6766
ms.service: service-fabric
ms.devlang: dotnet
ms.topic: conceptual
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 22ccb21a208bbe8e825bff9f7602bfca05990816
ms.sourcegitcommit: a52d48238d00161be5d1ed5d04132db4de43e076
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/20/2019
ms.locfileid: "67271644"
---
# <a name="describe-a-service-fabric-cluster-by-using-cluster-resource-manager"></a>Описание кластера Service Fabric с помощью диспетчера кластерных ресурсов
Компонент диспетчера кластерных ресурсов Azure Service Fabric предоставляет несколько механизмов для описания кластера:

* Домены сбоя
* Домены обновления
* Свойства узла
* Емкость узла

Во время выполнения диспетчер кластерных ресурсов использует эти сведения для обеспечения высокой доступности служб, запущенных в кластере. При применении этих важных правил он также пытается оптимизировать потребление ресурсов в кластере.

## <a name="fault-domains"></a>Домены сбоя
Домен сбоя — это любая область координированного сбоя. Одной машине — это домен сбоя. Он может завершиться ошибкой в свой собственный по различным причинам, от power сбоев питания и сбоев дисков до проблем встроенного по сетевой КАРТЫ. 

Компьютеры, подключенные к одному коммутатору Ethernet находятся в одном домене сбоя. Поэтому являются машин, совместно использующих один источник питания или в одном месте. 

Так как это сбои оборудования перекрывать естественным, домены сбоя являются по своей природе иерархическими. Они представлены в виде URI в Service Fabric.

Очень важно, что домены сбоя настроены правильно, так как Service Fabric использует эти сведения для безопасного размещения служб. Service Fabric не хочет размещать службы, службы выйти из строя, приводит к потере домена сбоя (из-за сбоя отдельного компонента). 

В среде Azure Service Fabric использует данные домена сбоя, предоставляемые средой для правильной настройки узлов в кластере от вашего имени. Для автономных экземпляров Service Fabric домены сбоя определяются во время настройки кластера. 

> [!WARNING]
> Очень важно, что сведения о домене сбоя, предоставляемые в Service Fabric будут точными. Например предположим, что узлы кластера Service Fabric работает 10 виртуальных машин, работающих на 5 физических узлах. В этом случае, даже если используется 10 виртуальных машин, имеется только 5 разных доменов сбоя (верхнего уровня). Общий доступ к одном физическом узле вызывает виртуальные машины для совместного использования одного корневого домена сбоя, так как виртуальные машины, если происходит сбой их физического узла возникнуть координированного сбоя.  
>
> Service Fabric ожидает, что домен сбоя узла не изменяется. Другие механизмы обеспечения высокого уровня доступности виртуальных машин, таких как [виртуальных машин высокой ДОСТУПНОСТИ](https://technet.microsoft.com/library/cc967323.aspx), может привести к конфликту с Service Fabric. Эти механизмы используют прозрачную миграцию виртуальных машин с одного узла на другой. Они не перенастроить или уведомляют код, выполняемый на виртуальной Машине. Таким образом, они *не поддерживается* качестве сред для Service Fabric кластеры. 
>
> Единственной применяемой технологией обеспечения высокого уровня доступности должна быть система Service Fabric. Механизмы, такие как динамический перенос виртуальной Машины и сети SAN не обязательны. Если эти механизмы, используются в сочетании с Service Fabric, они _уменьшить_ доступность и надежность приложений. Причина заключается в они повышают сложность, добавляют централизованные источники сбоев и использовать надежность и доступность стратегии, которые конфликтуют с механизмами Service Fabric. 
>
>

На следующем рисунке выделены цветом все сущности, которые складываются в доменах сбоя и список всех доменов сбоя, которые в случае. В этом примере у нас есть центры обработки данных (DC), стойки (R) и колонки (B). Если каждая колонка содержит несколько виртуальных машин, может существовать еще один уровень в иерархии домена сбоя.

<center>

![Узлы, организованные через домены сбоя][Image1]
</center>

Во время выполнения диспетчер кластерных ресурсов Service Fabric анализирует домены сбоя в кластере и планирует структуру. Реплики с отслеживанием состояния или экземпляры без отслеживания состояния службы распределяются, они будут в отдельных доменах сбоя. Распределение службы между доменами сбоя гарантирует, что доступность службы не будет нарушать при сбое домен сбоя на любом уровне иерархии.

Диспетчер кластерных ресурсов не имеет значения количество уровней в иерархии домена сбоя. Он пытается убедиться, что потеря какой-либо части иерархии не влияет на службы, работающие в ней. 

Проще всего, если такое же число узлов на каждом уровне глубины в иерархии домена сбоя. Если «дерево» доменов сбоя несбалансированным в кластере, довольно сложно для диспетчер кластерных ресурсов может выяснить, где лучше расположить службы. Структуры доменов сбоя несбалансированных означает, что некоторые домены влияет на доступность служб больше, чем других доменов. Таким образом диспетчер кластерных ресурсов разрывается между двумя целями: 

* Компания хочет использовать компьютеры в домен «heavy» размещая на них службы. 
* Ему нужно разместить службы в других доменах таким образом, чтобы отказ домена не вызывал проблем. 

Как выглядят несбалансированные домены? На следующей схеме показаны две различные структуры кластеров. В первом примере узлы равномерно распределены между доменами сбоя. Во втором примере в один домен сбоя имеет много больше узлов, чем в других доменах сбоя. 

<center>

![Две различные структуры кластеров][Image2]
</center>

В Azure выбор, какие ошибки домена содержит узел осуществляется автоматически. Но в зависимости от количества узлов, которые вы подготавливаете, вы все равно может оказаться с доменами сбоя, иметь больше узлов, чем в других случаях. 

Например, предположим есть пять доменов сбоя в кластере, но Подготовка семи узлов для типа узла (**NodeType**). В этом случае первый доменах размещено несколько узлов. Если вы по-прежнему для дополнительных развертываний **NodeType** экземпляров только несколько экземпляров, проблема усугубится. По этой причине мы рекомендуем, что число узлов в каждом типе узла, которое делится на количество доменов сбоя.

## <a name="upgrade-domains"></a>Домены обновления
Домены обновления — это еще одна функция, которая помогает кластера диспетчер ресурсов Service Fabric понять структуру кластера. Домены обновления определяют наборы узлов, которые обновляются одновременно. Домены обновления помогают диспетчеру кластерных ресурсов понимать и координировать операции управления, такие как обновление.

Домены обновления — гораздо похожи на домены сбоя, но с ряд ключевых отличий. Во-первых областями согласованных сбоев оборудования определить домены сбоев. С другой стороны, домены обновления определяются политикой. Вы сможете решить, сколько требуется, запретив среды, которые определяют число. Может иметь любое количество доменов обновления, как и узлов. Еще одно различие между доменами сбоя и доменов обновления является то, что домены обновления не являются иерархическими. Вместо этого они больше похожи на простой тег. 

На следующей схеме показано три домена обновления, чередующиеся с тремя доменами сбоя. Он также показывает одно возможное размещение трех разных реплик службы с отслеживанием состояния, где каждый заканчивается в различных неисправных и обновляемых доменах. Такое размещение отказ домена сбоя в процессе обновления службы и еще одна копия кода и данных.  

<center>

![Размещение с доменами сбоя и обновления][Image3]
</center>

Существуют, преимущества и недостатки наличие большого количества доменов обновления. Более домены обновления означают каждый этап обновления становится более детализированным и влияет на меньшем числе узлов или служб. Меньшее количество служб необходимо переместить одновременно, меньшую текучесть системы. Это повышает уровень надежности, поскольку меньше служб зависит от любой сбой во время обновления. Более домены обновления также означает, что необходимо снизить размер буфера на других узлах для обработки последствий обновления. 

Например если у вас есть пять доменов обновления, узлы в каждом из них обрабатывают примерно 20 процентов трафика. Если вам необходимо отключить этот домен обновления для обновления, эта нагрузка обычно необходимо перенаправить в другое место. Так как у вас будет четыре оставшихся доменов обновления, они должны иметь место для около 5 процентов от общего трафика. Более домены обновления означают, что требуется буфер меньшего размера на узлах в кластере. 

Попробуйте вместо этого необходимо было 10 доменов обновления. В этом случае каждый домен обновления происходит обработка только 10 процентов от общего трафика. При распространении обновления в кластере, каждого домена необходимо иметь место для лишь около 1.1 процентов от общего трафика. Более домены обновления обычно позволяют запускать узлы на более высокую степень использования, так как требуется меньшая зарезервированная емкость. То же самое верно для доменов сбоя.  

Недостатком при наличии большого количества доменов обновления что обновление выполняется дольше. Service Fabric ожидает короткое после завершения домен обновления и выполняет проверки, прежде чем начать обновление следующего. Эти задержки позволяют обнаружить проблемы, вызванные обновлением, прежде чем обновление будет продолжено. Возникающие при этом негативные последствия допустимы, так как если выбран этот вариант, то неверные изменения не влияют слишком сильно на слишком большую часть службы в определенный момент времени.

Наличие слишком малое количество доменов обновления имеет множество отрицательных побочных эффектов. Во время каждого домена обновления списка и обновления, большая часть общей емкости недоступна. Например при наличии только трех доменов обновления вы выделяете вниз около одной третьей общей службы или емкость кластера за раз. Наличие настолько значительная часть вашей службы работу за один раз нежелательно, так как должно быть достаточно объема в остальной части кластера для обработки рабочей нагрузки. Поддержка этого буфера означает, что во время обычной работы, эти узлы являются менее загружены, чем они бы в противном случае. Это увеличивает затраты на выполнение службы.

Фактических ограничений на общее количество доменов сбоя или обновления в среде либо же на то, как они перекрываются, не существует. Однако существуют распространенные шаблоны.

- Домены сбоя и обновления сопоставления 1:1
- Один домен обновления на узел (экземпляр физической или виртуальной ОС)
- «Чередованием» или «матричная» модель, в которой домены сбоя и домены обновления образуют матрицу с компьютерами, обычно выполняющимися по диагонали.

<center>

![Макеты доменов сбоя и обновления][Image4]
</center>

Нет не лучший ответ, для какой макет следует выбрать. У каждого варианта есть преимущества и недостатки. Например, модель "1 домен сбоя — 1 домен обновления" отличается простотой настройки. Модель на узел на один домен обновления — наиболее как тем, кто используются для. Во время обновления все узлы обновляются независимо друг от друга. Это аналогично тому, как раньше вручную обновлялись небольшие наборы компьютеров.

Самая распространенная модель — это матричная, где домены сбоя и домены обновления формируют таблицу и узлы располагаются по диагонали. Это модель, используемая по умолчанию в кластерах Service Fabric в Azure. Для кластеров со множеством узлов все будет как шаблон плотной матрицы.

> [!NOTE]
> Кластеры Service Fabric, размещенные в Azure не поддерживает изменение в стратегии по умолчанию. Только изолированные кластеры обеспечивают настройки.
>

## <a name="fault-and-upgrade-domain-constraints-and-resulting-behavior"></a>Ограничения доменов сбоя и обновления и соответствующее поведение
### <a name="default-approach"></a>Стандартный подход
По умолчанию диспетчер кластерных ресурсов сбалансированно распределяет службы между доменами сбоя и обновления. Это моделируется как [ограничение](service-fabric-cluster-resource-manager-management-integration.md). Ограничение для состояний доменов сбоя и обновления: «Для определенного раздела службы, существует не должно быть различие больше единицы в количестве объектов службы (экземпляры службы без отслеживания состояния или реплики службы с отслеживанием состояния) между двумя доменами одного уровня иерархии.»

Предположим, что это ограничение предоставляет гарантию «максимальной разницы». Ограничение для доменов сбоя и обновления предотвращает определенные перемещения или упорядочения, которые нарушают правила.

Например предположим, что у нас есть кластер с шестью узлами, настроены пять доменов сбоя и пять доменов обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | | |У3 | | |
| **ДО3** | | | |У4 | |
| **ДО4** | | | | |У5 |

Теперь предположим, что мы создаем службу с **TargetReplicaSetSize** (или, для службы без отслеживания состояния, **InstanceCount**) значение 5. Реплики размещаются на узлах У1-У5. Узел У6 фактически никогда не используется, вне зависимости от количества создаваемых служб. Но почему? Давайте рассмотрим разницу между текущей структурой и тем, что произошло бы, если бы мы выбрали У6.

Вот Наша текущая структура и общее число реплик на каждый домен сбоя и обновления:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** | |Р2 | | | |1 |
| **ДО2** | | |Р3 | | |1 |
| **ДО3** | | | |Р4 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Такая структура является сбалансированной в плане распределения узлов на домен сбоя и домен обновления. Он также в плане количества реплик на каждый домен сбоя и обновления. На каждый домен приходится одинаковое количество узлов и реплик.

Теперь давайте посмотрим, что произошло бы, если бы вместо У2 мы использовали У6. Как бы тогда распределялись реплики?

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |Р5 | | | | |1 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |2 |0 |1 |1 |1 |- |

Эта структура нарушает определение гарантии «максимальной разницы» ограничения для доменов сбоя. На дс0 приходится две реплики, тогда как FD1 имеет нулевой. Между 1 и на дс0 разница в общей сложности состоит из двух, которого больше, чем Максимальная разница одного. Так как ограничение нарушено, диспетчер кластерных ресурсов не разрешит использовать такую расстановку. Аналогично Если мы выбрали узлы у2 и у6 (вместо N1 и N2), то получили бы:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** | | | | | |0 |
| **ДО1** |Р5 |Р1 | | | |2 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Эта структура сбалансирована с точки зрения доменов сбоя. Но теперь она нарушает ограничение для доменов обновления, поскольку до0 ноль реплик, а у до1 их две. Эта структура также является недопустимым и она не будет выбрана диспетчером ресурсов кластера.

Такой подход к распределению реплик состояния или экземпляров без отслеживания состояния обеспечивает наилучшую отказоустойчивость из возможных. Если один домен выходит из строя, теряется минимальное количество реплик или экземпляров. 

С другой стороны, этот подход может не позволять кластеру использовать все ресурсы. Для некоторых конфигураций кластера нельзя использовать определенные узлы. Это может привести к Service Fabric, чтобы не размещать ваши службы, приводит к предупреждающие сообщения. В предыдущем примере, некоторые из узлов кластера не может быть использовать (у6 в примере). Даже если вы добавили узлы в этот кластер (у7-N10), реплики или экземпляры будут размещаться только в у1 – у5 из-за ограничений для доменов сбоя и обновления. 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | |У10 |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | |У9 |У5 |



### <a name="alternative-approach"></a>Альтернативный подход

Диспетчер кластерных ресурсов поддерживает другую версию ограничения для доменов сбоя и обновления. Она позволяет размещения, гарантируя минимальный уровень безопасности. Альтернативное ограничение можно сформулировать следующим образом: «Для определенного раздела службы, распределение реплик по доменам должно гарантировать, что секции не пострадает от потери кворума.» Предположим, что это ограничение предоставляет гарантию «сохранения кворума». 

> [!NOTE]
> Для службы с отслеживанием состояния мы определяем *потерю кворума* в ситуации, когда большинство реплик секций отключаются одновременно. Например если **TargetReplicaSetSize** равно пяти, набор любых трех реплик представляет собой кворум. Аналогично Если **TargetReplicaSetSize** равна 6, для создания кворума необходимо четыре реплики. В обоих случаях не более двух реплик может быть вниз в то же время если хочет нормального функционирования секции. 
>
> Для службы без отслеживания состояния, нет такого понятия, как *потери кворума*. Без отслеживания состояния службы по-прежнему работать нормально, даже если большинство экземпляров из строя в то же время. Таким образом мы сосредоточимся на службы с отслеживанием состояния в оставшейся части этой статьи.
>

Вернемся к предыдущему примеру. В версии «сохранения кворума» ограничения все три макета будут допустимы. Даже в том случае, если на дс0 сбой в втором макете или до1 их не удалось в третьем, секции, по-прежнему будет иметь кворум. (Большинство реплик останется.) С этой версией ограничения у6 можно было использовать практически всегда.

Подход «сохранения кворума» обеспечивает большую гибкость, чем подход «максимальной разницы». Причиной является то, что стало проще найти распределения реплик, которые являются допустимыми в почти любой топологии кластера. Однако этот подход не может гарантировать лучшие характеристики отказоустойчивости, так как некоторые сбои хуже других. 

В худшем случае большая часть реплик могут быть потеряны при сбое одного домена и одной дополнительной реплики. Например вместо трех сбоев, необходимых для потери кворума с пятью репликами или экземплярами, вы можете теперь потерять большинство со всего лишь двух сбоев. 

### <a name="adaptive-approach"></a>Адаптивный подход
Так как оба подхода имеют недостатки и преимущества, мы представляем адаптивный подход, который объединяет эти две стратегии.

> [!NOTE]
> Это поведение по умолчанию, начиная с Service Fabric версии 6.2. 
> 
> Адаптивный подход по умолчанию использует логику "максимальной разницы", а при необходимости переключается на логику "сохранения кворума". Диспетчер кластерных ресурсов автоматически определяет, какие стратегии необходим, просмотрев настройки кластера и служб.
> 
> Диспетчер кластерных ресурсов следует использовать логику «на основе кворума» для службы обоих из этих условий:
>
> * **Размер набора целевых реплик** для службы делится на количество доменов сбоя и количество доменов обновления.
> * Число узлов, которое меньше или равно числу доменов сбоя, умноженное на количество доменов обновления.
>
> Не забывайте, что диспетчер кластерных ресурсов будет использовать этот подход для служб без отслеживания состояния и с отслеживанием состояния, несмотря на то, что они не нужны для служб без отслеживания состояния потери кворума.

Вернемся обратно к предыдущему примеру и Предположим, что теперь кластер содержит 8 узлов. Кластер по-прежнему настроен с пятью доменами сбоя и пять доменов обновления и **TargetReplicaSetSize** значение службы, размещенной на этом кластере остается пять. 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | | |У5 |

Так как все необходимые условия соблюдены, диспетчер кластерных ресурсов будет использовать логику «на основе кворума» в распространении службы. Это позволяет у8 у6. В этом случае один возможных распределений службы может выглядеть следующим образом:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |Р2 | | | | |1 |
| **ДО2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | | | |0 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |2 |1 |1 |0 |1 |- |

Если службы **TargetReplicaSetSize** значение уменьшается до четырех (например), диспетчер кластерных ресурсов будет заметит это изменение. Задание будет возобновлено, используя логику «максимальной разницы», так как **TargetReplicaSetSize** больше не делится на количество доменов сбоя и доменах обновления больше. Таким образом отдельные перемещения реплики будет выполняться для распределения оставшихся четырех реплик на узлах у1 – у5. Таким образом, версия «максимальной разницы» логики домена сбоя и обновления не нарушается. 

В предыдущем макете Если **TargetReplicaSetSize** значение 5 и у1 будет удален из кластера, количество доменов обновления становится равным четырем. Опять же, диспетчер кластерных ресурсов начинается с помощью логики «максимальной разницы», так как количество доменов обновления не равномерно разделить службы **TargetReplicaSetSize** значение больше. В результате реплики R1, при построении опять же, должен попасть на у4, таким образом, чтобы не нарушить ограничение домена сбоя и обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Н/Д |Н/Д |Н/Д |Н/Д |Н/Д |Н/Д |
| **ДО1** |Р2 | | | | |1 |
| **ДО2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | |Р1 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

## <a name="configuring-fault-and-upgrade-domains"></a>Настройка доменов сбоя и обновления
В развертываниях Service Fabric, размещенного в Azure домены сбоя и домены обновления определяются автоматически. Service Fabric просто извлекает и использует сведения о среде из Azure.

Если вы создаете собственный кластер (или хотите запустить определенную топологию в среде разработки), можно указать домен сбоя, а затем обновить сведения о домене, самостоятельно. В этом примере мы определяем кластер локальной разработки девяти узлов, охватывающий тремя центрами обработки данных (каждый с тремя стойками). Этот кластер также имеет три домена обновления, чередующиеся с этими тремя центрами обработки данных. Ниже приведен пример конфигурации в ClusterManifest.xml:

```xml
  <Infrastructure>
    <!-- IsScaleMin indicates that this cluster runs on one box/one single server -->
    <WindowsServer IsScaleMin="true">
      <NodeList>
        <Node NodeName="Node01" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType01" FaultDomain="fd:/DC01/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node02" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType02" FaultDomain="fd:/DC01/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node03" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType03" FaultDomain="fd:/DC01/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node04" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType04" FaultDomain="fd:/DC02/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node05" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType05" FaultDomain="fd:/DC02/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node06" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType06" FaultDomain="fd:/DC02/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node07" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType07" FaultDomain="fd:/DC03/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node08" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType08" FaultDomain="fd:/DC03/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node09" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType09" FaultDomain="fd:/DC03/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
      </NodeList>
    </WindowsServer>
  </Infrastructure>
```

В этом примере используется ClusterConfig.json для автономных развертываний:

```json
"nodes": [
  {
    "nodeName": "vm1",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm2",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm3",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm4",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm5",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm6",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm7",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm8",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm9",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD3"
  }
],
```

> [!NOTE]
> При определении кластеров с помощью Azure Resource Manager, Azure назначает домены сбоя и доменах обновления. Поэтому задает определения типов узлов и масштабирования виртуальных машин шаблоне Azure Resource Manager не включает сведения о домене сбоя или домене обновления.
>

## <a name="node-properties-and-placement-constraints"></a>Свойства узлов и ограничения размещения
Иногда (фактически большую часть времени) необходимо убедиться, что некоторые рабочие нагрузки выполняются только для определенных типов узлов в кластере. Например некоторых рабочих нагрузок могут требоваться графические процессоры или SSDs и другим пользователям не может. 

Хорошим примером использования оборудования для конкретных рабочих нагрузок является практически в каждой n уровневая архитектура. Одни машины служат в качестве внешнего интерфейса или части обслуживания API приложения, они предоставляются клиентам или в Интернете. Другие компьютеры (часто с разными аппаратными ресурсами) обрабатывают рабочие нагрузки уровней вычислений и хранилища. Как правило, они _не_ предоставляются непосредственно клиентам или в Интернете. 

Service Fabric ожидает, что в некоторых случаях определенные рабочие нагрузки могут должны выполняться на конкретных конфигурациях оборудования. Пример:

* Приложение n уровневых был «удален и перемещен», в среде Service Fabric.
* Рабочая нагрузка должна выполняться на конкретном оборудовании, по соображениям производительности, масштабирования или безопасности изоляции.
* Рабочая нагрузка должны быть изолированы от других рабочих нагрузок по соображениям политики или ресурсов потребления.

Для поддержки таких типов конфигурации, Service Fabric включает теги, которые можно применить к узлам. Эти теги называются *свойствами узла*. *Ограничения на размещение* представляют собой операторы, привязанные к отдельным службам, выбранными для одного или нескольких свойств узла. Ограничения размещения определяют, где должны запускаться службы. Набор ограничений расширяется. Можно работать любая пара ключ/значение. 

<center>

![Разные рабочие нагрузки для макета кластера][Image5]
</center>

### <a name="built-in-node-properties"></a>Свойства встроенного узла
Service Fabric определяет некоторые свойства узла по умолчанию, которые могут использоваться автоматически, поэтому не нужно определять их. В каждом узле определены свойства по умолчанию, **NodeType** и **NodeName**. 

Например, можно написать ограничения для размещения как `"(NodeType == NodeType03)"`. **NodeType** — это свойство, часто используемых. Это полезно, так как он соответствует типу компьютера 1:1. Каждый тип компьютера соответствует типу рабочей нагрузки в традиционном n-уровневом приложении.

<center>

![Ограничения на размещение и свойства узлов][Image6]
</center>

## <a name="placement-constraints-and-node-property-syntax"></a>Ограничения на размещение и синтаксис свойства узла 
Значение, указанное в свойстве узла может быть строкой, логический, или signed long. Оператор в службе называется размещение *ограничение* так как он ограничивает, где эта служба может работать в кластере. Ограничение может быть любой логический оператор, который работает со свойствами узла в кластере. Ниже приведены допустимые селекторы в этих логических операторах:

* Условные проверки для создания определенных операторов:

  | Инструкция | Синтаксис |
  | --- |:---:|
  | "равно" | "==" |
  | "не равно" | "!=" |
  | "больше" | ">" |
  | "больше или равно" | ">=" |
  | "меньше" | "<" |
  | "меньше или равно" | "<=" |

* Логические операторы для группирования и логических операций:

  | Инструкция | Синтаксис |
  | --- |:---:|
  | "и" | "&&" |
  | "или" | "&#124;&#124;" |
  | "не" | "!" |
  | "группа как отдельный оператор" | "()" |

Вот несколько примеров основных операторов ограничения:

  * `"Value >= 5"`
  * `"NodeColor != green"`
  * `"((OneProperty < 100) || ((AnotherProperty == false) && (OneProperty >= 100)))"`

Служба может быть размещена только на тех узлах, где оператор ограничения размещения принимает общее значение True. Узлы, у которых нет определенного свойства не совпадают любой ограничениями на размещение, содержащий свойство.

Предположим, что для типа узла в ClusterManifest.xml были определены следующие свойства узла:

```xml
    <NodeType Name="NodeType01">
      <PlacementProperties>
        <Property Name="HasSSD" Value="true"/>
        <Property Name="NodeColor" Value="green"/>
        <Property Name="SomeProperty" Value="5"/>
      </PlacementProperties>
    </NodeType>
```

Пример свойства узла, определенных использование clusterconfig.JSON для автономных развертываний или Template.json, размещенный в Azure кластеров. 

> [!NOTE]
> В шаблоне Azure Resource Manager обычно параметризуется тип узла. Оно выглядело так `"[parameters('vmNodeType1Name')]"` а не NodeType01.
>

```json
"nodeTypes": [
    {
        "name": "NodeType01",
        "placementProperties": {
            "HasSSD": "true",
            "NodeColor": "green",
            "SomeProperty": "5"
        },
    }
],
```

Можно создавать размещение службы *ограничения* для службы, как показано ниже:

```csharp
FabricClient fabricClient = new FabricClient();
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
serviceDescription.PlacementConstraints = "(HasSSD == true && SomeProperty >= 4)";
// Add other required ServiceDescription fields
//...
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

```PowerShell
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceType -Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton -PlacementConstraint "HasSSD == true && SomeProperty >= 4"
```

Если все узлы NodeType01 являются допустимыми, можно также выбрать этот тип узла с ограничением `"(NodeType == NodeType01)"`.

Ограничения на размещение службы могут обновляться динамически во время выполнения. Если вам нужно, можно перемещать службу в кластере, добавлять и удалять требования и так далее. Service Fabric гарантирует, что служба остается работу и доступность, даже в том случае, если вносятся такие типы изменений.

```csharp
StatefulServiceUpdateDescription updateDescription = new StatefulServiceUpdateDescription();
updateDescription.PlacementConstraints = "NodeType == NodeType01";
await fabricClient.ServiceManager.UpdateServiceAsync(new Uri("fabric:/app/service"), updateDescription);
```

```PowerShell
Update-ServiceFabricService -Stateful -ServiceName $serviceName -PlacementConstraints "NodeType == NodeType01"
```

Ограничения размещения задаются для каждого экземпляра именованной службы. Обновления всегда заменяют (перезаписывают) свойства, заданные ранее.

Определение кластера задает свойства узла. Изменение свойств узла необходимо обновить конфигурацию кластера. Для обновления свойств узла нужно перезапустить каждый затронутый узел, чтобы он сообщил о своих новых свойствах. Service Fabric управляет эти обновления.

## <a name="describing-and-managing-cluster-resources"></a>Описания и управление кластерными ресурсами
Одна из важнейших задач любого оркестратора — помощь в управлении потреблением ресурсов в кластере. Управление кластерными ресурсами связано с несколькими аспектами. 

Во-первых, необходимо гарантировать, что компьютеры не будут перегружены. То есть нужно сделать так, чтобы на компьютерах не было запущено больше служб, чем они могут обрабатывать. 

Во-вторых есть Балансировка нагрузки и оптимизация, критически важных для эффективного выполнения служб. Предложения услуг экономичные или чувствительные к производительности не может разрешить некоторые узлы, чтобы быть "Горячий", а другие — нет. Узлов с высокой нагрузкой привести к конфликту ресурсов и снижению производительности. Узлы нецелесообразной растрате ресурсов или увеличению расходов. 

Service Fabric ресурсы представлены в виде *метрики*. Метрики — это любые логические или физические ресурсы, которые нужно описать в Service Fabric. Примерами метрик являются «Потребляется» или «МБ от этой емкости.» Сведения о физических ресурсов, определяющих Service Fabric на узлах, см. в разделе [ресурсами](service-fabric-resource-governance.md). Сведения о настройке пользовательских метрик и их использовании см. в разделе [в этой статье](service-fabric-cluster-resource-manager-metrics.md).

Метрики отличаются от ограничений на размещение и свойств узлов. Свойства узлов — это статические дескрипторы самих узлов. Метрики описывают ресурсы, которые имеют узлах и потребляемых службами при запуске на узле. Свойство узла может быть **HasSSD** и может быть установлено значение true или false. Объем места на этом Твердотельном и объем, службами, будет метрикой, например «DriveSpaceInMb.» 

Так же, как для ограничений на размещение и свойства узла, диспетчер кластерных ресурсов Service Fabric не понимает, какие имена среднее значение метрики. Имена метрик — это просто строки. Рекомендуется объявлять единицы как часть имена метрик, создаваемые, когда они могут быть неоднозначными.

## <a name="capacity"></a>Capacity
Если вы отключили все ресурсов *балансировки*, диспетчер кластерных ресурсов Service Fabric по-прежнему будет убедиться, что узел не превысит его емкость. Управление превышением емкости возможно в том случае, если кластер не перегружен и рабочая нагрузка не превышает возможности любого узла. Емкость — это другое *ограничение* что диспетчер кластерных ресурсов использует, чтобы понять, какая часть ресурса используется на узле. Оставшаяся емкость также отслеживается для кластера в целом. 

На уровне службы и емкость, и потребление выражаются в виде метрик. Например метрика может называться «ClientConnections», и узел может иметь емкость для «ClientConnections» из 32 768. Другие узлы могут иметь другие ограничения. Службы, работающей на этом узле может сообщать, что в настоящий момент она потребляет 32,256 метрики «ClientConnections.»

Во время выполнения диспетчер кластерных ресурсов отслеживает оставшуюся емкость в кластере и на узлах. Чтобы отслеживать емкость, диспетчер кластерных ресурсов вычитает использование каждой службы на основе емкости узла, где выполняется служба. На основе этой информации диспетчер кластерных ресурсов можно выяснить, где следует разместить или переместить реплики так, чтобы не была превышена емкость узлов.

<center>

![Узлы кластера и емкость][Image7]
</center>

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
ServiceLoadMetricDescription metric = new ServiceLoadMetricDescription();
metric.Name = "ClientConnections";
metric.PrimaryDefaultLoad = 1024;
metric.SecondaryDefaultLoad = 0;
metric.Weight = ServiceLoadMetricWeight.High;
serviceDescription.Metrics.Add(metric);
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

```PowerShell
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("ClientConnections,High,1024,0)
```

Вы можете увидеть емкость, определенная в манифесте кластера. Ниже приведен пример для ClusterManifest.xml.

```xml
    <NodeType Name="NodeType03">
      <Capacities>
        <Capacity Name="ClientConnections" Value="65536"/>
      </Capacities>
    </NodeType>
```

Ниже приведен пример емкость, определенную использование clusterconfig.JSON для автономных развертываний или Template.json для кластеров, размещенных в Azure: 

```json
"nodeTypes": [
    {
        "name": "NodeType03",
        "capacities": {
            "ClientConnections": "65536",
        }
    }
],
```

Нагрузка службы динамически часто изменяться. Предположим, что нагрузка реплики из «ClientConnections» изменилась с 1024 на 2048. Узел, который она выполнялась затем составляет только 512, оставшихся для этой метрики. Теперь этой реплики или экземпляра размещения является недопустимым, поскольку не хватает места на этом узле. Диспетчер кластерных ресурсов должен получать загруженность узла. Он уменьшает нагрузку на узел, емкость которого превышена, перемещая один или несколько экземпляров или реплик с этого узла на другие узлы. 

Диспетчер кластерных ресурсов пытается свести к минимуму затраты на перемещение реплики. Дополнительные сведения о [стоимость перемещения](service-fabric-cluster-resource-manager-movement-cost.md) и около [перераспределения стратегии и правила](service-fabric-cluster-resource-manager-metrics.md).

## <a name="cluster-capacity"></a>Емкость кластера
Как диспетчер кластерных ресурсов Service Fabric поддерживает хранение весь кластер не перегружается? Динамическая нагрузка не могли бы сделать много. Службы могут иметь всплески нагрузки независимо от действий, выполняемых Cluster Resource Manager. В результате кластер с большим резервом сегодня может быть маломощным, если происходит резкое завтра. 

Элементы управления в диспетчере кластерных ресурсов помогают предотвратить возникновение проблем. Первое, что можно сделать, — это предотвратить создание новых рабочих нагрузок, которые могут привести к переполнению кластера.

Предположим, вы создаете службу без отслеживания состояния, что он имеет часть нагрузки, связанные с ней. Служба заботится о метрику «DiskSpaceInMb». Служба будет использовать 5 единиц «DiskSpaceInMb» для каждого экземпляра службы. Вы хотите создать три экземпляра службы. Это означает, что требуется 15 единиц «DiskSpaceInMb» должны присутствовать в кластере можно даже создать эти экземпляры службы.

Диспетчер кластерных ресурсов постоянно вычисляет емкость и потребление для каждой метрики, чтобы можно было определить оставшуюся емкость в кластере. Если емкости недостаточно, диспетчер кластерных ресурсов отклоняет вызов создания службы.

Так как требования только что 15 единиц будут доступны, вы можете выделить это пространство различными способами. Например может существовать одна оставшаяся единица емкости на 15 различных узлах или три оставшиеся единицы емкости на 5 разных узлах. Если диспетчер кластерных ресурсов может реорганизовать элементы, поэтому доступны пять единиц на трех узлах, он размещает службу. Реорганизация кластера невозможна, если кластер почти полностью заполнен или существующие службы по какой-либо причине нельзя объединить. В остальных случаях она, как правило, возможна.

## <a name="buffered-capacity"></a>Емкость буфера
Буферизованная емкость — еще одна функция диспетчера кластерных ресурсов. Она позволяет резервировать некоторую часть общей емкости узла. Этот буфер емкости используется только для размещения служб во время обновлений и сбоев узлов. 

Буферизованная мощность указывается глобально для каждой метрики для всех узлов. Значение, выбираемое для зарезервированной емкости зависит от количества доменов сбоя и обновления, которые имеются в кластере. Дополнительные домены сбоя и обновления означает, что можно выбрать меньшее значение зарезервированной емкости. Чем больше доменов, тем меньшее количество кластеров будет недоступным во время обновлений и сбоев. Указание буферизованной емкости имеет смысл, только в том случае, если указана емкость узла для метрики.

Ниже приведен пример указания емкости буфера в ClusterManifest.xml:

```xml
        <Section Name="NodeBufferPercentage">
            <Parameter Name="SomeMetric" Value="0.15" />
            <Parameter Name="SomeOtherMetric" Value="0.20" />
        </Section>
```

Ниже приведен пример указания емкости буфера использование clusterconfig.JSON для автономных развертываний или Template.json для кластеров, размещенных в Azure:

```json
"fabricSettings": [
  {
    "name": "NodeBufferPercentage",
    "parameters": [
      {
          "name": "SomeMetric",
          "value": "0.15"
      },
      {
          "name": "SomeOtherMetric",
          "value": "0.20"
      }
    ]
  }
]
```

При недостаточной емкости буфера для метрики создание служб завершается сбоем. Предотвращение создания новых служб для сохранения буфера гарантирует, что обновления и сбои не приведут к превышению емкости узлов. Емкость буфера является необязательным, но мы рекомендуем во всех кластерах, определяющих емкость для метрики.

Диспетчер кластерных ресурсов предоставляет эти сведения о нагрузке. Для каждой метрики эти сведения содержат: 
- Параметры емкости буфера.
- Общая емкость.
- Текущее потребление.
- Является ли каждая метрика сбалансированной или нет.
- Статистика стандартное отклонение.
- Узлы, которые иметь под рукой и минимальных нагрузки.  
  
Ниже показан пример таких выходных данных:

```PowerShell
PS C:\Users\user> Get-ServiceFabricClusterLoadInformation
LastBalancingStartTimeUtc : 9/1/2016 12:54:59 AM
LastBalancingEndTimeUtc   : 9/1/2016 12:54:59 AM
LoadMetricInformation     :
                            LoadMetricName        : Metric1
                            IsBalancedBefore      : False
                            IsBalancedAfter       : False
                            DeviationBefore       : 0.192450089729875
                            DeviationAfter        : 0.192450089729875
                            BalancingThreshold    : 1
                            Action                : NoActionNeeded
                            ActivityThreshold     : 0
                            ClusterCapacity       : 189
                            ClusterLoad           : 45
                            ClusterRemainingCapacity : 144
                            NodeBufferPercentage  : 10
                            ClusterBufferedCapacity : 170
                            ClusterRemainingBufferedCapacity : 125
                            ClusterCapacityViolation : False
                            MinNodeLoadValue      : 0
                            MinNodeLoadNodeId     : 3ea71e8e01f4b0999b121abcbf27d74d
                            MaxNodeLoadValue      : 15
                            MaxNodeLoadNodeId     : 2cc648b6770be1bc9824fa995d5b68b1
```

## <a name="next-steps"></a>Дальнейшие действия
* Сведения об архитектуре и потоке информации в диспетчер кластерных ресурсов см. в разделе [Общие сведения об архитектуре диспетчера кластерных ресурсов](service-fabric-cluster-resource-manager-architecture.md).
* Определение метрик дефрагментации — один из способов объединения нагрузки на узлах вместо ее рассредоточения. Дополнительные сведения о настройке дефрагментации, см. в разделе [дефрагментация метрик и нагрузкой в Service Fabric](service-fabric-cluster-resource-manager-defragmentation-metrics.md).
* Начать с самого начала и [ознакомьтесь с введением в кластере диспетчер ресурсов Service Fabric](service-fabric-cluster-resource-manager-introduction.md).
* Чтобы узнать, как диспетчер кластерных ресурсов управляет и балансирует нагрузку в кластере, см. в разделе [Балансировка кластера Service Fabric](service-fabric-cluster-resource-manager-balancing.md).

[Image1]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-domains.png
[Image2]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-uneven-fault-domain-layout.png
[Image3]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domains-with-placement.png
[Image4]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domain-layout-strategies.png
[Image5]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-layout-different-workloads.png
[Image6]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-placement-constraints-node-properties.png
[Image7]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-nodes-and-capacity.png
