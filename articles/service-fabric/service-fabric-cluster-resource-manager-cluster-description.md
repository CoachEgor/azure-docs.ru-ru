---
title: Опишите кластер с помощью менеджера кластерных ресурсов
description: Опишите кластер Service Fabric, указав домены неисправностей, обновления доменов, свойств узлов и возможностей узлов для менеджера кластерных ресурсов.
author: masnider
ms.topic: conceptual
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 7142e3f9aaa25e7ba327194c04ad6a9b5f4e3ad1
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "79258776"
---
# <a name="describe-a-service-fabric-cluster-by-using-cluster-resource-manager"></a>Опишите кластер service Fabric с помощью менеджера кластерных ресурсов
Функция менеджера кластерных ресурсов Azure Service Fabric предоставляет несколько механизмов описания кластера:

* Домены сбоя
* Домены обновления
* Свойства узла
* Емкость узла

Во время выполнения менеджер кластерных ресурсов использует эту информацию для обеспечения высокой доступности служб, работающих в кластере. При соблюдении этих важных правил он также пытается оптимизировать потребление ресурсов в кластере.

## <a name="fault-domains"></a>Домены сбоя
Домен сбоя — это любая область координированного сбоя. Одна машина является доменом неисправности. Он может выйти из строя сам по себе по различным причинам, от сбоев питания до сбоев в работе с плохой прошивки NIC. 

Машины, подключенные к тому же коммутатору Ethernet, находятся в одном домене неисправности. Так же, как машины, которые разделяют один источник энергии или в одном месте. 

Поскольку сбои в аппаратных ошибках перекрываются, домены неисправностей по своей сути иерархичны. Они представлены как URIs в Сервисной Ткани.

Важно, чтобы домены неисправностей были настроены правильно, потому что Service Fabric использует эту информацию для безопасного размещения служб. Service Fabric не хочет размещать службы таким образом, что потеря домена неисправности (вызванного сбоем некоторых компонентов) приводит к отключению службы. 

В среде Azure ткань службы использует информацию о домене неисправностей, предоставляемую средой, для правильной настройки узлов кластера от вашего имени. Для автономных экземпляров Service Fabric домены неисправностей определяются во время настройки кластера. 

> [!WARNING]
> Важно, чтобы информация о домене неисправности, предоставляемая Service Fabric, была точной. Например, предположим, что узлы кластера Service Fabric работают внутри 10 виртуальных машин, работающих на 5 физических узлах. В этом случае, даже если используется 10 виртуальных машин, имеется только 5 разных доменов сбоя (верхнего уровня). Совместное использование одного и того же физического узла приводит к тому, что VMs разделяют один и тот же домен корневой неисправности, поскольку в случае сбоя физического узла сбои в VMs выполняется скоординированный сбой.  
>
> Service Fabric ожидает, что разлом домена узла не изменится. Другие механизмы обеспечения высокой доступности ВМ, такие как [HA-VMs,](https://technet.microsoft.com/library/cc967323.aspx)могут вызвать конфликты с Service Fabric. Эти механизмы используют прозрачную миграцию вс-м вс от одного узла к другому. Они не перенастраивают и не уведомляют работающий код внутри VM. Таким образом, они *не поддерживаются* в качестве сред для запуска кластеров Service Fabric. 
>
> Единственной применяемой технологией обеспечения высокого уровня доступности должна быть система Service Fabric. Такие механизмы, как живая миграция VM и SAN, не являются необходимыми. Если эти механизмы используются совместно с Service Fabric, они _снижают_ доступность и надежность приложений. Причина в том, что они вводят дополнительную сложность, добавляют централизованные источники сбоев и используют стратегии надежности и доступности, которые противоречат стратегиям Service Fabric. 
>
>

В следующем графическом, мы цвет всех объектов, которые способствуют неисправности доменов и перечислить все различные домены неисправности, которые в результате. В этом примере у нас есть центры обработки данных (DC), стойки (R) и колонки (B). Если каждое лезвие содержит более одной виртуальной машины, в иерархии доменов неисправности может быть еще один слой.

<center>

![Узлы, организованные через домены неисправностей][Image1]
</center>

Во время выполнения менеджер кластерных ресурсов Service Fabric рассматривает домены неисправностей в кластере и планирует макеты. Реплики состояния или экземпляры без состояния для службы распространяются, так что они находятся в отдельных доменах неисправностей. Распространение службы по разбоным доменотам гарантирует, что доступность службы не будет нарушена, если домен неисправности выходит из строя на любом уровне иерархии.

Менеджеру кластерных ресурсов все равно, сколько слоев находится в иерархии доменов разлома. Он пытается убедиться, что потеря какой-либо части иерархии не влияет на службы, работающие в ней. 

Лучше всего, если одинаковое количество узлов находится на каждом уровне глубины в иерархии доменов разлома. Если «дерево» доменов разлома несбалансировано в кластере, менеджеру кластерных ресурсов сложнее выяснить наилучшее распределение служб. Несбалансированные макеты доменов сбоя означают, что потеря некоторых доменов влияет на доступность служб больше, чем другие домены. В результате менеджер кластерных ресурсов разрывается между двумя целями: 

* Он хочет использовать машины в этом "тяжелом" домене, размещая на них сервисы. 
* Он хочет размещать службы в других областях, чтобы потеря домена не вызывала проблем. 

Как выглядят несбалансированные домены? На следующей диаграмме показаны два различных кластерных макета. В первом примере узлы равномерно распределены по доменам неисправностей. Во втором примере один домена неисправности имеет гораздо больше узлов, чем другие домены неисправности. 

<center>

![Две различные структуры кластеров][Image2]
</center>

В Azure выбор домена неисправности содержит узел, управляемый для вас. Но в зависимости от количества узлов, которые вы предоставляете, вы все равно можете в конечном итоге с доменами неисправности, которые имеют больше узлов в них, чем в других. 

Например, скажем, у вас есть пять доменов неисправностей в кластере, но предусмотрено семь узлов для типа узла **(NodeType**). В этом случае первые два домена неисправности в конечном итоге с большим количеством узлов. Если вы продолжите развертывать больше экземпляров **NodeType** с несколькими экземплярами, проблема усугубится. По этой причине мы рекомендуем, чтобы количество узлов в каждом типе узлов было кратное от числа доменов неисправностей.

## <a name="upgrade-domains"></a>Домены обновления
Области обновления являются еще одной функцией, которая помогает менеджеру кластерных ресурсов service Fabric понять расположение кластера. Области обновления определяют наборы узлов, которые обновляются одновременно. Обновления доменов помогают менеджеру кластерных ресурсов понять и организовать управление такими операциями, как обновления.

Обновление доменов много, как вина доменов, но с парой ключевых различий. Во-первых, области скоординированных аппаратных сбоев определяют домены неисправностей. Обновления доменов, с другой стороны, определяются политикой. Вы можете решить, сколько вы хотите, вместо того, чтобы окружающая среда диктовать номер. Вы можете иметь столько доменов обновления, как вы делаете узлы. Еще одно различие между доменами неисправностей и обновления доменов заключается в том, что домены обновления не являются иерархическими. Вместо этого они больше похожи на простой тег. 

На следующей диаграмме показаны три домена обновления, разлитые по трем доменам неисправностей. Он также показывает одно возможное размещение для трех различных реплик службы состояния, где каждый из них попадает в различные ошибки и обновления доменов. Это размещение позволяет утерять домена неисправности в середине обновления службы и по-прежнему иметь одну копию кода и данных.  

<center>

![Размещение с ошибками и обновление доменов][Image3]
</center>

Есть плюсы и минусы, имеющие большое количество доменов обновления. Больше доменов обновления означает, что каждый шаг обновления является более детальным и влияет на меньшее количество узлов или служб. Меньше ежей служб приходится перемещать одновременно, вводя меньше оттока в систему. Это приводит к повышению надежности, поскольку на меньшую службу влияет любая проблема, введенная во время обновления. Больше доменов обновления также означает, что вам нужно меньше доступных буферов на других узлах для обработки воздействия обновления. 

Например, если у вас есть пять доменов обновления, узлы в каждом из них обработки примерно 20 процентов трафика. Если вам нужно снять этот домен обновления для обновления, эта нагрузка обычно должна куда-то идти. Поскольку у вас есть четыре оставшихся домена обновления, каждый должен иметь место для около 5 процентов от общего трафика. Больше доменов обновления означает, что вам нужно меньше буфера на узлах в кластере. 

Рассмотрим, если у вас было 10 доменов обновления вместо. В этом случае каждый домен обновления будет обрабатывать только около 10 процентов от общего трафика. При прохождении обновления через кластер каждый домен должен иметь место только для 1,1 процента от общего трафика. Более обновления доменов обычно позволяют запускать узлы при более высоком использовании, потому что вам нужно меньше зарезервированной емкости. То же самое относится и к доменам неисправностей.  

Недостатком наличия многих доменов обновления является то, что обновления, как правило, занимают больше времени. Service Fabric ждет короткий период после завершения домен обновления и выполняет проверки перед началом обновления следующего. Эти задержки позволяют обнаружить проблемы, вызванные обновлением, прежде чем обновление будет продолжено. Возникающие при этом негативные последствия допустимы, так как если выбран этот вариант, то неверные изменения не влияют слишком сильно на слишком большую часть службы в определенный момент времени.

Наличие слишком мало доменов обновления имеет много негативных побочных эффектов. В то время как каждый домен обновления не работает и обновляется, большая часть общей емкости недоступна. Например, если у вас есть только три домена обновления, вы занимаете примерно одну треть общей емкости обслуживания или кластера одновременно. Нетактление столь значительной части службы сразу нежелательно, потому что вам требуется достаточная емкость в остальной части кластера для обработки рабочей нагрузки. Поддержание этого буфера означает, что во время обычной работы эти узлы менее загружены, чем в противном случае. Это увеличивает затраты на выполнение службы.

Фактических ограничений на общее количество доменов сбоя или обновления в среде либо же на то, как они перекрываются, не существует. Но есть общие закономерности:

- Недостатки доменов и обновления доменов, отображенных 1:1
- Один домен обновления на узла (физический или виртуальный экземпляр ОС)
- "Полосатая" или "матрица" модель, где домены разлома и обновления доменов образуют матрицу с машинами, обычно бегущими по диагонали

<center>

![Планы неисправностей и обновления доменов][Image4]
</center>

Там нет лучшего ответа, для которого макет выбрать. У каждого варианта есть преимущества и недостатки. Например, модель "1 домен сбоя — 1 домен обновления" отличается простотой настройки. Модель одного домена обновления на модель узлов больше всего похожа на то, к чему люди привыкли. Во время обновления каждый узла обновляется независимо. Это аналогично тому, как раньше вручную обновлялись небольшие наборы компьютеров.

Наиболее распространенной моделью является матрица FD/UD, где домены разлома и домены обновления образуют таблицу, а узлы размещаются по диагонали. Это модель, используемая по умолчанию в кластерах Service Fabric в Azure. Для кластеров с большим количеством узлов все выглядит как плотный матричной узор.

> [!NOTE]
> Кластеры service Fabric, размещенные в Azure, не поддерживают изменение стратегии по умолчанию. Только автономные кластеры предлагают такую настройку.
>

## <a name="fault-and-upgrade-domain-constraints-and-resulting-behavior"></a>Ограничения доменов сбоя и обновления и соответствующее поведение
### <a name="default-approach"></a>Стандартный подход
По умолчанию менеджер кластерных ресурсов обеспечивает сбалансированность служб в доменах неисправностей и обновлений. Это моделируется как [ограничение](service-fabric-cluster-resource-manager-management-integration.md). Ограничение для сбоя и обновления доменов гласит: "Для данной раздела службы никогда не должно быть разницы, превышающей разницу, превышающее 1 в количестве объектов обслуживания (инстанции без состояния службы или реплики службы состояния) между любыми двумя доменами на одном и том же уровня иерархии».

Допустим, это ограничение обеспечивает гарантию «максимальной разницы». Ограничение для доменов сбовичества и обновления предотвращает определенные ходы или механизмы, нарушающие правило.

Например, предположим, что у нас есть кластер с шестью узлами, настроенный с пятью доменами неисправностей и пятью доменами обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **Ud0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | | |У3 | | |
| **ДО3** | | | |У4 | |
| **ДО4** | | | | |У5 |

Теперь предположим, что мы создаем сервис с **помощью TargetReplicaSetSize** (или, для службы без состояния, **InstanceCount)** значение пять. Реплики размещаются на узлах У1-У5. Узел У6 фактически никогда не используется, вне зависимости от количества создаваемых служб. Но почему? Давайте рассмотрим разницу между текущей структурой и тем, что произошло бы, если бы мы выбрали У6.

Вот макет мы получили и общее количество реплик на неисправность и обновления домена:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **Ud0** |Р1 | | | | |1 |
| **ДО1** | |R2 | | | |1 |
| **ДО2** | | |Р3 | | |1 |
| **ДО3** | | | |Р4 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Этот макет сбалансирован с точки зрения узлов на домене неисправности и домена обновления. Он также сбалансирован с точки зрения количества реплик на дефект и обновление домена. На каждый домен приходится одинаковое количество узлов и реплик.

Теперь давайте посмотрим, что произошло бы, если бы вместо У2 мы использовали У6. Как бы тогда распределялись реплики?

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **Ud0** |Р1 | | | | |1 |
| **ДО1** |Р5 | | | | |1 |
| **ДО2** | | |R2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |2 |0 |1 |1 |1 |- |

Этот макет нарушает наше определение гарантии "максимальная разница" для ограничения домена разлома. FD0 имеет две реплики, в то время как FD1 имеет ноль. Разница между FD0 и FD1 в общей сложности два, что больше, чем максимальная разница в одном. Поскольку ограничение нарушено, менеджер кластерных ресурсов не допускает такого механизма. Аналогичным образом, если бы мы выбрали N2 и N6 (вместо N1 и N2), мы бы получили:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **Ud0** | | | | | |0 |
| **ДО1** |Р5 |Р1 | | | |2 |
| **ДО2** | | |R2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Этот макет сбалансирован с точки зрения доменов неисправностей. Но теперь он нарушает ограничение домена обновления, потому что UD0 имеет нулевые реплики и UD1 имеет два. Эта компоновка также недействительна и не будет выбрана менеджером кластерных ресурсов.

Такой подход к распределению реплик состояния или экземпляров без отслеживания состояния обеспечивает наилучшую отказоустойчивость из возможных. При падении одного домена теряется минимальное количество реплик/экземпляров. 

С другой стороны, этот подход может не позволять кластеру использовать все ресурсы. Для некоторых конфигураций кластера нельзя использовать определенные узлы. Это может привести к тому, что Service Fabric не размещает свои службы, что приведет к повлекли за себя предупреждающие сообщения. В предыдущем примере некоторые кластерные узлы не могут быть использованы (N6 в примере). Даже если вы добавили узлы в этот кластер (N7-N10), реплики/экземпляры будут размещаться только на N1-N5 из-за ограничений на неисправность и обновление доменов. 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **Ud0** |У1 | | | |У10 |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | |У9 |У5 |



### <a name="alternative-approach"></a>Альтернативный подход

Менеджер кластерных ресурсов поддерживает другую версию ограничения для доменов сбоя и обновления. Это позволяет размещение в то же время гарантируя минимальный уровень безопасности. Альтернативное ограничение может быть указано следующим образом: "Для данного раздела службы распределение реплик и по размножению должно гарантировать, что раздел не понесет потери кворума". Допустим, это ограничение обеспечивает гарантию «безопасного кворума». 

> [!NOTE]
> Для службы с отслеживанием состояния мы определяем *потерю кворума* в ситуации, когда большинство реплик секций отключаются одновременно. Например, если **TargetReplicaSetSize** растаи 5, то набор из трех реплик представляет собой кворум. Аналогичным образом, если **TargetReplicaSetSize** шесть, четыре реплики необходимы для кворума. В обоих случаях не более двух реплик могут быть вниз в то же время, если раздел хочет продолжать нормально функционировать. 
>
> Для службы без гражданства, нет такого понятия, как *потеря кворума*. Службы без гражданства продолжают функционировать в обычном режиме, даже если в большинстве случаев одновременно не работает. Таким образом, мы сосредоточимся на государственных услуг в остальной части этой статьи.
>

Вернемся к предыдущему примеру. С "кворум безопасной" версии ограничения, все три макета будут действительными. Даже если FD0 не удалось во втором макете или UD1 не удалось в третьем макете, раздел будет по-прежнему кворум. (Большинство реплик будет по-прежнему вверх.) С этой версией ограничения, N6 почти всегда может быть использован.

Подход "безопасный кворум" обеспечивает большую гибкость, чем подход "максимальная разница". Причина в том, что легче найти распределения реплик, которые действительны практически в любой топологии кластеров. Однако этот подход не может гарантировать лучшие характеристики отказоустойчивости, так как некоторые сбои хуже других. 

В худшем случае большинство реплик может быть потеряно при сбое одного домена и одной дополнительной реплики. Например, вместо трех сбоев, необходимых для того, чтобы потерять кворум с пятью репликами или экземплярами, теперь можно потерять большинство с двумя сбоями. 

### <a name="adaptive-approach"></a>Адаптивный подход
Поскольку оба подхода имеют сильные и слабые стороны, мы внедрили адаптивный подход, который сочетает в себе эти две стратегии.

> [!NOTE]
> Это поведение по умолчанию, начиная с версии Service Fabric 6.2. 
> 
> Адаптивный подход по умолчанию использует логику "максимальной разницы", а при необходимости переключается на логику "сохранения кворума". Менеджер кластерных ресурсов автоматически определяет, какая стратегия необходима, глядя на то, как настроен кластер и службы.
> 
> Менеджер кластерных ресурсов должен использовать логику «основанной на кворуме» для службы, оба этих условия верны:
>
> * **TargetReplicaSetSize** для службы равномерно делится на количество доменов неисправностей и количество доменов обновления.
> * Количество узлов меньше или равно числу доменов неисправностей, умноженных на количество доменов обновления.
>
> Имейте в виду, что менеджер кластерных ресурсов будет использовать этот подход как для служб без состояния, так и для служб состояния, даже если потеря кворума не актуальна для служб без гражданства.

Вернемся к предыдущему примеру и предположим, что кластер теперь имеет восемь узлов. Кластер по-прежнему настроен с пятью доменами неисправностей и пятью доменами обновления, а значение **службы TargetReplicaSetSize** службы, размещенной в этом кластере, остается пятью. 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **Ud0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | | |У5 |

Поскольку все необходимые условия выполнены, менеджер кластерных ресурсов будет использовать логику «на основе кворума» при распространении службы. Это позволяет использовать N6-N8. Один из возможных дистрибутивов услуг в этом случае может выглядеть следующим образом:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **Ud0** |Р1 | | | | |1 |
| **ДО1** |R2 | | | | |1 |
| **ДО2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | | | |0 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |2 |1 |1 |0 |1 |- |

Если значение вашего сервиса **TargetReplicaSetSize** уменьшено до четырех (например), менеджер кластерных ресурсов заметит это изменение. Он возобновит использование логики "максимальной разницы", поскольку **TargetReplicaSetSize** больше не делится на количество доменов неисправностей и обновления доменов. В результате произойдут определенные движения реплик для распространения оставшихся четырех реплик на узлах N1-N5. Таким образом, не нарушается версия "максимальной разницы" домена сбоя и логики домена обновления. 

В предыдущем макете, если значение **TargetReplicaSetSize** составляет пять, а N1 удаляется из кластера, число доменов обновления становится равным четырем. Опять же, менеджер кластерных ресурсов начинает использовать логику «максимальной разницы», потому что количество доменов обновления больше не делит значение **TargetReplicaSetSize** службы. В результате реплика R1, когда построена снова, должна приземлиться на N4, чтобы ограничение на неисправность и обновление домена не нарушалось.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **Ud0** |Недоступно |Недоступно |Недоступно |Недоступно |Недоступно |Недоступно |
| **ДО1** |R2 | | | | |1 |
| **ДО2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | |Р1 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

## <a name="configuring-fault-and-upgrade-domains"></a>Настройка доменов сбоя и обновления
В развертывании системы обслуживания, размещенной в Azure, домены неисправностей и домены обновления определяются автоматически. Service Fabric просто извлекает и использует сведения о среде из Azure.

Если вы создаете свой собственный кластер (или хотите запустить определенную топологию в разработке), вы можете предоставить домену неисправности и обновить информацию о домене самостоятельно. В этом примере мы определяем кластер локальной разработки из девяти узлов, охватывающий три центра обработки данных (каждый из которых имеет три стойки). Этот кластер также имеет три домена обновления, полосатые через эти три центра обработки данных. Вот пример конфигурации в ClusterManifest.xml:

```xml
  <Infrastructure>
    <!-- IsScaleMin indicates that this cluster runs on one box/one single server -->
    <WindowsServer IsScaleMin="true">
      <NodeList>
        <Node NodeName="Node01" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType01" FaultDomain="fd:/DC01/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node02" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType02" FaultDomain="fd:/DC01/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node03" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType03" FaultDomain="fd:/DC01/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node04" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType04" FaultDomain="fd:/DC02/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node05" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType05" FaultDomain="fd:/DC02/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node06" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType06" FaultDomain="fd:/DC02/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node07" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType07" FaultDomain="fd:/DC03/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node08" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType08" FaultDomain="fd:/DC03/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node09" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType09" FaultDomain="fd:/DC03/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
      </NodeList>
    </WindowsServer>
  </Infrastructure>
```

Этот пример использует ClusterConfig.json для автономных развертываний:

```json
"nodes": [
  {
    "nodeName": "vm1",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm2",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm3",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm4",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm5",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm6",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm7",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm8",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm9",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD3"
  }
],
```

> [!NOTE]
> При определении кластеров с помощью менеджера ресурсов Azure Azure Azure назначает домены сбоев и обновляет домены. Таким образом, определение типов узлов и виртуальных наборов масштабов машин в шаблоне Azure Resource Manager не содержит информации о домене неисправности или домене обновления.
>

## <a name="node-properties-and-placement-constraints"></a>Свойства узлов и ограничения размещения
Иногда (на самом деле, в большинстве случаев) требуется, чтобы определенные рабочие нагрузки запускались только на определенных типах узлов в кластере. Например, некоторые рабочие нагрузки могут потребовать графических процессоров или SSD, а другие нет. 

Отличным примером таргетинга оборудования на определенные рабочие нагрузки является практически каждая архитектура n-уровня. Некоторые машины служат в качестве передней части или API-обслуживающей стороны приложения и подвергаются воздействию клиентов или Интернета. Другие компьютеры (часто с разными аппаратными ресурсами) обрабатывают рабочие нагрузки уровней вычислений и хранилища. Как правило, они _не_ предоставляются непосредственно клиентам или в Интернете. 

Service Fabric предполагает, что в некоторых случаях определенные рабочие нагрузки могут потребоваться для выполнения определенных конфигураций оборудования. Пример:

* Существующее приложение n-tier было «поднято и перенесено» в среду Service Fabric.
* Рабочая нагрузка должна быть запущена на определенном оборудовании для причин производительность, масштаб или изоляции безопасности.
* Рабочая нагрузка должна быть изолирована от других рабочих нагрузок по причинам политики или потребления ресурсов.

Для поддержки такого рода конфигураций Service Fabric включает теги, которые можно применить к узлам. Эти теги называются *свойствами узла*. *Ограничения размещения* — это операторы, прилагаемые к отдельным службам, которые вы выбираете для одного или нескольких свойств узлов. Ограничения размещения определяют, где должны запускаться службы. Набор ограничений является расширяемым. Любая пара ключей/значений может работать. 

<center>

![Различные рабочие нагрузки для макета кластера][Image5]
</center>

### <a name="built-in-node-properties"></a>Встроенные свойства узлов
Service Fabric определяет некоторые свойства узлов по умолчанию, которые могут быть использованы автоматически, так что вам не придется их определять. Свойства по умолчанию, определенные на каждом узлах, являются **NodeType** и **NodeName.** 

Например, можно написать ограничение `"(NodeType == NodeType03)"`размещения как. **NodeType** является широко используемым свойством. Это полезно, потому что он соответствует 1:1 с типом машины. Каждый тип компьютера соответствует типу рабочей нагрузки в традиционном n-уровневом приложении.

<center>

![Ограничения размещения и свойства узлов][Image6]
</center>

## <a name="placement-constraints-and-node-property-syntax"></a>Ограничения размещения и синтаксис собственности узлов 
Значение, указанное в свойстве узла, может быть строкой, Boolean или подписанной долго. Заявление в службе называется *ограничением* размещения, поскольку оно ограничивает работу службы в кластере. Ограничением может быть любое заявление Boolean, которое работает на свойствах узла в кластере. Действительные селекторы в этих заявлениях Boolean являются:

* Условные проверки для создания конкретных инструкций:

  | . | Синтаксис |
  | --- |:---:|
  | "равно" | "==" |
  | "не равно" | "!=" |
  | "больше" | ">" |
  | "больше или равно" | ">=" |
  | "меньше" | "<" |
  | "меньше или равно" | "<=" |

* Boolean заявления для группировки и логических операций:

  | . | Синтаксис |
  | --- |:---:|
  | "и" | "&&" |
  | "или" | "&#124;&#124;" |
  | "не" | "!" |
  | "группа как отдельный оператор" | "()" |

Вот несколько примеров основных заявлений о ограничениях:

  * `"Value >= 5"`
  * `"NodeColor != green"`
  * `"((OneProperty < 100) || ((AnotherProperty == false) && (OneProperty >= 100)))"`

Служба может быть размещена только на тех узлах, где оператор ограничения размещения принимает общее значение True. Узлы, на которых не определено свойство, не соответствуют никаким ограничениям размещения, содержащим свойство.

Допустим, следующие свойства узлов были определены для типа узла в ClusterManifest.xml:

```xml
    <NodeType Name="NodeType01">
      <PlacementProperties>
        <Property Name="HasSSD" Value="true"/>
        <Property Name="NodeColor" Value="green"/>
        <Property Name="SomeProperty" Value="5"/>
      </PlacementProperties>
    </NodeType>
```

В следующем примере показаны свойства узлов, определяемые с помощью ClusterConfig.json для автономных развертываний или Template.json для кластеров, размещенных в Azure. 

> [!NOTE]
> В шаблоне управления ресурсами Azure тип узла обычно параметрыруется. Это будет `"[parameters('vmNodeType1Name')]"` выглядеть, а не NodeType01.
>

```json
"nodeTypes": [
    {
        "name": "NodeType01",
        "placementProperties": {
            "HasSSD": "true",
            "NodeColor": "green",
            "SomeProperty": "5"
        },
    }
],
```

Можно создать *ограничения для* размещения службы следующим образом:

```csharp
FabricClient fabricClient = new FabricClient();
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
serviceDescription.PlacementConstraints = "(HasSSD == true && SomeProperty >= 4)";
// Add other required ServiceDescription fields
//...
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

```PowerShell
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceType -Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton -PlacementConstraint "HasSSD == true && SomeProperty >= 4"
```

Если все узлы NodeType01 действительны, можно также выбрать тип этого `"(NodeType == NodeType01)"`узла с ограничением.

Ограничения размещения службы могут быть динамически обновлены во время выполнения. При необходимости можно переместить службу в кластере, добавить и удалить требования и так далее. Service Fabric гарантирует, что служба остается и доступна даже при внесении таких изменений.

```csharp
StatefulServiceUpdateDescription updateDescription = new StatefulServiceUpdateDescription();
updateDescription.PlacementConstraints = "NodeType == NodeType01";
await fabricClient.ServiceManager.UpdateServiceAsync(new Uri("fabric:/app/service"), updateDescription);
```

```PowerShell
Update-ServiceFabricService -Stateful -ServiceName $serviceName -PlacementConstraints "NodeType == NodeType01"
```

Ограничения размещения указаны для каждого указанного экземпляра службы. Обновления всегда заменяют (перезаписывают) свойства, заданные ранее.

Определение кластера задает свойства узла. Изменение свойств узла требует обновления конфигурации кластера. Для обновления свойств узла нужно перезапустить каждый затронутый узел, чтобы он сообщил о своих новых свойствах. Service Fabric управляет этими обновлениями прокатки.

## <a name="describing-and-managing-cluster-resources"></a>Описание и управление кластерными ресурсами
Одна из важнейших задач любого оркестратора — помощь в управлении потреблением ресурсов в кластере. Управление кластерными ресурсами связано с несколькими аспектами. 

Во-первых, необходимо гарантировать, что компьютеры не будут перегружены. То есть нужно сделать так, чтобы на компьютерах не было запущено больше служб, чем они могут обрабатывать. 

Во-вторых, есть балансировка и оптимизация, которые имеют решающее значение для эффективного выполнения служб. Экономически эффективные или чувствительные к производительности предложения услуг не могут позволить некоторым узлам быть горячими, в то время как другие холодные. Горячие узлы приводят к раздору ресурсов и низкой производительности. Холодные узлы представляют собой потраченные впустую ресурсы и увеличение затрат. 

Service Fabric представляет ресурсы как *метрики.* Метрики — это любые логические или физические ресурсы, которые нужно описать в Service Fabric. Примерами метрик являются "РаботаУтУвер" или "MemoryInMb". Для получения информации о физических ресурсах, [Resource governance](service-fabric-resource-governance.md)которые Service Fabric может управлять на узлах, см. Для получения информации о метриках по умолчанию, используемых менеджером кластерных ресурсов, и о том, как настроить пользовательские метрики, смотрите [эту статью](service-fabric-cluster-resource-manager-metrics.md).

Метрики отличаются от ограничений размещения и свойств узлов. Свойства узлов — это статические дескрипторы самих узлов. Метрики описывают ресурсы, которые имеют узлы и которые службы потребляют при запуске на узлах. Свойство узла может быть **HasSSD** и может быть установлено на истинное или ложное. Количество пространства, доступного на этом SSD и сколько потребляется служб будет метрики, как "DriveSpaceInMb". 

Как и для ограничений по размещению и свойств узлов, менеджер кластерных ресурсов Service Fabric не понимает, что означают названия метрик. Имена метрик — это просто строки. Это хорошая практика, чтобы объявить единицы как часть метрических имен, которые вы создаете, когда они могут быть неоднозначными.

## <a name="capacity"></a>Capacity
Если вы отключите *балансировку*всех ресурсов, менеджер кластерных ресурсов Service Fabric по-прежнему гарантирует, что ни один узла не перейдет на его емкость. Управление превышением емкости возможно в том случае, если кластер не перегружен и рабочая нагрузка не превышает возможности любого узла. Емкость — это еще одно *ограничение,* которое использует менеджер кластерных ресурсов для понимания того, сколько ресурса узла имеется. Оставшаяся емкость также отслеживается для кластера в целом. 

На уровне службы и емкость, и потребление выражаются в виде метрик. Например, метрика может быть "Клиентские соединения", а узла может быть емкость "ClientConnections" 32 768. Другие узлы могут иметь другие пределы. Служба, работающая на этом уде, может сказать, что в настоящее время она потребляет 32 256 метрических "Клиентских подключений".

Во время выполнения диспетчер кластерных ресурсов отслеживает оставшуюся емкость в кластере и на узлах. Для отслеживания емкости диспетчер кластерных ресурсов вычитает использование каждой службы из емкости узла, в котором работает служба. С помощью этой информации менеджер кластерных ресурсов может выяснить, где разместить или переместить реплики, чтобы узлы не перемещались по емкости.

<center>

![Узлы и емкость кластера][Image7]
</center>

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
ServiceLoadMetricDescription metric = new ServiceLoadMetricDescription();
metric.Name = "ClientConnections";
metric.PrimaryDefaultLoad = 1024;
metric.SecondaryDefaultLoad = 0;
metric.Weight = ServiceLoadMetricWeight.High;
serviceDescription.Metrics.Add(metric);
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

```PowerShell
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("ClientConnections,High,1024,0)
```

Вы можете увидеть возможности, определенные в кластерном манифесте. Вот пример для ClusterManifest.xml:

```xml
    <NodeType Name="NodeType03">
      <Capacities>
        <Capacity Name="ClientConnections" Value="65536"/>
      </Capacities>
    </NodeType>
```

Вот пример емкости, определяемые с помощью ClusterConfig.json для автономных развертываний или Template.json для кластеров, размещенных в Azure: 

```json
"nodeTypes": [
    {
        "name": "NodeType03",
        "capacities": {
            "ClientConnections": "65536",
        }
    }
],
```

Нагрузка службы часто изменяется динамически. Скажем, что загрузка реплики "ClientConnections" изменилась с 1024 до 2048. Узла, на который он работал, тогда емкость составляла всего 512, оставшиеся для этой метрики. Теперь, когда реплика или размещение экземпляра недействительны, потому что на этом узеле недостаточно места. Менеджер кластерных ресурсов должен вернуть узла ниже емкости. Снижает нагрузку на узор, который превышает емкость, перемещая одну или несколько реплик или экземпляров из этого узла в другие узлы. 

Менеджер кластерных ресурсов пытается свести к минимуму затраты на перемещение реплик. Вы можете узнать больше о [стоимости движения](service-fabric-cluster-resource-manager-movement-cost.md) и о [стратегии и правилах перебалансировки.](service-fabric-cluster-resource-manager-metrics.md)

## <a name="cluster-capacity"></a>Емкость кластера
Каким образом менеджер кластерных ресурсов Service Fabric не дает общему кластеру быть слишком полным? С динамической нагрузкой, есть не так много он может сделать. Службы могут иметь всплеск нагрузки независимо от действий, которые принимает менеджер кластерных ресурсов. В результате, ваш кластер с большим количеством запаса сегодня может быть недостаточно, если есть всплеск завтра. 

Элементы управления в управлении кластерными ресурсами помогают предотвратить проблемы. Первое, что вы можете сделать, это предотвратить создание новых рабочих нагрузок, которые могут привести к тому, что кластер станет полным.

Допустим, вы создаете службу без состояния, и у нее есть некоторая нагрузка, связанная с ней. Служба заботится о метрике "DiskSpaceInMb". Услуга будет потреблять пять единиц "DiskSpaceInMb" для каждого экземпляра службы. Вы хотите создать три экземпляра службы. Это означает, что вам нужно 15 единиц "DiskSpaceInMb", чтобы присутствовать в кластере для вас, чтобы даже создать эти экземпляры службы.

Менеджер кластерных ресурсов постоянно вычисляет емкость и потребление каждой метрики, чтобы определить оставшуюся емкость в кластере. Если места не хватает, менеджер кластерных ресурсов отклоняет вызов для создания службы.

Поскольку требование состоит только в том, что 15 единиц будут доступны, вы можете выделить это пространство по-разному. Например, на 15 различных узлах может находиться одна оставшаяся единица емкости, или три оставшихся блока емкости на пяти различных узлах. Если менеджер кластерных ресурсов может переставить вещи так, что на трех узлах доступно пять единиц, он размещает службу. Реорганизация кластера невозможна, если кластер почти полностью заполнен или существующие службы по какой-либо причине нельзя объединить. В остальных случаях она, как правило, возможна.

## <a name="buffered-capacity"></a>Буферная емкость
Еще одной особенностью менеджера кластерных ресурсов является буферная емкость. Она позволяет резервировать некоторую часть общей емкости узла. Этот буфер емкости используется только для размещения служб во время обновления и сбоев узлов. 

Буферная емкость определяется глобально в метрике для всех узлов. Значение, которое вы выбираете для зарезервированной емкости, зависит от количества доменов неисправностей и обновления, которые есть в кластере. Больше доменов неисправностей и обновления означает, что вы можете выбрать меньшее число для буферной емкости. Чем больше доменов, тем меньшее количество кластеров будет недоступным во время обновлений и сбоев. Указание буферизированной емкости имеет смысл только в том случае, если вы также указали емкость узла для метрики.

Вот пример того, как указать буферную емкость в ClusterManifest.xml:

```xml
        <Section Name="NodeBufferPercentage">
            <Parameter Name="SomeMetric" Value="0.15" />
            <Parameter Name="SomeOtherMetric" Value="0.20" />
        </Section>
```

Вот пример того, как указать буферную емкость через ClusterConfig.json для автономных развертываний или Template.json для кластеров, размещенных в Azure:

```json
"fabricSettings": [
  {
    "name": "NodeBufferPercentage",
    "parameters": [
      {
          "name": "SomeMetric",
          "value": "0.15"
      },
      {
          "name": "SomeOtherMetric",
          "value": "0.20"
      }
    ]
  }
]
```

При недостаточной емкости буфера для метрики создание служб завершается сбоем. Предотвращение создания новых служб для сохранения буфера гарантирует, что обновления и сбои не приведут к превышению емкости узлов. Буферная емкость не является обязательной, но мы рекомендуем ее в любом кластере, который определяет емкость для метрики.

Менеджер ресурсов кластера предоставляет информацию об этой нагрузке. Для каждой метрики эти сведения содержат: 
- Параметры буферизированной емкости.
- Общая мощность.
- Текущее потребление.
- Считается ли каждая метрика сбалансированной или нет.
- Статистика о стандартном отклонении.
- Узлы, которые имеют наибольшую и наименьшую нагрузку.  
  
Следующий код показывает пример этого вывода:

```PowerShell
PS C:\Users\user> Get-ServiceFabricClusterLoadInformation
LastBalancingStartTimeUtc : 9/1/2016 12:54:59 AM
LastBalancingEndTimeUtc   : 9/1/2016 12:54:59 AM
LoadMetricInformation     :
                            LoadMetricName        : Metric1
                            IsBalancedBefore      : False
                            IsBalancedAfter       : False
                            DeviationBefore       : 0.192450089729875
                            DeviationAfter        : 0.192450089729875
                            BalancingThreshold    : 1
                            Action                : NoActionNeeded
                            ActivityThreshold     : 0
                            ClusterCapacity       : 189
                            ClusterLoad           : 45
                            ClusterRemainingCapacity : 144
                            NodeBufferPercentage  : 10
                            ClusterBufferedCapacity : 170
                            ClusterRemainingBufferedCapacity : 125
                            ClusterCapacityViolation : False
                            MinNodeLoadValue      : 0
                            MinNodeLoadNodeId     : 3ea71e8e01f4b0999b121abcbf27d74d
                            MaxNodeLoadValue      : 15
                            MaxNodeLoadNodeId     : 2cc648b6770be1bc9824fa995d5b68b1
```

## <a name="next-steps"></a>Дальнейшие действия
* Для получения информации об архитектуре и [Cluster Resource Manager architecture overview](service-fabric-cluster-resource-manager-architecture.md)потоке информации в рамках управления кластерными ресурсами см.
* Определение метрик дефрагментации является одним из способов консолидации нагрузки на узлы, а не ее распространения. Чтобы узнать, как настроить дефрагментацию, [см. Дефрагментации метрик и загрузки в Service Fabric](service-fabric-cluster-resource-manager-defragmentation-metrics.md).
* Начните с самого начала и [получите введение в сервис Fabric кластера ресурсов менеджера.](service-fabric-cluster-resource-manager-introduction.md)
* Чтобы узнать, как менеджер кластерных ресурсов управляет и уравновешивает нагрузку в кластере, [см.](service-fabric-cluster-resource-manager-balancing.md)

[Image1]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-domains.png
[Image2]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-uneven-fault-domain-layout.png
[Image3]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domains-with-placement.png
[Image4]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domain-layout-strategies.png
[Image5]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-layout-different-workloads.png
[Image6]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-placement-constraints-node-properties.png
[Image7]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-nodes-and-capacity.png
