---
title: TCP/IP-настройка производительности для Пазур VMs Документы Майкрософт
description: Изучите различные общие методы настройки производительности TCP/IP и их связь с MMs Azure.
services: virtual-network
documentationcenter: na
author: rimayber
manager: paragk
editor: ''
ms.assetid: ''
ms.service: virtual-network
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: infrastructure-services
ms.date: 04/02/2019
ms.author: rimayber
ms.reviewer: dgoddard, stegag, steveesp, minale, btalb, prachank
ms.openlocfilehash: bb23484903ac3ce129c6e7a7a27e0765c227fb1d
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/27/2020
ms.locfileid: "68297782"
---
# <a name="tcpip-performance-tuning-for-azure-vms"></a>TCP/IP performance tuning for Azure VMs (Настройка производительности TCP/IP для виртуальных машин Azure)

В этой статье рассматриваются общие методы настройки производительности TCP/IP и некоторые вещи, которые следует учитывать при их использовании для виртуальных машин, работающих на Azure. Он будет предоставлять базовый обзор методов и изучить, как они могут быть настроены.

## <a name="common-tcpip-tuning-techniques"></a>Общие методы настройки TCP/IP

### <a name="mtu-fragmentation-and-large-send-offload"></a>MTU, фрагментация и большая разгрузка отправки

#### <a name="mtu"></a>Максимальный передаваемый блок данных

Максимальная трансмиссия (MTU) представляет собой самую большую рамку (пакет), указанную в байтах, которая может быть отправлена через сетевой интерфейс. MTU является настраиваемой настройкой. MTU по умолчанию, используемый на MMs Azure, и настройка по умолчанию на большинстве сетевых устройств по всему миру составляет 1500 байтов.

#### <a name="fragmentation"></a>Фрагментации

Фрагментация происходит при отправке пакета, превышающей MTU сетевого интерфейса. Стек TCP/IP разбивает пакет на более мелкие части (фрагменты), которые соответствуют MTU интерфейса. Фрагментация происходит на слое IP и не зависит от базового протокола (например, TCP). Когда пакет с 2000 байт отправляется через сетевой интерфейс с MTU 1500, пакет будет разбит на один пакет с 1500 байт и один пакет на 500 байт.

Сетевые устройства в пути между источником и пунктом назначения могут либо отбросить пакеты, превышаюющие MTU, либо раздрогнуть пакет на более мелкие части.

#### <a name="the-dont-fragment-bit-in-an-ip-packet"></a>Не фрагмент бит в IP-пакет

Бит Don't Fragment (DF) является флагом в заголовке протокола IP. Бит DF указывает, что сетевые устройства на пути между отправителем и приемником не должны фрагментирования пакета. Этот бит может быть установлен по многим причинам. (См. раздел "Открытие МТУ" в этой статье, например.) Когда сетевое устройство получает пакет с набором битов Don't Fragment и этот пакет превышает интерфейс mTU устройства, стандартное поведение заключается в том, чтобы устройство отбросило пакет. Устройство отправляет ICMP Фрагментация Необходимое сообщение обратно в исходный источник пакета.

#### <a name="performance-implications-of-fragmentation"></a>Последствия фрагментации производительности

Фрагментация может иметь негативные последствия для производительности. Одной из основных причин влияния на производительность является влияние фрагментации и сборки пакетов на процессор/память. Когда сетевому устройству необходимо фрагментировать пакет, ему придется выделить ресурсы процессора/памяти для выполнения фрагментации.

То же самое происходит, когда пакет собирается повторно. Сетевое устройство должно хранить все фрагменты до тех пор, пока они не будут получены, чтобы собрать их в исходный пакет. Этот процесс фрагментации и повторной сборки также может вызвать задержку.

Другим возможным негативным последствием фрагментации является то, что фрагментированные пакеты могут поступать из строя. Когда пакеты получают из строя, некоторые типы сетевых устройств могут отказаться от них. Когда это происходит, весь пакет должен быть повторно передан.

Фрагменты обычно отсутываются устройствами безопасности, такими как сетевые брандмауэры или когда буферы получения сетевого устройства исчерпаны. Когда буферы получения сетевого устройства исчерпаны, сетевое устройство пытается собрать фрагментированный пакет, но не имеет ресурсов для хранения и повторного создания пакета.

Фрагментация может рассматриваться как негативная операция, но поддержка фрагментации необходима при подключении различных сетей через Интернет.

#### <a name="benefits-and-consequences-of-modifying-the-mtu"></a>Преимущества и последствия изменения МТУ

Вообще говоря, вы можете создать более эффективную сеть за счет увеличения MTU. Каждый передаваемый пакет содержит информацию о заголовке, добавленную в исходный пакет. Когда фрагментация создает больше пакетов, накладные расходы накладные расходы больше, и это делает сеть менее эффективной.

Ниже приведен пример. Размер заголовка Ethernet составляет 14 байтов плюс последовательность проверки кадра на 4 байта, чтобы обеспечить консистенцию кадра. При отправке одного пакета с 2000 байт в сеть добавляется 18 байтов накладных расходов Ethernet. Если пакет фрагментирован на пакет объемом 1500 байт и пакет объемом 500 байт, каждый пакет будет иметь 18 байтов заголовка Ethernet, в общей сложности 36 байтов.

Имейте в виду, что увеличение MTU не обязательно создаст более эффективную сеть. Если приложение отправляет только 500-байт пакеты, то же заголовок накладные расходы будут существовать ли MTU 1500 байтов или 9000 байтов. Сеть станет более эффективной только в том случае, если она использует большие размеры пакетов, которые зависят от MTU.

#### <a name="azure-and-vm-mtu"></a>Azure и ВМ МТУ

MTU по умолчанию для VMs Azure составляет 1500 байтов. Стек виртуальной сети Azure попытается фрагментировать пакет на 1400 байт.

Обратите внимание, что стек виртуальной сети по своей сути не является неэффективным, поскольку он фрагментирует пакеты на 1400 байт, даже если у виртуальных вс-карт MTU 1500. Значительная часть сетевых пакетов намного меньше, чем 1400 или 1500 байтов.

#### <a name="azure-and-fragmentation"></a>Лазурный и фрагментационный

Стек виртуальной сети настроен на выпадение «фрагментов порядка», то есть фрагментированных пакетов, которые не поступают в исходном фрагментированном порядке. Эти пакеты удавливаются главным образом из-за уязвимости сетевой безопасности, объявленной в ноябре 2018 года под названием FragmentSmack.

FragmentSmack является дефектом в том, как ядро Linux обрабатывается сборка фрагментированных пакетов IPv4 и IPv6. Удаленный злоумышленник может использовать этот недостаток для запуска дорогостоящих операций сборки фрагментов, что может привести к увеличению процессора и отказу в обслуживании в целевой системе.

#### <a name="tune-the-mtu"></a>Настройте MTU

Вы можете настроить Azure VM MTU, как вы можете в любой другой операционной системе. Но при настройке MTU следует учитывать фрагментацию, которая происходит в Azure, описанную выше.

Мы не поощряем клиентов увеличивать ВМ МТУ. Это обсуждение призвано объяснить детали того, как Azure реализует MTU и выполняет фрагментацию.

> [!IMPORTANT]
>Увеличение MTU, как известно, не повышает производительность и может негативно сказаться на производительности приложения.
>
>

#### <a name="large-send-offload"></a>Большая разгрузка отправки

Большая разгрузка отправки (LSO) может повысить производительность сети за счет разгрузки сегментации пакетов в адаптер Ethernet. Когда LSO включен, стек TCP/IP создает большой пакет TCP и отправляет его в адаптер Ethernet для сегментации перед его пересылкой. Преимущество LSO заключается в том, что он может освободить процессор от сегментации пакетов на размеры, которые соответствуют MTU и разгрузить, что обработка интерфейса Ethernet, где она выполняется в оборудовании. Чтобы узнать больше о преимуществах LSO, [см.](https://docs.microsoft.com/windows-hardware/drivers/network/performance-in-network-adapters#supporting-large-send-offload-lso)

При включении LSO клиенты Azure могут видеть большие размеры кадров при выполнении захватов пакетов. Эти большие размеры кадров могут привести некоторых клиентов к мысли о том, что фрагментация происходит или что большой MTU используется, когда это не так. С LSO адаптер Ethernet может рекламировать больший максимальный размер сегмента (MSS) в стек TCP/IP для создания большего пакета TCP. Вся эта несегментированная рамка затем перенаправляется в адаптер Ethernet и будет видна в захвате пакетов, выполняемом на VM. Но пакет будет разбит на множество небольших кадров адаптером Ethernet, согласно MTU адаптера Ethernet.

### <a name="tcp-mss-window-scaling-and-pmtud"></a>Масштабирование окна TCP MSS и PMTUD

#### <a name="tcp-maximum-segment-size"></a>Максимальный размер сегмента TCP

Максимальный размер сегмента TCP (MSS) — это параметр, ограничивающий размер сегментов TCP, что позволяет избежать фрагментации пакетов TCP. Операционные системы, как правило, используют эту формулу для установки MSS:

`MSS = MTU - (IP header size + TCP header size)`

Заголовок IP и заголовок TCP составляют 20 байтов каждый, или 40 байтов в общей сложности. Таким образом, интерфейс с MTU 1500 будет иметь MSS 1460. Но MSS настраивается.

Эта настройка согласовывается в трехстороннем рукопожатии TCP при настройке сеанса TCP между источником и пунктом назначения. Обе стороны отправляют значение MSS, а нижняя часть двух используется для соединения TCP.

Имейте в виду, что МТу источника и назначения не являются единственными факторами, определяющими значение MSS. Промежуточные сетевые устройства, такие как VPN шлюзы, включая Azure VPN Gateway, могут настроить MTU независимо от источника и назначения, чтобы обеспечить оптимальную производительность сети.

#### <a name="path-mtu-discovery"></a>Путь MTU Открытие

MSS обсуждается, но он может не указывать на фактические MSS, которые могут быть использованы. Это связано с тем, что другие сетевые устройства на пути между источником и пунктом назначения могут иметь более низкое значение MTU, чем источник и пункт назначения. В этом случае устройство, MTU которого меньше пакета, отбросит пакет. Устройство отправит обратно icMP Фрагментация Необходимая (Тип 3, код 4) сообщение, которое содержит его MTU. Это сообщение ICMP позволяет хосту-источнику соответствующим образом уменьшить свой Путь MTU. Процесс называется Path MTU Discovery (PMTUD).

Процесс PMTUD неэффективен и влияет на производительность сети. При отправке пакетов, превышающей MTU сетевого пути, пакеты должны быть повторно переданы с более низким MSS. Если отправитель не получает icMP Фрагментация Необходимое сообщение, может быть, из-за сетевого брандмауэра в пути (обычно называют *PMTUD blackhole),* отправитель не знает, что необходимо снизить MSS и будет постоянно ретранслировать пакет. Вот почему мы не рекомендуем увеличивать Azure VM MTU.

#### <a name="vpn-and-mtu"></a>VPN и MTU

Если вы используете VMs, выполняющие инкапсуляцию (например, IPsec Vpn), есть некоторые дополнительные соображения, касающиеся размера пакетов и MTU. VPN добавляют больше заголовков в пакеты, что увеличивает размер пакета и требует меньшего MSS.

Для Azure мы рекомендуем установить зажим TCP MSS до 1350 байтов и туннельный интерфейс MTU до 1400. Для получения дополнительной [информации смотрите VPN-устройства и страницу параметров IPSec/IKE.](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpn-devices)

### <a name="latency-round-trip-time-and-tcp-window-scaling"></a>Задержка, время в пути и масштабирование окна TCP

#### <a name="latency-and-round-trip-time"></a>Задержка и время в пути туда и обратно

Задержка сети регулируется скоростью света по волоконно-оптической сети. Пропускная пропускная связь сети TCP также эффективно регулируется временем в оба конца (RTT) между двумя сетевыми устройствами.

| | | | |
|-|-|-|-|
|**Маршрут**|**Distance**|**Одностороннее время**|**RTT**|
|Нью-йорк - Сан-Франциско|4 148 км|21 мс|42 мс|
|Нью-йорк - Лондон|5 585 км|28 мс|56 мс|
|Нью-йорк - Сидней|15 993 км|80 мс|160 мс|

В этой таблице отображается расстояние прямой линии между двумя местоположениями. В сетях расстояние, как правило, больше, чем расстояние прямой линии. Вот простая формула для расчета минимального RTT, регулируемая скоростью света:

`minimum RTT = 2 * (Distance in kilometers / Speed of propagation)`

Вы можете использовать 200 для скорости распространения. Это расстояние, в метрах, что свет проходит в 1 миллисекунду.

Возьмем в качестве примера Нью-йорк в Сан-Франциско. Расстояние прямой линии составляет 4148 км. Подключив это значение к уравнению, мы получаем следующее:

`Minimum RTT = 2 * (4,148 / 200)`

Выход уравнения составляет миллисекунды.

Если вы хотите получить наилучшую производительность сети, логичным вариантом является выбор направлений с кратчайшим расстоянием между ними. Вы также должны разработать виртуальную сеть для оптимизации пути трафика и снижения задержки. Для получения дополнительной информации смотрите раздел "Соображения проектирования сети" в этой статье.

#### <a name="latency-and-round-trip-time-effects-on-tcp"></a>Задержка и время в оба конца влияет на TCP

Время в пути оказывает непосредственное влияние на максимальную пропускную связь TCP. В протоколе TCP *размер окна* — это максимальный объем трафика, который может быть отправлен через соединение TCP до того, как отправителю необходимо получить подтверждение от получателя. Если TCP MSS установлен до 1460, а размер окна TCP установлен до 65 535, отправитель может отправить 45 пакетов, прежде чем он должен получить подтверждение от получателя. Если отправитель не получает подтверждения, он будет ретранслировать данные. Формула выглядит так:

`TCP window size / TCP MSS = packets sent`

В этом примере 65 535 / 1460 округляется до 45.

Это состояние "ожидания признания", механизм для обеспечения надежной передачи данных, является то, что причины RTT повлиять на пропускную стоимость TCP. Чем дольше отправитель ждет подтверждения, тем дольше ему нужно ждать, прежде чем отправлять дополнительные данные.

Вот формула для расчета максимальной пропускной связи одного соединения TCP:

`Window size / (RTT latency in milliseconds / 1,000) = maximum bytes/second`

В этой таблице отображается максимальная мегабайт/в секунду пропускной однако одно соединение TCP. (Для читаемости мегабайты используются для единицы измерения.)

| | | | |
|-|-|-|-|
|**Размер окна TCP (байты)**|**Задержка RTT (ms)**|**Максимальная мегабайтная/секундная пропускная их часть**|**Максимальная пропускная сила мегабита/секунда**|
|65 535|1|65.54|524.29|
|65 535|30|2.18|17.48|
|65 535|60|1,09|8.74|
|65 535|90|.73|5.83|
|65 535|120|.55|4.37|

Если пакеты потеряны, максимальная пропускная плата соединения TCP будет уменьшена, в то время как отправитель повторно передает данные, которые он уже отправил.

#### <a name="tcp-window-scaling"></a>Масштабирование окна TCP

Масштабирование окна TCP — это метод, который динамически увеличивает размер окна TCP, чтобы можно было отправлять больше данных до того, как потребуется подтверждение. В предыдущем примере 45 пакетов будут отправлены до того, как потребуется подтверждение. Если увеличить количество пакетов, которые могут быть отправлены до подтверждения необходимо, вы уменьшаете количество раз отправитель ждет подтверждения, что увеличивает максимальную пропускную стоимость TCP.

Эта таблица иллюстрирует эти отношения:

| | | | |
|-|-|-|-|
|**Размер окна TCP (байты)**|**Задержка RTT (ms)**|**Максимальная мегабайтная/секундная пропускная их часть**|**Максимальная пропускная сила мегабита/секунда**|
|65 535|30|2.18|17.48|
|131,070|30|4.37|34.95|
|262,140|30|8.74|69.91|
|524,280|30|17.48|139.81|

Но значение зазаголовка TCP для размера окна TCP составляет всего 2 байта длиной, что означает, что максимальное значение для окна получения составляет 65 535. Для увеличения максимального размера окна был введен коэффициент шкалы окна TCP.

Коэффициент масштаба также является параметром, который можно настроить в операционной системе. Вот формула для расчета размера окна TCP с помощью факторов масштаба:

`TCP window size = TCP window size in bytes \* (2^scale factor)`

Вот расчет для коэффициента шкалы окна 3 и размера окна 65 535:

`65,535 \* (2^3) = 262,140 bytes`

Коэффициент масштаба 14 приводит к размеру окна TCP 14 (максимально допустимое смещение). Размер окна TCP составит 1 073 725 440 байтов (8,5 гигабит).

#### <a name="support-for-tcp-window-scaling"></a>Поддержка масштабирования окна TCP

Windows может устанавливать различные факторы масштабирования для различных типов соединений. (Классы соединений включают центр обработки данных, Интернет и так далее.) Для просмотра `Get-NetTCPConnection` типа соединения масштабирования окон используется команда PowerShell:

```powershell
Get-NetTCPConnection
```

Для просмотра `Get-NetTCPSetting` значений каждого класса можно использовать команду PowerShell:

```powershell
Get-NetTCPSetting
```

Вы можете установить начальный размер окна TCP и фактор `Set-NetTCPSetting` масштабирования TCP в Windows с помощью команды PowerShell. Для получения дополнительной информации [см.](https://docs.microsoft.com/powershell/module/nettcpip/set-nettcpsetting?view=win10-ps)

```powershell
Set-NetTCPSetting
```

Это эффективные настройки TCP для: `AutoTuningLevel`

| | | | |
|-|-|-|-|
|**АвтоТюнингУровень**|**Коэффициент масштабирования**|**Масштабирование множителя**|**Формула<br/>для расчета максимального размера окна**|
|Выключено|None|None|Размер окна|
|С ограниченным доступом|4|2х4|Размер окна|
|Высоко ограничено|2|2х2 х 2|Размер окна|
|Нормальный|8|2х8|Размер окна|
|В экспериментальном режиме|14|От 2до 14|Размер окна|

Эти параметры, скорее всего, влияют на производительность TCP, но имейте в виду, что многие другие факторы в Интернете, вне контроля Azure, также могут повлиять на производительность TCP.

#### <a name="increase-mtu-size"></a>Увеличение размера MTU

Поскольку более крупный MTU означает больший MSS, вы можете задаться вопросом, может ли увеличение MTU увеличить производительность TCP. Скорее всего, нет. Есть плюсы и минусы размера пакета за пределами просто tCP трафика. Как отмечалось ранее, наиболее важными факторами, влияющими на производительность пропускной способности TCP, являются размер окна TCP, потеря пакетов и RTT.

> [!IMPORTANT]
> Мы не рекомендуем клиентам Azure изменять значение MTU по умолчанию на виртуальных машинах.
>
>

### <a name="accelerated-networking-and-receive-side-scaling"></a>Ускоренная сеть и получение бокового масштабирования

#### <a name="accelerated-networking"></a>Ускорение работы в сети

Функции виртуальной сети машин исторически были интенсивными процессором как на гостевом VM, так и на гипервизоре/хозяине. Каждый пакет, проходящий через хост, обрабатывается в программном обеспечении процессором-хозяином, включая всю виртуальную сетевую инкапсуляцию и декапсулуляцию. Таким образом, чем больше трафика проходит через хост, тем выше нагрузка процессора. И если процессор хоста занят другими операциями, это также повлияет на пропускную силу сети и задержку. Azure решает эту проблему с помощью ускоренной сети.

Ускоренная сеть обеспечивает последовательную сверхнизкую задержку сети через штатное программируемое оборудование Azure и такие технологии, как SR-IOV. Ускоренная сеть перемещает большую часть программно-определяемой сети Azure с пирочетных процессоров в SmartNICs на базе FPGA. Это изменение позволяет конечным пользователям приложений восстановить вычислительные циклы, что снижает нагрузку на VM, уменьшая дразнили и несоответствие в задержке. Другими словами, производительность может быть более детерминированной.

Ускоренная сеть повышает производительность, позволяя гостевому VM обходить хост и устанавливать траекторию данных непосредственно с SmartNIC хоста. Вот некоторые преимущества ускоренной сети:

- **Более низкая задержка / более высокая задержка пакетов в секунду (pps):** Удаление виртуального переключателя с пути данных исключает время пакетов, затрагэвных в хосте для обработки политики, и увеличивает количество пакетов, которые могут быть обработаны в VM.

- **Снижение испуга:** Виртуальная обработка коммутатора зависит от объема политики, которая должна быть применена и рабочей нагрузки процессора, который делает обработку. Разгрузка политики на аппаратное обеспечение удаляет эту изменчивость путем доставки пакетов непосредственно в VM, устраняя связь host-to-VM и все прерывания программного обеспечения и переключения контекста.

- **Снижение использования процессора**: Обход виртуального коммутатора в хосте приводит к уменьшению использования процессора для обработки сетевого трафика.

Для использования ускоренной сети необходимо четко включить ее на каждом применимом VM. Смотрите [Создать виртуальную машину Linux с ускоренной сети](https://docs.microsoft.com/azure/virtual-network/create-vm-accelerated-networking-cli) для инструкций.

#### <a name="receive-side-scaling"></a>Получение бокового масштабирования

Получение бокового масштабирования (RSS) — это технология сетевого драйвера, которая более эффективно распределяет прием сетевого трафика путем распределения обработки получаемых процессоров по нескольким процессорам в многопроцессорной системе. Проще говоря, RSS позволяет системе обрабатывать больше полученного трафика, поскольку она использует все доступные процессоры, а не только один. Для более технического обсуждения RSS [см.](https://docs.microsoft.com/windows-hardware/drivers/network/introduction-to-receive-side-scaling)

Чтобы получить наилучшую производительность при включении ускоренной сети на VM, необходимо включить RSS. RSS также может предоставлять преимущества на vMs, которые не используют ускоренные сети. Для получения обзора того, как можно определить, включен ли RSS и как включить его, [см.](https://aka.ms/FastVM)

### <a name="tcp-time_wait-and-time_wait-assassination"></a>TIME_WAIT и TIME_WAIT убийства TCP

TCP TIME_WAIT является еще одним распространенным параметром, который влияет на производительность сети и приложения. На занятых VMs, которые открывают и закрывают многие розетки, либо в качестве клиентов или в качестве серверов (Источник IP:Source Port и Destination PORT), во время нормальной работы TCP, данный гнездо может в конечном итоге в TIME_WAIT состоянии в течение длительного времени. Состояние TIME_WAIT предназначено для доставки любых дополнительных данных на розетку перед его закрытием. Таким образом, стеки TCP/IP обычно предотвращают повторное использование гнезда, бесшумно сбрасывая пакет TCP SYN клиента.

Количество времени, в TIME_WAIT настраивается. Он может варьироваться от 30 секунд до 240 секунд. Розетки являются конечным ресурсом, и количество розеток, которые могут быть использованы в любой момент времени, настраивается. (Количество доступных розеток, как правило, около 30000.) Если доступные розетки потребляются, или если клиенты и серверы имеют несоответствие TIME_WAIT настройках, а VM пытается повторно использовать розетку в TIME_WAIT состоянии, новые соединения сбой, как TCP SYN пакеты молча упал.

Значение для диапазона портов для исходящих розеток обычно настраивается в стеке TCP/IP операционной системы. То же самое верно для TCP TIME_WAIT настройки и разъем повторного использования. Изменение этих чисел потенциально может улучшить масштабируемость. Но, в зависимости от ситуации, эти изменения могут вызвать проблемы со совместимости. Вы должны быть осторожны, если вы измените эти значения.

Вы можете использовать TIME_WAIT убийство для решения этого ограничения масштабирования. TIME_WAIT убийство позволяет повторно использовать гнездо в определенных ситуациях, например, когда номер последовательности в пакете IP нового соединения превышает номер последовательности последнего пакета из предыдущего соединения. В этом случае операционная система позволит установить новое соединение (оно примет новый SYN/ACK) и заставить закрыть предыдущее соединение, которое находилось в TIME_WAIT состоянии. Эта возможность поддерживается на Windows VMs в Azure. Чтобы узнать о поддержке в других vMs, обратитесь к поставщику ОС.

Чтобы узнать о настройке TCP TIME_WAIT настройках [Settings that can be modified to improve network performance](https://docs.microsoft.com/biztalk/technical-guides/settings-that-can-be-modified-to-improve-network-performance)и диапазоне исходных портов, см.

## <a name="virtual-network-factors-that-can-affect-performance"></a>Виртуальные сетевые факторы, которые могут повлиять на производительность

### <a name="vm-maximum-outbound-throughput"></a>Максимальная исходящие пропускная их часть VM

Azure предоставляет различные размеры и типы VM, каждый из которых имеет различное сочетание возможностей производительности. Одной из таких возможностей является пропускная способность сети (или пропускная способность), которая измеряется в мегабитах в секунду (Мбит/с). Поскольку виртуальные машины размещаются на общем оборудовании, сетевая емкость должна быть справедливо распределена между виртуальными машинами с использованием одного и того же оборудования. Большие виртуальные машины выделяются больше пропускной способности, чем меньше виртуальных машин.

Пропускная способность сети, выделяемая каждой виртуальной машине, определяет скорость передачи данных от виртуальной машины (исходящий трафик). Ограничение распространяется на весь сетевой трафик, покидающий виртуальную машину, независимо от его назначения. Например, если виртуальная машина имеет ограничение в 1000 Мбит/с, этот предел применяется независимо от того, предназначен ли исходящий трафик для другой виртуальной машины в той же виртуальной сети или за пределами Azure.

Входящий трафик не измеряется и не ограничивается напрямую. Но есть и другие факторы, такие как процессор и ограничения на хранение, которые могут повлиять на способность виртуальной машины обрабатывать входящие данные.

Ускоренная сеть предназначена для повышения производительности сети, включая задержку, пропускную способность и использование процессора. Ускоренная сеть может улучшить пропускную способность виртуальной машины, но она может сделать это только до выделенной пропускной способности виртуальной машины.

Виртуальные машины Azure имеют по крайней мере один сетевой интерфейс, прикрепленный к ним. У них может быть несколько. Пропускная способность, выделенная виртуальной машине, представляет собой сумму всего исходящего трафика по всем сетевым интерфейсам, прикрепленным к машине. Другими словами, пропускная способность распределяется на основе виртуальной машины, независимо от того, сколько сетевых интерфейсов подключено к машине.

Ожидаемая исходящие пропускная информация и количество сетевых интерфейсов, поддерживаемых каждым размером VM, подробно описаны в [размерах для виртуальных машин Windows в Azure.](https://docs.microsoft.com/azure/virtual-machines/windows/sizes?toc=%2fazure%2fvirtual-network%2ftoc.json) Чтобы увидеть максимальную пропускную выливку, выберите тип, **например, Общее назначение,** а затем найдите раздел о серии размеров на резвящемся странице (например, "Dv2-серия"). Для каждой серии есть таблица, которая предоставляет сетевые спецификации в последнем столбце, который называется "Max NICs / Ожидаемая пропускная способность сети (Mbps) ".

Ограничение пропускной способности применяется ко всей виртуальной машине. Пропускная часть не зависит от этих факторов:

- **Количество сетевых интерфейсов**: Ограничение пропускной способности применяется к сумме всего исходящего трафика с виртуальной машины.

- **Ускоренная сеть**: Хотя эта функция может быть полезна в достижении опубликованного лимита, она не меняет лимита.

- **Назначение трафика.** При оценке ограничения на исходящий трафик полностью учитываются все назначения.

- **Протокол.** При оценке ограничения на исходящий трафик полностью учитываются все протоколы.

Для получения дополнительной информации, см [Виртуальная сеть машин пропускной способности](https://aka.ms/AzureBandwidth).

### <a name="internet-performance-considerations"></a>Соображения производительности Интернета

Как уже говорилось в этой статье, факторы в Интернете и вне контроля Azure могут повлиять на производительность сети. Вот некоторые из этих факторов:

- **Задержка**: Время в оба конца между двумя направлениями может зависеть от проблем в промежуточных сетях, трафика, который не принимает "короткий" путь расстояния, и неоптимальные пути вглядывания.

- **Потеря пакета**: Потеря пакета может быть вызвана перегрузкой сети, проблемами с физическим исполнимым уровнем безопасности и неудовлетворительными сетевыми устройствами.

- **Размер MTU/Фрагментация**: Фрагментация вдоль пути может привести к задержкам в прибытии данных или в пакетах, прибывающих из строя, что может повлиять на доставку пакетов.

Traceroute является хорошим инструментом для измерения характеристик производительности сети (например, потеря пакетов и задержка) на каждом сетевом пути между исходным устройством и устройством назначения.

### <a name="network-design-considerations"></a>Соображения проектирования сети

Наряду с соображениями, рассмотренными ранее в этой статье, топология виртуальной сети может повлиять на производительность сети. Например, дизайн концентратора и комментатора, который возводит трафик по всему миру в виртуальную сеть с одним концентратором, введет задержку сети, что повлияет на общую производительность сети.

Количество сетевых устройств, через которые проходит сетевой трафик, также может повлиять на общую задержку. Например, в дизайне концентратора и спица, если трафик проходит через виртуальное приложение спицовой сети и виртуальный прибор концентратора перед переходом в Интернет, сетевые виртуальные приборы могут ввести задержку.

### <a name="azure-regions-virtual-networks-and-latency"></a>Регионы Azure, виртуальные сети и задержка

Области Azure состоят из нескольких центров обработки данных, которые существуют в общей географической области. Эти центры обработки данных могут физически не находиться рядом друг с другом. В некоторых случаях они разделены на целых 10 километров. Виртуальная сеть является логическим наложением поверх сети физического центра обработки данных Azure. Виртуальная сеть не подразумевает какой-либо конкретной топологии сети в центре обработки данных.

Например, два виртуальных ввоза, которые находятся в одной виртуальной сети и подсети, могут находиться в разных стойках, рядах или даже центрах обработки данных. Они могут быть разделены ногами волоконно-оптического кабеля или километрами волоконно-оптического кабеля. Это изменение может привести к переменной задержке (разница в несколько миллисекунд) между различными ВМ.

Географическое размещение ВМ и потенциальная задержка между двумя ВМ могут зависеть от конфигурации наборов доступности и зон доступности. Но расстояние между центрами обработки данных в регионе зависит от региона и в первую очередь зависит от топологии центров обработки данных в регионе.

### <a name="source-nat-port-exhaustion"></a>Источник NAT порт истощения

Развертывание в Azure может общаться с конечными точками за пределами Azure в общедоступном интернете и/или в общедоступном пространстве IP. Когда экземпляр инициирует исходящие соединения, Azure динамически отображает личный IP-адрес на общедоступный IP-адрес. После того, как Azure создает это отображение, возвратный трафик для исходящего потока может также достичь частного IP-адреса, по которому возник поток.

Для каждого исходящего соединения балансируру Azure Load Balancer необходимо поддерживать это отображение в течение некоторого периода времени. С мультитенантным характером Azure сохранение этого отображения для каждого исходящего потока для каждого ВМ может быть ресурсоемким. Таким образом, существуют ограничения, которые устанавливаются и основаны на конфигурации виртуальной сети Azure. Или, точнее, Azure VM может сделать только определенное количество исходящих соединений в данный момент времени. Когда эти ограничения будут достигнуты, VM не сможет сделать больше исходящих соединений.

Но такое поведение настраивается. Для получения дополнительной информации об истощении порта SNAT и SNAT [см.](https://docs.microsoft.com/azure/load-balancer/load-balancer-outbound-connections)

## <a name="measure-network-performance-on-azure"></a>Измерение производительности сети на Azure

Ряд максимумов производительности в этой статье связаны с задержкой сети / время в оба конца (RTT) между двумя VMs. В этом разделе представлены некоторые предложения о том, как проверить задержку/RTT и как проверить производительность TCP и VM-сети. Вы можете настроить и проверить производительность TCP / IP и сетевые значения, обсуждаемые ранее, используя методы, описанные в этом разделе. Вы можете подключить значения задержки, MTU, MSS и размера окна в приведенные ранее вычисления и сравнить теоретические максимумы с фактическими значениями, которые вы наблюдаете во время тестирования.

### <a name="measure-round-trip-time-and-packet-loss"></a>Измерение времени в оба конца и потери пакета

Производительность TCP в значительной степени зависит от RTT и потери пакетов. Утилита PING, доступная в Windows и Linux, обеспечивает самый простой способ измерения потери RTT и пакетов. Выход PING покажет минимальную/максимальную/среднюю задержку между источником и пунктом назначения. Он также покажет потерю пакета. PING использует протокол ICMP по умолчанию. Вы можете использовать PsPing для тестирования TCP RTT. Для получения дополнительной информации [см.](https://docs.microsoft.com/sysinternals/downloads/psping)

### <a name="measure-actual-throughput-of-a-tcp-connection"></a>Измерение фактической пропускной связи TCP

NTttcp является инструментом для тестирования производительности TCP Linux или Windows VM. Вы можете изменить различные настройки TCP, а затем проверить преимущества с помощью NTttcp. Для получения дополнительных сведений см. следующие ресурсы.

- [Тестирование пропускной способности/пропускной способности (NTttcp)](https://aka.ms/TestNetworkThroughput)

- [Утилита NTttcp](https://gallery.technet.microsoft.com/NTttcp-Version-528-Now-f8b12769)

### <a name="measure-actual-bandwidth-of-a-virtual-machine"></a>Измерение фактической пропускной способности виртуальной машины

Вы можете проверить производительность различных типов VM, ускоренные сети и так далее, используя инструмент под названием iPerf. iPerf также доступен на Linux и Windows. iPerf может использовать TCP или UDP для проверки общей пропускной связи сети. IPerf TCP пропускной тесты зависят от факторов, обсуждаемых в этой статье (например, задержка и RTT). Таким образом, UDP может дать лучшие результаты, если вы просто хотите проверить максимальную пропускную выливку.

Дополнительные сведения вы найдете в следующих статьях:

- [Производительность сети Expressroute с устранением неполадок](https://docs.microsoft.com/azure/expressroute/expressroute-troubleshooting-network-performance)

- [Порядок проверки пропускной способности VPN для виртуальной сети](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-validate-throughput-to-vnet)

### <a name="detect-inefficient-tcp-behaviors"></a>Обнаружение неэффективных моделей поведения TCP

В захватах пакетов клиенты Azure могут видеть пакеты TCP с флагами TCP (SACK, DUP ACK, RETRANSMIT и FAST RETRANSMIT), которые могут указывать на проблемы с производительностью сети. Эти пакеты конкретно указывают на неэффективность сети, которая является результатом потери пакетов. Но потеря пакетов не обязательно вызвана проблемами с производительностью Azure. Проблемы с производительностью могут быть результатом проблем с приложением, проблем с операционной системой или других проблем, которые могут не быть непосредственно связаны с платформой Azure.

Кроме того, имейте в виду, что некоторые ретрансляции и дубликаты ACKs являются нормальными в сети. Протоколы TCP были построены, чтобы быть надежными. Доказательства этих пакетов TCP в захвате пакетов не обязательно указывают на системную сетевую проблему, если они не являются чрезмерными.

Тем не менее, эти типы пакетов являются признаками того, что пропускная способность TCP не достигает своей максимальной производительности, по причинам, обсуждаемым в других разделах этой статьи.

## <a name="next-steps"></a>Дальнейшие действия

Теперь, когда вы узнали о настройке производительности TCP/IP для Azure VMs, вы можете прочитать о других соображениях [планирования виртуальных сетей](https://docs.microsoft.com/azure/virtual-network/virtual-network-vnet-plan-design-arm) или [узнать больше о подключении и настройке виртуальных сетей.](https://docs.microsoft.com/azure/virtual-network/)
