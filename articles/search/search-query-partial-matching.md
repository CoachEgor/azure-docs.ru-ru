---
title: Частичные термины, шаблоны и специальные символы
titleSuffix: Azure Cognitive Search
description: Используйте запросы подстановочных знаков, regex и префиксов, чтобы соответствовать всем или частичным терминам в запросе Azure Cognitive Search. Трудносовпадающие шаблоны, включающие специальные символы, могут быть решены с помощью полного синтаксиса запросов и пользовательских анализаторов.
manager: nitinme
author: HeidiSteen
ms.author: heidist
ms.service: cognitive-search
ms.topic: conceptual
ms.date: 04/09/2020
ms.openlocfilehash: 5a05f2973ac17460250fb3e80eb7bc0da9849940
ms.sourcegitcommit: 8dc84e8b04390f39a3c11e9b0eaf3264861fcafc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/13/2020
ms.locfileid: "81262882"
---
# <a name="partial-term-search-and-patterns-with-special-characters-wildcard-regex-patterns"></a>Частичный поиск термина и шаблоны со специальными символами (wildcard, regex, шаблоны)

*Частичный поиск термина* относится к запросам, состоящим из фрагментов термина, где вместо целого термина, возможно, есть только начало, середина или конец термина (иногда называемый префиксом, инфиксом или запросами суффикса). *Шаблон* может быть комбинацией фрагментов, часто с особыми символами, такими как тире или слэши, которые являются частью строки запроса. Общие случаи использования включают запрос на части номера телефона, URL- н.с. людей или кодов продуктов или сложных слов.

Частичный поиск шаблонов может быть проблематичным, если индекс не имеет терминов в ожидаемом формате. Во время [фазы лексического анализа](search-lucene-query-architecture.md#stage-2-lexical-analysis) индексирования (при условии анализа стандарта по умолчанию) специальные символы отбрасываются, составные и составные строки расщепляются, а белое пространство удаляется; все это может привести к сбою запросов шаблонов, когда не найдено совпадений. Например, такой номер `+1 (425) 703-6214` `"1"`телефона (токенизированный как , `"425"` `"703"`, ) `"3-62"` `"6214"`не будет отображаться в запросе, потому что это содержимое на самом деле не существует в индексе. 

Решение состоит в том, чтобы вызвать анализатор, который сохраняет полную строку, включая пробелы и специальные символы, если это необходимо, так что вы можете соответствовать на частичных условиях и шаблонах. Создание дополнительного поля для нетронутой строки, а также использование контент-сохраняющего анализатора, является основой решения.

> [!TIP]
> Знакомы с Почтальоном и REST AIS? [Загрузите коллекцию примеров запросов](https://github.com/Azure-Samples/azure-search-postman-samples/tree/master/full-syntax-examples) на запрос о частичных терминах и специальных символах, описанных в этой статье.

## <a name="what-is-partial-search-in-azure-cognitive-search"></a>Что такое частичный поиск в Azure Cognitive Search

В Azure Cognitive Search в следующих формах доступен частичный поиск и шаблон:

+ [Поиск префикса](query-simple-syntax.md#prefix-search), такие как `search=cap*`, соответствие на "Cap'n Джека Waterfront Inn" или "Gacc Capital". Для поиска префиксов можно использовать простой синтаксис запроса или полный синтаксис запроса Lucene.

+ [Поиск Wildcard](query-lucene-syntax.md#bkmk_wildcard) или [регулярные выражения,](query-lucene-syntax.md#bkmk_regex) которые ищут шаблон или части встроенной строки. Wildcard и регулярные выражения требуют полного синтаксиса Lucene. Суффикс и индексные запросы формулируются как регулярное выражение.

  Некоторые примеры частичного поиска термина включают в себя следующее. Для запроса суффикса, учитывая термин "буквенное",`search=/.*numeric.*/`вы будете использовать поиск подстановочных знаков () для поиска совпадения. Для частичного термина, включающий внутренние символы, такие как фрагмент URL, может потребоваться добавить символы побега. В JSON, вперед `/` слэш сбежал `\`с отсталым черта . Таким образом, `search=/.*microsoft.com\/azure\/.*/` синтаксис для фрагмента URL "microsoft.com/azure/".

Как уже отмечалось, все вышеперечисленное требует, чтобы индекс содержит строки в формате, способствующий сопоставлению шаблонов, что стандартный анализатор не обеспечивает. Следуя шагам в этой статье, можно убедиться, что для поддержки этих сценариев существует необходимое содержимое.

## <a name="solving-partialpattern-search-problems"></a>Решение проблем частичного/поиска шаблонов

Когда вам нужно искать фрагменты или шаблоны или специальные символы, вы можете переопределить анализатор по умолчанию с пользовательским анализатором, который работает в соответствии с более простыми правилами токенизации, сохраняя всю строку. Шаг назад, подход выглядит следующим образом:

+ Определите поле для хранения нетронутой версии строки (если вы хотите анализировать и не анализировать текст)
+ Выберите предопределенный анализатор или определите пользовательский анализатор для вывода неанализируемых нетронутых строк
+ Назначить пользовательский анализатор на поле
+ Создание и тестирование индекса

> [!TIP]
> Оценка анализаторов является итеративным процессом, который требует частых перестроек индекса. Вы можете сделать этот шаг проще, используя Postman, REST AIS для [создания индекса,](https://docs.microsoft.com/rest/api/searchservice/create-index) [Удалить индекс](https://docs.microsoft.com/rest/api/searchservice/delete-index),[Загрузочные документы](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents), и [поисковые документы](https://docs.microsoft.com/rest/api/searchservice/search-documents). Для «Документов нагрузки» орган запроса должен содержать небольшой репрезентативный набор данных, который необходимо протестировать (например, поле с номерами телефонов или кодами продуктов). С помощью этих AAP в той же коллекции Postman вы можете быстро пробираться по этим шагам.

## <a name="duplicate-fields-for-different-scenarios"></a>Двойные поля для различных сценариев

Анализаторы назначаются на основе поля, что означает, что вы можете создавать поля в индексе для оптимизации для различных сценариев. В частности, можно определить "featureCode" и "featureCodeRegex" для поддержки регулярного полного поиска текста на первом, и расширенный шаблон, соответствующий на втором.

```json
{
  "name": "featureCode",
  "type": "Edm.String",
  "retrievable": true,
  "searchable": true,
  "analyzer": null
},
{
  "name": "featureCodeRegex",
  "type": "Edm.String",
  "retrievable": true,
  "searchable": true,
  "analyzer": "my_custom_analyzer"
},
```

## <a name="choose-an-analyzer"></a>Выбор анализатора

При выборе анализатора, который производит токены на целые сроки, следующие анализаторы являются общими вариантами:

| Анализатор | Расширения функциональности |
|----------|-----------|
| [языковые анализаторы](index-add-language-analyzers.md) | Сохраняет дефисы в сложных словах или струнах, гласных мутациях и глагольных формах. Если шаблоны запроса включают тире, использование анализатора языка может быть достаточным. |
| [keyword](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html) | Содержимое всего поля токенизировано как единый термин. |
| [whitespace](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/WhitespaceAnalyzer.html) | Отделяется только на белых пространствах. Термины, включающие тире или другие символы, рассматриваются как единый маркер. |
| [пользовательский анализатор](index-add-custom-analyzers.md) | (рекомендуется) Создание пользовательского анализатора позволяет указать как маркеризатор, так и фильтр маркеров. Предыдущие анализаторы должны использоваться как есть. Пользовательский анализатор позволяет выбрать токенизаторы и маркерные фильтры для использования. <br><br>Рекомендуемая комбинация — [это токенизатор ключевых слов](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html) с [фильтром токенов нижнего корпуса.](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.html) Сам по себе предопределенный [анализатор ключевых слов](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html) не имеет нижнего случая какого-либо верхнего текста, что может привести к сбою запросов. Пользовательский анализатор дает вам механизм для добавления нижнего маркерного фильтра. |

Если вы используете инструмент тестирования веб-API, такой как Postman, вы можете добавить [вызов Test Analyzer REST](https://docs.microsoft.com/rest/api/searchservice/test-analyzer) для проверки токенизированного вывода.

Вы должны иметь существующий индекс для работы. Учитывая существующий индекс и поле, содержащее тире или частичные термины, можно попробовать различные анализаторы на определенных терминах, чтобы увидеть, какие токены испускаются.  

1. Проверьте анализатор Стандарта, чтобы увидеть, как термины токенизируются по умолчанию.

   ```json
   {
   "text": "SVP10-NOR-00",
   "analyzer": "standard"
   }
    ```

1. Оцените ответ, чтобы увидеть, как текст токенизирован в индексе. Обратите внимание, как каждый термин ниже, чем в более низком случае и разбит.

    ```json
    {
        "tokens": [
            {
                "token": "svp10",
                "startOffset": 0,
                "endOffset": 5,
                "position": 0
            },
            {
                "token": "nor",
                "startOffset": 6,
                "endOffset": 9,
                "position": 1
            },
            {
                "token": "00",
                "startOffset": 10,
                "endOffset": 12,
                "position": 2
            }
        ]
    }
    ```
1. Измените запрос на `whitespace` `keyword` использование или анализатор:

    ```json
    {
    "text": "SVP10-NOR-00",
    "analyzer": "keyword"
    }
    ```

1. Теперь ответ состоит из одного маркера, верхнего корпуса, с тире, сохраненной как часть строки. Если вам нужно искать по шаблону или частичному термину, движок запроса теперь имеет основание для поиска совпадения.


    ```json
    {

        "tokens": [
            {
                "token": "SVP10-NOR-00",
                "startOffset": 0,
                "endOffset": 12,
                "position": 0
            }
        ]
    }
    ```
> [!Important]
> Имейте в виду, что parsers запросов часто ниже, если они могут быть в выражении поиска при создании дерева запросов. Если вы используете анализатор, который не имеет нижнего случая ввода текста, и вы не получаете ожидаемых результатов, это может быть причиной. Решение заключается в добавлении фильтра токенов нижнего регистра, как описано в разделе "Использование пользовательских анализаторов" ниже.

## <a name="configure-an-analyzer"></a>Настройка анализатора
 
Независимо от того, оцениваете ли вы анализаторы или продвигаетесь вперед с определенной конфигурацией, необходимо указать анализатор в определении поля и, возможно, настроить анализатор, если вы не используете встроенный анализатор. При замене анализаторов обычно необходимо восстановить индекс (падение, воссоздание и перезагрузка). 

### <a name="use-built-in-analyzers"></a>Использование встроенных анализаторов

Встроенные или предопределенные анализаторы могут `analyzer` быть указаны по имени на свойстве определения поля, без дополнительной конфигурации, необходимой в индексе. Следующий пример показывает, как `whitespace` вы бы установить анализатор на поле. 

Для других сценариев и узнать больше о других встроенных анализаторов, [см.](https://docs.microsoft.com/azure/search/index-add-custom-analyzers#predefined-analyzers-reference) 

```json
    {
      "name": "phoneNumber",
      "type": "Edm.String",
      "key": false,
      "retrievable": true,
      "searchable": true,
      "analyzer": "whitespace"
    }
```

### <a name="use-custom-analyzers"></a>Использование пользовательских анализаторов

Если вы используете [пользовательский анализатор,](index-add-custom-analyzers.md)определите его в индексе с помощью пользовательского комбинации токенизатора, фильтра токенов, с возможными настройками конфигурации. Далее, ссылайтесь на определение поля, точно так же, как вы бы встроенный анализатор.

Когда целью является токенизация на всю срок, рекомендуется пользовательский анализатор, состоящий из **маркеризатора ключевых слов** и **фильтра токенов нижнего корпуса.**

+ Токенизатор ключевых слов создает один маркер для всего содержимого поля.
+ Фильтр маркера нижнего регистра преобразует буквы верхнего корпуса в текст нижнего корпуса. Анализы запросов обычно нижний регистр любых входов текста верхнего регистра. Нижняя оболочка гомогенизируют входы с токенизированными терминами.

Ниже приводится пример пользовательского анализатора, который предоставляет маркеризатор ключевых слов и фильтр токенов нижнего регистра.

```json
{
"fields": [
  {
  "name": "accountNumber",
  "analyzer":"myCustomAnalyzer",
  "type": "Edm.String",
  "searchable": true,
  "filterable": true,
  "retrievable": true,
  "sortable": false,
  "facetable": false
  }
],

"analyzers": [
  {
  "@odata.type":"#Microsoft.Azure.Search.CustomAnalyzer",
  "name":"myCustomAnalyzer",
  "charFilters":[],
  "tokenizer":"keyword_v2",
  "tokenFilters":["lowercase"]
  }
],
"tokenizers":[],
"charFilters": [],
"tokenFilters": []
```

> [!NOTE]
> Токенизатор `keyword_v2` `lowercase` и маркерфильтр известны системе и используют их конфигурации по умолчанию, поэтому вы можете ссылаться на них по имени, не определяя их в первую очередь.

## <a name="build-and-test"></a>Сборка и тестирование

После определения индекса с анализаторами и определениями полей, поддерживающих сценарий, загрузите документы с репрезентативными строками, чтобы можно было протестировать частичные строки запросов. 

Предыдущие разделы объясняли логику. Этот раздел проходит через каждый API, который следует вызывать при тестировании решения. Как отмечалось ранее, если вы используете интерактивный веб-инструмент тестирования, такие как Postman, вы можете пройти через эти задачи быстро.

+ [Удалить индекс](https://docs.microsoft.com/rest/api/searchservice/delete-index) удаляет существующий индекс с тем же именем, так что вы можете воссоздать его.

+ [Создание индекса](https://docs.microsoft.com/rest/api/searchservice/create-index) создает структуру индекса в поисковой службе, включая определения анализаторов и поля со спецификацией анализатора.

+ [Загрузить документы](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents) импортируют документы, имеющие ту же структуру, что и индекс, а также содержимое, способное к поиску. После этого шага индекс готов к запросу или тестированию.

+ [Анализатор теста](https://docs.microsoft.com/rest/api/searchservice/test-analyzer) был представлен в [Выберите анализатор](#choose-an-analyzer). Проверьте некоторые строки в индексе, используя различные анализаторы, чтобы понять, как термины токенизируются.

+ [Документы поиска](https://docs.microsoft.com/rest/api/searchservice/search-documents) объясняют, как построить запрос запроса запроса, используя либо [простой синтаксис,](query-simple-syntax.md) либо [полный синтаксис Lucene](query-lucene-syntax.md) для подстановочных и регулярных выражений.

  Для частичных запросов термина, таких как запрос "3-6214", чтобы найти совпадение на "No 1 (425) 703-6214", вы можете использовать простой синтаксис: `search=3-6214&queryType=simple`.

  Для запросов infix и suffix, таких как запрос "num" или "числовая, чтобы найти совпадение на "буквенно-цифровой", используйте полный синтаксис Lucene и регулярное выражение:`search=/.*num.*/&queryType=full`

## <a name="tips-and-best-practices"></a>Советы и рекомендации

### <a name="tune-query-performance"></a>Настройка производительности запросов

При реализации рекомендуемой конфигурации, включающая keyword_v2 маркеризатора и фильтр маркеров нижего регистра, можно заметить снижение производительности запроса из-за дополнительной обработки фильтра токенов по сравнению с существующими маркерами в индексе. 

Следующий пример добавляет [EdgeNGramTokenFilter,](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html) чтобы сделать префикс совпадений быстрее. Дополнительные токены генерируются для комбинаций символов 2-25 символов, которые включают символы: (не только MS, MSF, MSFT, MSFT/, MSFT/S, MSFT/S, MSFT/S, MSFT/S). Как вы можете себе представить, дополнительная токенизация приводит к более широкому индексу.

```json
{
"fields": [
  {
  "name": "accountNumber",
  "analyzer":"myCustomAnalyzer",
  "type": "Edm.String",
  "searchable": true,
  "filterable": true,
  "retrievable": true,
  "sortable": false,
  "facetable": false
  }
],

"analyzers": [
  {
  "@odata.type":"#Microsoft.Azure.Search.CustomAnalyzer",
  "name":"myCustomAnalyzer",
  "charFilters":[],
  "tokenizer":"keyword_v2",
  "tokenFilters":["lowercase", "my_edgeNGram"]
  }
],
"tokenizers":[],
"charFilters": [],
"tokenFilters": [
  {
  "@odata.type":"#Microsoft.Azure.Search.EdgeNGramTokenFilterV2",
  "name":"my_edgeNGram",
  "minGram": 2,
  "maxGram": 25,
  "side": "front"
  }
]
```

### <a name="use-different-analyzers-for-indexing-and-query-processing"></a>Используйте различные анализаторы для индексации и обработки запросов

Анализаторы вызываются во время индексации и во время выполнения запроса. Обычно используется один и тот же анализатор для обоих, но вы можете настроить пользовательские анализаторы для каждой рабочей нагрузки. Переопределения анализатора указаны в `analyzers` [определении индекса](https://docs.microsoft.com/rest/api/searchservice/create-index) в разделе, а затем ссылаются на конкретные поля. 

Когда пользовательский анализ требуется только во время индексации, можно применить пользовательский анализатор для простой индексации и продолжать использовать стандартный анализатор Lucene (или другой анализатор) для запросов.

Чтобы указать анализ, связанный с ролевым и стечением роли, можно настроить свойства на поле для каждого из них, параметр и `indexAnalyzer` `searchAnalyzer` вместо свойства по умолчанию. `analyzer`

```json
"name": "featureCode",
"indexAnalyzer":"my_customanalyzer",
"searchAnalyzer":"standard",
```

## <a name="next-steps"></a>Дальнейшие действия

В этой статье объясняется, как анализаторы вносят свой вклад в проблемы запросов и решают проблемы запросов. В качестве следующего шага взгляните на влияние анализатора на индексацию и обработку запросов. В частности, рассмотрите возможность использования API Анализа Текста для возврата токенизированного вывода, чтобы можно было точно увидеть, что создает анализатор для вашего индекса.

+ [Языковые анализаторы](search-language-support.md)
+ [Анализы для обработки текста в Azure Cognitive Search](search-analyzers.md)
+ [Анализ API текста (REST)](https://docs.microsoft.com/rest/api/searchservice/test-analyzer)
+ [Как работает полный поиск текста (архитектура запросов)](search-lucene-query-architecture.md)