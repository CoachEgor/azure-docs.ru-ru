---
title: Шаблоны матчей и специальные символы
titleSuffix: Azure Cognitive Search
description: Используйте подстановочные карты и запросы префиксов, чтобы соответствовать в целом или частично в запросе Azure Cognitive Search. Трудносовпадающие шаблоны, включающие специальные символы, могут быть решены с помощью полного синтаксиса запросов и пользовательских анализаторов.
manager: nitinme
author: HeidiSteen
ms.author: heidist
ms.service: cognitive-search
ms.topic: conceptual
ms.date: 01/14/2020
ms.openlocfilehash: f78ba5b351a3da46d7b8b3780cf00772c4f3b2ea
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "80289317"
---
# <a name="match-on-patterns-and-special-characters-dashes"></a>Матч по шаблонам и специальным символам (dashes)

Для запросов, включающие`-, *, (, ), /, \, =`специальные символы (), или для шаблонов запросов, основанных на частичных терминах в течение более крупного срока, обычно необходимы дополнительные шаги конфигурации для обеспечения того, чтобы индекс содержит ожидаемое содержимое в правильном формате. 

По умолчанию, номер `+1 (425) 703-6214` телефона, как `"1"` `"425"`токенизирован как , , `"703"`. `"6214"` Как вы можете себе `"3-62"`представить, поиск на, частичные термины, которые включают тире, не удастся, потому что содержание на самом деле не существует в индексе. 

Когда вам нужно искать на частичных строках или специальных символах, вы можете переопределить анализатор по умолчанию с пользовательским анализатором, который работает в соответствии с более простыми правилами токенизации, сохраняя целые термины, необходимые, когда строки запроса включают части термина или специальные Символов. Шаг назад, подход выглядит следующим образом:

+ Выберите предопределенный анализатор или определите пользовательский анализатор, который производит желаемый выход
+ Назначить анализатор на поле
+ Создайте индекс и протест

Эта статья вас через эти задачи. Описанный здесь подход полезен и в других сценариях: подстановочные знаки и регулярные запросы выражения также нуждаются в целых терминах в качестве основы для сопоставления шаблонов. 

> [!TIP]
> Оценка аналиров является итеративным процессом, который требует частой перестроения индексов. Вы можете сделать этот шаг проще, используя Postman, REST AIS для [создания индекса,](https://docs.microsoft.com/rest/api/searchservice/create-index) [Удалить индекс](https://docs.microsoft.com/rest/api/searchservice/delete-index),[Загрузочные документы](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents), и [поисковые документы](https://docs.microsoft.com/rest/api/searchservice/search-documents). Для «Документов нагрузки» орган запроса должен содержать небольшой репрезентативный набор данных, который необходимо протестировать (например, поле с номерами телефонов или кодами продуктов). С помощью этих AAP в той же коллекции Postman вы можете быстро пробираться по этим шагам.

## <a name="choosing-an-analyzer"></a>Выбор анализатора

При выборе анализатора, который производит токены на целые сроки, следующие анализаторы являются общими вариантами:

| Анализатор | поведения |
|----------|-----------|
| [keyword](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html) | Содержимое всего поля токенизировано как единый термин. |
| [whitespace](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/WhitespaceAnalyzer.html) | Отделяется только на белых пространствах. Термины, включающие тире или другие символы, рассматриваются как единый маркер. |
| [пользовательский анализатор](index-add-custom-analyzers.md) | (рекомендуется) Создание пользовательского анализатора позволяет указать как маркеризатор, так и фильтр маркеров. Предыдущие анализаторы должны использоваться как есть. Пользовательский анализатор позволяет выбрать токенизаторы и маркерные фильтры для использования. <br><br>Рекомендуемая комбинация — [это токенизатор ключевых слов](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html) с [фильтром токенов нижнего корпуса.](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.html) Сам по себе предопределенный [анализатор ключевых слов](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html) не имеет нижнего случая какого-либо верхнего текста, что может привести к сбою запросов. Пользовательский анализатор дает вам механизм для добавления нижнего маркерного фильтра. |

Если вы используете инструмент тестирования веб-API, такой как Postman, вы можете добавить [вызов Test Analyzer REST](https://docs.microsoft.com/rest/api/searchservice/test-analyzer) для проверки токенизированного вывода. Учитывая существующий индекс и поле, содержащее тире или частичные термины, можно попробовать различные анализаторы на определенных терминах, чтобы увидеть, какие токены испускаются.  

1. Проверьте анализатор Стандарта, чтобы увидеть, как термины токенизируются по умолчанию.

   ```json
   {
   "text": "SVP10-NOR-00",
   "analyzer": "standard"
   }
    ```

1. Оцените ответ, чтобы увидеть, как текст токенизирован в индексе. Обратите внимание, как каждый термин ниже, чем в более низком случае и разбит.

    ```json
    {
        "tokens": [
            {
                "token": "svp10",
                "startOffset": 0,
                "endOffset": 5,
                "position": 0
            },
            {
                "token": "nor",
                "startOffset": 6,
                "endOffset": 9,
                "position": 1
            },
            {
                "token": "00",
                "startOffset": 10,
                "endOffset": 12,
                "position": 2
            }
        ]
    }
    ```
1. Измените запрос на `whitespace` `keyword` использование или анализатор:

    ```json
    {
    "text": "SVP10-NOR-00",
    "analyzer": "keyword"
    }
    ```

1. Теперь ответ состоит из одного маркера, верхнего корпуса, с тире, сохраненной как часть строки. Если вам нужно искать по шаблону или частичному термину, движок запроса теперь имеет основание для поиска совпадения.


    ```json
    {

        "tokens": [
            {
                "token": "SVP10-NOR-00",
                "startOffset": 0,
                "endOffset": 12,
                "position": 0
            }
        ]
    }
    ```
> [!Important]
> Имейте в виду, что parsers запросов часто ниже, если они могут быть в выражении поиска при создании дерева запросов. Если вы используете анализатор, который не имеет нижнего случая ввода текста, и вы не получаете ожидаемых результатов, это может быть причиной. Решение заключается в добавлении фильтра маркеров lwower.case.

## <a name="analyzer-definitions"></a>Определения анализатора
 
Независимо от того, оцениваете ли вы анализаторы или продвигаетесь вперед с определенной конфигурацией, необходимо указать анализатор в определении поля и, возможно, настроить анализатор, если вы не используете встроенный анализатор. При замене анализаторов обычно необходимо восстановить индекс (падение, воссоздание и перезагрузка). 

### <a name="use-built-in-analyzers"></a>Использование встроенных анализаторов

Встроенные или предопределенные анализаторы могут `analyzer` быть указаны по имени на свойстве определения поля, без дополнительной конфигурации, необходимой в индексе. Следующий пример показывает, как `whitespace` вы бы установить анализатор на поле.

```json
    {
      "name": "phoneNumber",
      "type": "Edm.String",
      "key": false,
      "retrievable": true,
      "searchable": true,
      "analyzer": "whitespace"
    }
```
Для получения дополнительной информации обо всех доступных встроенных анализаторов, [см.](https://docs.microsoft.com/azure/search/index-add-custom-analyzers#predefined-analyzers-reference) 

### <a name="use-custom-analyzers"></a>Использование пользовательских анализаторов

Если вы используете [пользовательский анализатор,](index-add-custom-analyzers.md)определите его в индексе с помощью пользовательского сочетания токенизатора, токенфильтра, с возможными настройками конфигурации. Далее, ссылайтесь на определение поля, точно так же, как вы бы встроенный анализатор.

Когда целью является токенизация на всю срок, рекомендуется пользовательский анализатор, состоящий из **маркеризатора ключевых слов** и **фильтра токенов нижнего корпуса.**

+ Токенизатор ключевых слов создает один маркер для всего содержимого поля.
+ Фильтр маркера нижнего регистра преобразует буквы верхнего корпуса в текст нижнего корпуса. Анализы запросов обычно нижний регистр любых входов текста верхнего регистра. Снижение гомогенизации входных данных с помощью токенизированных терминов.

Ниже приводится пример пользовательского анализатора, который предоставляет маркеризатор ключевых слов и фильтр токенов нижнего регистра.

```json
{
"fields": [
  {
  "name": "accountNumber",
  "analyzer":"myCustomAnalyzer",
  "type": "Edm.String",
  "searchable": true,
  "filterable": true,
  "retrievable": true,
  "sortable": false,
  "facetable": false
  }
],

"analyzers": [
  {
  "@odata.type":"#Microsoft.Azure.Search.CustomAnalyzer",
  "name":"myCustomAnalyzer",
  "charFilters":[],
  "tokenizer":"keyword_v2",
  "tokenFilters":["lowercase"]
  }
],
"tokenizers":[],
"charFilters": [],
"tokenFilters": []
```

> [!NOTE]
> Токенизатор `keyword_v2` `lowercase` и маркерфильтр известны системе и используют их конфигурации по умолчанию, поэтому вы можете ссылаться на них по имени, не определяя их в первую очередь.

## <a name="tips-and-best-practices"></a>Советы и рекомендации

### <a name="tune-query-performance"></a>Настройка производительности запросов

При реализации рекомендуемой конфигурации, включающая keyword_v2 маркеризатора и фильтр маркеров нижего регистра, можно заметить снижение производительности запроса из-за дополнительной обработки фильтра токенов по сравнению с существующими маркерами в индексе. 

Следующий пример добавляет [EdgeNGramTokenFilter,](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html) чтобы сделать префикс совпадений быстрее. Дополнительные токены генерируются для комбинаций символов 2-25 символов, которые включают символы: (не только MS, MSF, MSFT, MSFT/, MSFT/S, MSFT/S, MSFT/S, MSFT/S). Как вы можете себе представить, дополнительная токенизация приводит к более широкому индексу.

```json
{
"fields": [
  {
  "name": "accountNumber",
  "analyzer":"myCustomAnalyzer",
  "type": "Edm.String",
  "searchable": true,
  "filterable": true,
  "retrievable": true,
  "sortable": false,
  "facetable": false
  }
],

"analyzers": [
  {
  "@odata.type":"#Microsoft.Azure.Search.CustomAnalyzer",
  "name":"myCustomAnalyzer",
  "charFilters":[],
  "tokenizer":"keyword_v2",
  "tokenFilters":["lowercase", "my_edgeNGram"]
  }
],
"tokenizers":[],
"charFilters": [],
"tokenFilters": [
  {
  "@odata.type":"#Microsoft.Azure.Search.EdgeNGramTokenFilterV2",
  "name":"my_edgeNGram",
  "minGram": 2,
  "maxGram": 25,
  "side": "front"
  }
]
```

### <a name="use-different-analyzers-for-indexing-and-query-processing"></a>Используйте различные анализаторы для индексации и обработки запросов

Анализаторы вызываются во время индексации и во время выполнения запроса. Обычно используется один и тот же анализатор для обоих, но вы можете настроить пользовательские анализаторы для каждой рабочей нагрузки. Переопределения анализатора указаны в `analyzers` [определении индекса](https://docs.microsoft.com/rest/api/searchservice/create-index) в разделе, а затем ссылаются на конкретные поля. 

Когда пользовательский анализ требуется только во время индексации, можно применить пользовательский анализатор для простой индексации и продолжать использовать стандартный анализатор Lucene (или другой анализатор) для запросов.

Чтобы указать анализ, связанный с ролевым и стечением роли, можно настроить свойства на поле для каждого из них, параметр и `indexAnalyzer` `searchAnalyzer` вместо свойства по умолчанию. `analyzer`

```json
"name": "featureCode",
"indexAnalyzer":"my_customanalyzer",
"searchAnalyzer":"standard",
```

### <a name="duplicate-fields-for-different-scenarios"></a>Двойные поля для различных сценариев

Другой вариант использует назначение анализализатора на поле для оптимизации для различных сценариев. В частности, можно определить "featureCode" и "featureCodeRegex" для поддержки регулярного полного поиска текста на первом, и расширенный шаблон, соответствующий на втором.

```json
{
  "name": "featureCode",
  "type": "Edm.String",
  "retrievable": true,
  "searchable": true,
  "analyzer": null
},
{
  "name": "featureCodeRegex",
  "type": "Edm.String",
  "retrievable": true,
  "searchable": true,
  "analyzer": "my_customanalyzer"
},
```

## <a name="next-steps"></a>Дальнейшие действия

В этой статье объясняется, как анализаторы вносят свой вклад в проблемы запросов и решают проблемы запросов. В качестве следующего шага взгляните на влияние анализатора на индексацию и обработку запросов. В частности, рассмотрите возможность использования API Анализа Текста для возврата токенизированного вывода, чтобы можно было точно увидеть, что создает анализатор для вашего индекса.

+ [Языковые анализаторы](search-language-support.md)
+ [Анализы для обработки текста в Azure Cognitive Search](search-analyzers.md)
+ [Анализ API текста (REST)](https://docs.microsoft.com/rest/api/searchservice/test-analyzer)
+ [Как работает полный поиск текста (архитектура запросов)](search-lucene-query-architecture.md)
