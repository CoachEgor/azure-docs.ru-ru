---
title: Частичные термины, шаблоны и специальные символы
titleSuffix: Azure Cognitive Search
description: Используйте запросы подстановочных знаков, regex и префиксов, чтобы соответствовать всем или частичным терминам в запросе Azure Cognitive Search. Трудносовпадающие шаблоны, включающие специальные символы, могут быть решены с помощью полного синтаксиса запросов и пользовательских анализаторов.
manager: nitinme
author: HeidiSteen
ms.author: heidist
ms.service: cognitive-search
ms.topic: conceptual
ms.date: 04/02/2020
ms.openlocfilehash: 3e0e0291ff855b4502224466e17696a4fe668c2a
ms.sourcegitcommit: 62c5557ff3b2247dafc8bb482256fef58ab41c17
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/03/2020
ms.locfileid: "80656001"
---
# <a name="partial-term-search-in-azure-cognitive-search-queries-wildcard-regex-fuzzy-search-patterns"></a>Частичный поиск терминов в запросах Azure Cognitive Search (wildcard, regex, нечеткий поиск, шаблоны)

*Частичный поиск термина* относится к запросам, состоящим из фрагментов термина, таких как первая, последняя или внутренняя части строки, или шаблон, состоящий из комбинации фрагментов, часто разделенных специальными символами, такими как тире или слэши. Общие случаи использования включают запрос на части номера телефона, URL- н.с. людей или кодов продуктов или сложных слов.

Частичный поиск может быть проблематичным, поскольку сам индекс обычно не хранит термины таким образом, что это способствует частичному сопоставлению строк и шаблонов. На этапе анализа текста индексирование отбрасываются специальные символы, составные и составные строки расходятся, в результате чего запросы шаблонов терпят неудачу при отсутствии совпадения. Например, такой номер `+1 (425) 703-6214` `"1"`телефона (токенизированный как , `"425"` `"703"`, ) `"3-62"` `"6214"`не будет отображаться в запросе, потому что это содержимое на самом деле не существует в индексе. 

Решение заключается в хранении нетронутых версий этих строк в индексе, чтобы можно было поддерживать сценарии частичного поиска. Создание дополнительного поля для нетронутой строки, а также использование контент-сохраняющего анализатора, является основой решения.

## <a name="what-is-partial-search-in-azure-cognitive-search"></a>Что такое частичный поиск в Azure Cognitive Search

В Azure Cognitive Search частичный поиск доступен в следующих формах:

+ [Поиск префикса](query-simple-syntax.md#prefix-search), такие как `search=cap*`, соответствие на "Cap'n Джека Waterfront Inn" или "Gacc Capital". Для поиска префиксов можно использовать синтаксис простого запроса.
+ [Поиск Wildcard](query-lucene-syntax.md#bkmk_wildcard) или [регулярные выражения,](query-lucene-syntax.md#bkmk_regex) которые ищут шаблон или части встроенной строки, включая суффикс. Например, учитывая термин "буквенное", вы будете`search=/.*numeric.*/`использовать поиск подстановочных знаков () для суффикса запроса совпадают на этот термин. Wildcard и регулярные выражения требуют полного синтаксиса Lucene.

Когда в клиентском приложении требуется какой-либо из вышеуказанных типов запросов, выполните последующие действия в этой статье, чтобы обеспечить наличие необходимого содержимого в индексе.

## <a name="solving-partial-search-problems"></a>Решение проблем частичного поиска

Когда вам нужно искать по шаблонам или специальным символам, вы можете переопределить анализатор по умолчанию с пользовательским анализатором, который работает в соответствии с более простыми правилами токенизации, сохраняя всю строку. Шаг назад, подход выглядит следующим образом:

+ Определите поле для хранения нетронутой версии строки (если вы хотите анализировать и не анализировать текст)
+ Выберите предопределенный анализатор или определите пользовательский анализатор для вывода нетронутой строки
+ Назначить анализатор на поле
+ Создание и тестирование индекса

> [!TIP]
> Оценка анализаторов является итеративным процессом, который требует частых перестроек индекса. Вы можете сделать этот шаг проще, используя Postman, REST AIS для [создания индекса,](https://docs.microsoft.com/rest/api/searchservice/create-index) [Удалить индекс](https://docs.microsoft.com/rest/api/searchservice/delete-index),[Загрузочные документы](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents), и [поисковые документы](https://docs.microsoft.com/rest/api/searchservice/search-documents). Для «Документов нагрузки» орган запроса должен содержать небольшой репрезентативный набор данных, который необходимо протестировать (например, поле с номерами телефонов или кодами продуктов). С помощью этих AAP в той же коллекции Postman вы можете быстро пробираться по этим шагам.

## <a name="duplicate-fields-for-different-scenarios"></a>Двойные поля для различных сценариев

Анализаторы назначаются на основе поля, что означает, что вы можете создавать поля в индексе для оптимизации для различных сценариев. В частности, можно определить "featureCode" и "featureCodeRegex" для поддержки регулярного полного поиска текста на первом, и расширенный шаблон, соответствующий на втором.

```json
{
  "name": "featureCode",
  "type": "Edm.String",
  "retrievable": true,
  "searchable": true,
  "analyzer": null
},
{
  "name": "featureCodeRegex",
  "type": "Edm.String",
  "retrievable": true,
  "searchable": true,
  "analyzer": "my_customanalyzer"
},
```

## <a name="choose-an-analyzer"></a>Выбор анализатора

При выборе анализатора, который производит токены на целые сроки, следующие анализаторы являются общими вариантами:

| Анализатор | поведения |
|----------|-----------|
| [keyword](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html) | Содержимое всего поля токенизировано как единый термин. |
| [whitespace](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/WhitespaceAnalyzer.html) | Отделяется только на белых пространствах. Термины, включающие тире или другие символы, рассматриваются как единый маркер. |
| [пользовательский анализатор](index-add-custom-analyzers.md) | (рекомендуется) Создание пользовательского анализатора позволяет указать как маркеризатор, так и фильтр маркеров. Предыдущие анализаторы должны использоваться как есть. Пользовательский анализатор позволяет выбрать токенизаторы и маркерные фильтры для использования. <br><br>Рекомендуемая комбинация — [это токенизатор ключевых слов](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html) с [фильтром токенов нижнего корпуса.](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.html) Сам по себе предопределенный [анализатор ключевых слов](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html) не имеет нижнего случая какого-либо верхнего текста, что может привести к сбою запросов. Пользовательский анализатор дает вам механизм для добавления нижнего маркерного фильтра. |

Если вы используете инструмент тестирования веб-API, такой как Postman, вы можете добавить [вызов Test Analyzer REST](https://docs.microsoft.com/rest/api/searchservice/test-analyzer) для проверки токенизированного вывода.

Вы должны иметь существующий индекс для работы. Учитывая существующий индекс и поле, содержащее тире или частичные термины, можно попробовать различные анализаторы на определенных терминах, чтобы увидеть, какие токены испускаются.  

1. Проверьте анализатор Стандарта, чтобы увидеть, как термины токенизируются по умолчанию.

   ```json
   {
   "text": "SVP10-NOR-00",
   "analyzer": "standard"
   }
    ```

1. Оцените ответ, чтобы увидеть, как текст токенизирован в индексе. Обратите внимание, как каждый термин ниже, чем в более низком случае и разбит.

    ```json
    {
        "tokens": [
            {
                "token": "svp10",
                "startOffset": 0,
                "endOffset": 5,
                "position": 0
            },
            {
                "token": "nor",
                "startOffset": 6,
                "endOffset": 9,
                "position": 1
            },
            {
                "token": "00",
                "startOffset": 10,
                "endOffset": 12,
                "position": 2
            }
        ]
    }
    ```
1. Измените запрос на `whitespace` `keyword` использование или анализатор:

    ```json
    {
    "text": "SVP10-NOR-00",
    "analyzer": "keyword"
    }
    ```

1. Теперь ответ состоит из одного маркера, верхнего корпуса, с тире, сохраненной как часть строки. Если вам нужно искать по шаблону или частичному термину, движок запроса теперь имеет основание для поиска совпадения.


    ```json
    {

        "tokens": [
            {
                "token": "SVP10-NOR-00",
                "startOffset": 0,
                "endOffset": 12,
                "position": 0
            }
        ]
    }
    ```
> [!Important]
> Имейте в виду, что parsers запросов часто ниже, если они могут быть в выражении поиска при создании дерева запросов. Если вы используете анализатор, который не имеет нижнего случая ввода текста, и вы не получаете ожидаемых результатов, это может быть причиной. Решение заключается в добавлении фильтра токенов нижнего регистра, как описано в разделе "Использование пользовательских анализаторов" ниже.

## <a name="configure-an-analyzer"></a>Настройка анализатора
 
Независимо от того, оцениваете ли вы анализаторы или продвигаетесь вперед с определенной конфигурацией, необходимо указать анализатор в определении поля и, возможно, настроить анализатор, если вы не используете встроенный анализатор. При замене анализаторов обычно необходимо восстановить индекс (падение, воссоздание и перезагрузка). 

### <a name="use-built-in-analyzers"></a>Использование встроенных анализаторов

Встроенные или предопределенные анализаторы могут `analyzer` быть указаны по имени на свойстве определения поля, без дополнительной конфигурации, необходимой в индексе. Следующий пример показывает, как `whitespace` вы бы установить анализатор на поле. Для получения дополнительной информации о доступных встроенных анализаторов, [см.](https://docs.microsoft.com/azure/search/index-add-custom-analyzers#predefined-analyzers-reference) 

```json
    {
      "name": "phoneNumber",
      "type": "Edm.String",
      "key": false,
      "retrievable": true,
      "searchable": true,
      "analyzer": "whitespace"
    }
```

### <a name="use-custom-analyzers"></a>Использование пользовательских анализаторов

Если вы используете [пользовательский анализатор,](index-add-custom-analyzers.md)определите его в индексе с помощью пользовательского комбинации токенизатора, фильтра токенов, с возможными настройками конфигурации. Далее, ссылайтесь на определение поля, точно так же, как вы бы встроенный анализатор.

Когда целью является токенизация на всю срок, рекомендуется пользовательский анализатор, состоящий из **маркеризатора ключевых слов** и **фильтра токенов нижнего корпуса.**

+ Токенизатор ключевых слов создает один маркер для всего содержимого поля.
+ Фильтр маркера нижнего регистра преобразует буквы верхнего корпуса в текст нижнего корпуса. Анализы запросов обычно нижний регистр любых входов текста верхнего регистра. Нижняя оболочка гомогенизируют входы с токенизированными терминами.

Ниже приводится пример пользовательского анализатора, который предоставляет маркеризатор ключевых слов и фильтр токенов нижнего регистра.

```json
{
"fields": [
  {
  "name": "accountNumber",
  "analyzer":"myCustomAnalyzer",
  "type": "Edm.String",
  "searchable": true,
  "filterable": true,
  "retrievable": true,
  "sortable": false,
  "facetable": false
  }
],

"analyzers": [
  {
  "@odata.type":"#Microsoft.Azure.Search.CustomAnalyzer",
  "name":"myCustomAnalyzer",
  "charFilters":[],
  "tokenizer":"keyword_v2",
  "tokenFilters":["lowercase"]
  }
],
"tokenizers":[],
"charFilters": [],
"tokenFilters": []
```

> [!NOTE]
> Токенизатор `keyword_v2` `lowercase` и маркерфильтр известны системе и используют их конфигурации по умолчанию, поэтому вы можете ссылаться на них по имени, не определяя их в первую очередь.

## <a name="build-and-test"></a>Сборка и тестирование

После определения индекса с анализаторами и определениями полей, поддерживающих сценарий, загрузите документы с репрезентативными строками, чтобы можно было протестировать частичные строки запросов. 

Предыдущие разделы объясняли логику. Этот раздел проходит через каждый API, который следует вызывать при тестировании решения. Как отмечалось ранее, если вы используете интерактивный веб-инструмент тестирования, такие как Postman, вы можете пройти через эти задачи быстро.

+ [Удалить индекс](https://docs.microsoft.com/rest/api/searchservice/delete-index) удаляет существующий индекс с тем же именем, так что вы можете воссоздать его.

+ [Создание индекса](https://docs.microsoft.com/rest/api/searchservice/create-index) создает структуру индекса в поисковой службе, включая определения анализаторов и поля со спецификацией анализатора.

+ [Загрузить документы](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents) импортируют документы, имеющие ту же структуру, что и индекс, а также содержимое, способное к поиску. После этого шага индекс готов к запросу или тестированию.

+ [Анализатор теста](https://docs.microsoft.com/rest/api/searchservice/test-analyzer) был представлен в [Выберите анализатор](#choose-an-analyzer). Проверьте некоторые строки в индексе, используя различные анализаторы, чтобы понять, как термины токенизируются.

+ [Документы поиска](https://docs.microsoft.com/rest/api/searchservice/search-documents) объясняют, как построить запрос запроса запроса, используя либо [простой синтаксис,](query-simple-syntax.md) либо [полный синтаксис Lucene](query-lucene-syntax.md) для подстановочных и регулярных выражений.

## <a name="tips-and-best-practices"></a>Советы и рекомендации

### <a name="tune-query-performance"></a>Настройка производительности запросов

При реализации рекомендуемой конфигурации, включающая keyword_v2 маркеризатора и фильтр маркеров нижего регистра, можно заметить снижение производительности запроса из-за дополнительной обработки фильтра токенов по сравнению с существующими маркерами в индексе. 

Следующий пример добавляет [EdgeNGramTokenFilter,](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html) чтобы сделать префикс совпадений быстрее. Дополнительные токены генерируются для комбинаций символов 2-25 символов, которые включают символы: (не только MS, MSF, MSFT, MSFT/, MSFT/S, MSFT/S, MSFT/S, MSFT/S). Как вы можете себе представить, дополнительная токенизация приводит к более широкому индексу.

```json
{
"fields": [
  {
  "name": "accountNumber",
  "analyzer":"myCustomAnalyzer",
  "type": "Edm.String",
  "searchable": true,
  "filterable": true,
  "retrievable": true,
  "sortable": false,
  "facetable": false
  }
],

"analyzers": [
  {
  "@odata.type":"#Microsoft.Azure.Search.CustomAnalyzer",
  "name":"myCustomAnalyzer",
  "charFilters":[],
  "tokenizer":"keyword_v2",
  "tokenFilters":["lowercase", "my_edgeNGram"]
  }
],
"tokenizers":[],
"charFilters": [],
"tokenFilters": [
  {
  "@odata.type":"#Microsoft.Azure.Search.EdgeNGramTokenFilterV2",
  "name":"my_edgeNGram",
  "minGram": 2,
  "maxGram": 25,
  "side": "front"
  }
]
```

### <a name="use-different-analyzers-for-indexing-and-query-processing"></a>Используйте различные анализаторы для индексации и обработки запросов

Анализаторы вызываются во время индексации и во время выполнения запроса. Обычно используется один и тот же анализатор для обоих, но вы можете настроить пользовательские анализаторы для каждой рабочей нагрузки. Переопределения анализатора указаны в `analyzers` [определении индекса](https://docs.microsoft.com/rest/api/searchservice/create-index) в разделе, а затем ссылаются на конкретные поля. 

Когда пользовательский анализ требуется только во время индексации, можно применить пользовательский анализатор для простой индексации и продолжать использовать стандартный анализатор Lucene (или другой анализатор) для запросов.

Чтобы указать анализ, связанный с ролевым и стечением роли, можно настроить свойства на поле для каждого из них, параметр и `indexAnalyzer` `searchAnalyzer` вместо свойства по умолчанию. `analyzer`

```json
"name": "featureCode",
"indexAnalyzer":"my_customanalyzer",
"searchAnalyzer":"standard",
```

## <a name="next-steps"></a>Дальнейшие действия

В этой статье объясняется, как анализаторы вносят свой вклад в проблемы запросов и решают проблемы запросов. В качестве следующего шага взгляните на влияние анализатора на индексацию и обработку запросов. В частности, рассмотрите возможность использования API Анализа Текста для возврата токенизированного вывода, чтобы можно было точно увидеть, что создает анализатор для вашего индекса.

+ [Языковые анализаторы](search-language-support.md)
+ [Анализы для обработки текста в Azure Cognitive Search](search-analyzers.md)
+ [Анализ API текста (REST)](https://docs.microsoft.com/rest/api/searchservice/test-analyzer)
+ [Как работает полный поиск текста (архитектура запросов)](search-lucene-query-architecture.md)