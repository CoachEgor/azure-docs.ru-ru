---
title: включить файл
description: включить файл
services: virtual-machines
author: roygara
ms.service: virtual-machines
ms.topic: include
ms.date: 04/08/2020
ms.author: rogarana
ms.custom: include file
ms.openlocfilehash: c3e5beaef7fcc9d407103834e2040957ff32984c
ms.sourcegitcommit: ae3d707f1fe68ba5d7d206be1ca82958f12751e8
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/10/2020
ms.locfileid: "81008572"
---
Общие диски Azure (предварительный просмотр) — это новая функция для управляемых дисков Azure, которая позволяет одновременно прикреплять управляемый диск к нескольким виртуальным машинам (Виртуальным машинам). Присоединение управляемого диска к нескольким ВМ позволяет развертывать новые или мигрировать в Azure.

## <a name="how-it-works"></a>Принцип работы

ВМ в кластере могут читать или писать на прилагаемый диск на основе резервирования, выбранного кластерным приложением с помощью [SCSI Persistent Reservations](https://www.t10.org/members/w_spc3.htm) (SCSI PR). SCSI PR является отраслевым стандартом, использующим приложения, работающие на базе хранилища (SAN) на местах. Включение SCSI PR на управляемом диске позволяет перенести эти приложения в Azure as-is.

Обмен управляемыми дисками предлагает общий блок хранения, которые могут быть доступны из нескольких VMs, они подвергаются как логические номера единицы (LUNs). Затем LUN представляются инициатору (VM) из цели (диска). Эти LUN выглядят как прямое присоединение-хранение (DAS) или локальный диск к VM.

Общие управляемые диски не предлагают полностью управляемую файловую систему, к которой можно получить доступ с помощью SMB/NFS. Вам необходимо использовать кластерный менеджер, например кластер Windows Server Failover Cluster (WSFC) или Pacemaker, который обрабатывает связь кластерных узлов, а также записывает блокировку.

## <a name="limitations"></a>Ограничения

[!INCLUDE [virtual-machines-disks-shared-limitations](virtual-machines-disks-shared-limitations.md)]

## <a name="disk-sizes"></a>Размеры диска

[!INCLUDE [virtual-machines-disks-shared-sizes](virtual-machines-disks-shared-sizes.md)]

## <a name="sample-workloads"></a>Примеры рабочих нагрузок

### <a name="windows"></a>Windows

Большинство кластеров на базе Windows основываются на WSFC, которая обрабатывает всю основную инфраструктуру для связи кластерных узлов, позволяя приложениям использовать параллельные шаблоны доступа. WSFC предоставляет опции на основе CSV и non-CSV в зависимости от вашей версии Windows Server. Для получения подробной информации обратитесь к [Созданию кластера failover.](https://docs.microsoft.com/windows-server/failover-clustering/create-failover-cluster)

Некоторые популярные приложения, работающие на WSFC, включают:

- Кластерные инстанции сервера S'L (FCI)
- Масштабирование файлового сервера (SoFS)
- Файловый сервер для общего использования (рабочая нагрузка IW)
- Удаленный диск профиля пользователя на настольного сервера (RDS UPD)
- SAP ASCS/SCS

### <a name="linux"></a>Linux

Кластеры Linux могут использовать кластерные менеджеры, такие как [Pacemaker.](https://wiki.clusterlabs.org/wiki/Pacemaker) Pacemaker основывается на [Corosync,](http://corosync.github.io/corosync/)позволяя кластерные коммуникации для приложений, развернутых в высокодоступных средах. Некоторые общие кластерные файловые системы включают [ocfs2](https://oss.oracle.com/projects/ocfs2/) и [gfs2.](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/global_file_system_2/ch-overview-gfs2) Вы можете манипулировать бронированием и регистрацией с помощью утилит, таких как [fence_scsi](http://manpages.ubuntu.com/manpages/eoan/man8/fence_scsi.8.html) и [sg_persist.](https://linux.die.net/man/8/sg_persist)

## <a name="persistent-reservation-flow"></a>Постоянный поток резервирования

Следующая диаграмма иллюстрирует образец 2-узлового кластерного приложения базы данных, которое использует SCSI PR, чтобы включить сбой от одного узла к другому.

![Двухузловое скопление. Приложение, работая на кластере, обрабатывает доступ к диску](media/virtual-machines-disks-shared-disks/shared-disk-updated-two-node-cluster-diagram.png)

Процесс выглядит следующим образом:

1. Кластерное приложение, работая на Azure VM1 и VM2, регистрирует свое намерение читать или писать на диск.
1. Экземпляр приложения на VM1 затем требует эксклюзивного бронирования для записи на диск.
1. Это резервирование обеспечивается на диске Azure, и база данных теперь может записываться исключительно на диск. Любые записи из экземпляра приложения на VM2 не увенчаются успехом.
1. Если экземпляр приложения на VM1 выходит из строя, экземпляр на VM2 теперь может инициировать сбой и поглощение диска базой данных.
1. Это резервирование теперь осуществляется на диске Azure, и диск больше не будет принимать записи с VM1. Он будет принимать только записи с VM2.
1. Кластерное приложение может завершить сбой базы данных и обслуживать запросы от VM2.

Следующая диаграмма иллюстрирует другую общую кластерную рабочую нагрузку, состоящую из нескольких узлов, считывающих данные с диска для выполнения параллельных процессов, таких как обучение моделям машинного обучения.

![Четыре узла VM кластера, каждый узла регистрирует намерение написать, приложение занимает эксклюзивное бронирование, чтобы правильно обрабатывать результаты записи](media/virtual-machines-disks-shared-disks/shared-disk-updated-machine-learning-trainer-model.png)

Процесс выглядит следующим образом:

1. Кластерное приложение, работая на всех ВМ, регистрирует намерение читать или писать на диск.
1. Экземпляр приложения на VM1 требует эксклюзивного резервирования для записи на диск при открытии считывания на диск ес других ВМ.
1. Это резервирование выполняется на диске Azure.
1. Все узлы в кластере теперь могут читать с диска. Только один узло записывает результаты на диск от имени всех узлов кластера.

### <a name="ultra-disks-reservation-flow"></a>Поток резервирования ультра дисков

Ультра диски предлагают дополнительную дроссельную заслонку, в общей сложности две дроссельной заслонки. Из-за этого поток резервирования ультра дисков может работать так, как описано в предыдущем разделе, или он может дроссельной заслонки и распространять производительность более детально.

:::image type="content" source="media/virtual-machines-disks-shared-disks/ultra-reservation-table.png" alt-text=" ":::

## <a name="ultra-disk-performance-throttles"></a>Ультра диск производительности дроссельной заслонки

Ультра диски имеют уникальную возможность, позволяя вам установить производительность, подвергая изменяемые атрибуты и позволяет изменить их. По умолчанию существует только два изменяемых атрибута, но общие ультрадиски имеют два дополнительных атрибута.


|Атрибут  |Описание  |
|---------|---------|
|DiskIOPSReadWrite     |Общее количество IOPS разрешено во всех VMs монтажа диска общего доступа с записью доступа.         |
|DiskMBpsReadWrite     |Общая пропускная запись (MB/s) разрешена во всех вд,конподнимающих общий диск с доступом к записи.         |
|DiskIOPSReadOnly     |Общее количество IOPS разрешено во всех VMs монтажа общего диска, как ReadOnly.         |
|DiskMBpsReadOnly     |Общая пропускная система (MB/s) разрешена во всех вд-х, устанавливающих общий диск под ReadOnly.         |

\*Применяется только к общим ультрадискам

Следующие формулы объясняют, как можно установить атрибуты производительности, поскольку они могут быть изменяемы:

- DiskIOPSReadWrite/DiskIOPSReadOnly: 
    - IOPS пределы 300 IOPS/GiB, до максимум160K IOPS на диск
    - Минимум 100 IOPS
    - DiskIOPSReadWrite - DiskIOPSReadТолько, по крайней мере, 2 IOPS/GiB
- DiskMBpsRead Запись / DiskMBpsReadТолько:
    - Лимит пропускной мощности одного диска составляет 256 киБ/с для каждого подготовленного IOPS, максимум 2000 Мбит/с на диск
    - Минимальная гарантированная пропускная плата на диске составляет 4KiB/s для каждого подготовленного IOPS, с общим базовым минимумом 1 Мбит/с

### <a name="examples"></a>Примеры

Следующие примеры изображают несколько сценариев, которые показывают, как регулирование может работать с общими ультра дисков, в частности.

#### <a name="two-nodes-cluster-using-cluster-shared-volumes"></a>Кластер двух узлов с использованием общих объемов кластера

Ниже приводится пример 2-узлового WSFC, использующего кластерные общие тома. При этой конфигурации оба ВМ имеют одновременный доступ к диску, что приводит к тому, что дроссельная заслонка ReadWrite делится на два ВМ, а дроссель ReadOnly не используется.

:::image type="complex" source="media/virtual-machines-disks-shared-disks/ultra-two-node-example.png" alt-text="CSV два узла ультра пример":::

:::image-end:::

#### <a name="two-node-cluster-without-cluster-share-volumes"></a>Двухузловый кластер без объемов кластерной доли

Ниже приводится пример 2-узлового WSFC, который не использует кластерные общие тома. При такой конфигурации только один VM имеет доступ к диску. Это приводит к тому, что дроссельная заслонка ReadWrite используется исключительно для первичного VM и readOnly дроссельной заслонки, используемой только вторичным.

:::image type="complex" source="media/virtual-machines-disks-shared-disks/ultra-two-node-no-csv.png" alt-text="CSV два узла без csv ультра диск апример":::

:::image-end:::

#### <a name="four-node-linux-cluster"></a>Кластер «Четыре узла Linux»

Ниже приводится пример кластера 4-узлового Linux с одним автором и тремя масштабными читателями. При такой конфигурации только один VM имеет доступ к диску. Это приводит к тому, что дроссельная заслонка ReadWrite используется исключительно для первичного VM и readOnly дроссельной заслонки, разделенной вторичными VMs.

:::image type="complex" source="media/virtual-machines-disks-shared-disks/ultra-four-node-example.png" alt-text="Пример четырех узлов ультра регулирования":::

:::image-end:::