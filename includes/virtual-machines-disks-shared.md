---
title: включить файл
description: включить файл
services: virtual-machines
author: roygara
ms.service: virtual-machines
ms.topic: include
ms.date: 02/18/2020
ms.author: rogarana
ms.custom: include file
ms.openlocfilehash: a14ae76e15c1adb59917e61fbcbdaa34a7efa2d8
ms.sourcegitcommit: 64def2a06d4004343ec3396e7c600af6af5b12bb
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/19/2020
ms.locfileid: "77472028"
---
Общие диски Azure (Предварительная версия) — это новая функция для управляемых дисков Azure, которая позволяет подключить управляемый диск Azure к нескольким виртуальным машинам одновременно. Присоединение управляемого диска к нескольким виртуальным машинам позволяет либо развернуть новые или перенести существующие кластерные приложения в Azure.

## <a name="how-it-works"></a>Принцип работы

Виртуальные машины в кластере могут выполнять чтение или запись на подключенный диск в соответствии с резервированием, выбранным кластеризованным приложением, с использованием [постоянных резервирований SCSI](https://www.t10.org/members/w_spc3.htm) (по протоколу SCSI). По протоколу SCSI PR — хорошо известный промышленный стандарт, используемый приложениями, работающими в локальной сети хранения данных (SAN). Включение параметра SCSI PR на управляемом диске позволяет перенести эти приложения в Azure "как есть".

Управляемые диски с включенными общими дисками предлагают общее хранилище блоков, доступное для нескольких виртуальных машин, которое предоставляется в виде логических номеров устройств (LUN). Затем LUN представляются инициатору (виртуальной машине) из целевого объекта (диска). Эти LUN похожи на виртуальную машину с прямым подключением (DAS) или локальным диском.

Управляемые диски с включенными общими дисками изначально не предлагают полностью управляемую файловую систему, доступ к которой можно получить с помощью SMB/NFS. Необходимо использовать диспетчер кластеров, например отказоустойчивый кластер Windows Server (WSFC) или Pacemaker, который управляет связью с узлом кластера, а также блокировкой записи.

## <a name="limitations"></a>Ограничения

[!INCLUDE [virtual-machines-disks-shared-limitations](virtual-machines-disks-shared-limitations.md)]

## <a name="disk-sizes"></a>Размеры диска

[!INCLUDE [virtual-machines-disks-shared-sizes](virtual-machines-disks-shared-sizes.md)]

## <a name="sample-workloads"></a>Примеры рабочих нагрузок

### <a name="windows"></a>Windows

Большинство построений кластеризации на основе Windows в WSFC, которые обрабатывают всю основную инфраструктуру для обмена данными с узлом кластера, позволяя приложениям использовать шаблоны параллельного доступа. WSFC включает параметры CSV и не на основе CSV в зависимости от используемой версии Windows Server. Дополнительные сведения см. в статье [Создание отказоустойчивого кластера](https://docs.microsoft.com/windows-server/failover-clustering/create-failover-cluster).

Ниже перечислены некоторые популярные приложения, работающие в кластере WSFC.

- Экземпляры отказоустойчивого кластера SQL Server (FCI)
- Масштабируемый файловый сервер (SoFS)
- Файловый сервер для общего использования (Рабочая нагрузка IW)
- Диск профиля пользователя удаленный рабочий стол Server (RDS UPD)
- SAP ASCS/SCS

### <a name="linux"></a>Linux

Кластеры Linux могут использовать диспетчеры кластеров, например [Pacemaker](https://wiki.clusterlabs.org/wiki/Pacemaker). Pacemaker строится на [Corosync](http://corosync.github.io/corosync/), обеспечивая связь с кластером для приложений, развернутых в средах с высокой доступностью. Некоторые распространенные кластерные файловой системы включают [OCFS2](https://oss.oracle.com/projects/ocfs2/) и [GFS2](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/global_file_system_2/ch-overview-gfs2). Вы можете управлять резервированием и регистрацией с помощью таких служебных программ, как [fence_scsi](http://manpages.ubuntu.com/manpages/eoan/man8/fence_scsi.8.html) и [sg_persist](https://linux.die.net/man/8/sg_persist).

## <a name="persistent-reservation-flow"></a>Поток постоянного резервирования

На следующей схеме показан пример приложения кластеризованной базы данных с двумя узлами, которое использует параметр SCSI PR для включения отработки отказа с одного узла на другой.

![Кластер с двумя узлами. Приложение, работающее в кластере, обрабатывает доступ к диску](media/virtual-machines-disks-shared-disks/shared-disk-updated-two-node-cluster-diagram.png)

Процесс выглядит следующим образом:

1. Кластерное приложение, работающее в Azure VM1 и VM2, регистрирует цель чтения или записи на диск.
1. Затем экземпляр приложения в VM1 принимает монопольное резервирование для записи на диск.
1. Это резервирование применяется на диске Azure, и база данных теперь может выполнять запись только на диск. Любые операции записи из экземпляра приложения в VM2 не будут выполнены.
1. Если экземпляр приложения в VM1 выходит из строя, экземпляр в VM2 может инициировать отработку отказа базы данных и перехватить диск.
1. Теперь это резервирование применяется на диске Azure, и диск больше не будет принимать записи от VM1. Он будет принимать только операции записи из VM2.
1. Кластеризованное приложение может выполнить отработку отказа базы данных и обслуживать запросы от VM2.

На следующей схеме показана еще одна общая кластеризованная Рабочая нагрузка, состоящая из нескольких узлов, считывающих данные с диска для выполнения параллельных процессов, таких как обучение моделей машинного обучения.

![Кластер виртуальных машин с четырьмя узлами. Каждый узел регистрирует намерение для записи, приложение принимает монопольное резервирование для правильной работы записи результатов.](media/virtual-machines-disks-shared-disks/shared-disk-updated-machine-learning-trainer-model.png)

Процесс выглядит следующим образом:

1. Кластерное приложение, работающее на всех виртуальных машинах, регистрирует намерение чтения или записи на диск.
1. Экземпляр приложения в VM1 принимает монопольное резервирование для записи на диск при открытии операций чтения с диска с других виртуальных машин.
1. Это резервирование принудительно применяется на диске Azure.
1. Теперь все узлы в кластере можно считывать с диска. Только один узел записывает результаты обратно на диск от имени всех узлов в кластере.