---
title: включить файл
description: включить файл
services: virtual-machines
author: roygara
ms.service: virtual-machines
ms.topic: include
ms.date: 02/18/2020
ms.author: rogarana
ms.custom: include file
ms.openlocfilehash: a14ae76e15c1adb59917e61fbcbdaa34a7efa2d8
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/28/2020
ms.locfileid: "77472028"
---
Общие диски Azure (предварительный просмотр) — это новая функция для управляемых дисков Azure, которая позволяет одновременно прикреплять управляемый диск Azure к нескольким виртуальным машинам (Виртуальным машинам). Присоединение управляемого диска к нескольким ВМ позволяет развертывать новые или мигрировать в Azure.

## <a name="how-it-works"></a>Принцип работы

ВМ в кластере могут читать или писать на прилагаемый диск на основе резервирования, выбранного кластерным приложением с помощью [SCSI Persistent Reservations](https://www.t10.org/members/w_spc3.htm) (SCSI PR). SCSI PR является известным отраслевым стандартом, использующим приложения, работающие на базе системхранения хранения данных (SAN). Включение SCSI PR на управляемом диске позволяет перенести эти приложения в Azure as-is.

Управляемые диски с общими дисками с включенным предложением общего хранения блоков, которые могут быть доступны нескольким ими, это подвергается как логические номера единицы (LUNs). Затем LUN представляются инициатору (VM) из цели (диска). Эти LUN выглядят как прямое присоединение-хранение (DAS) или локальный диск к VM.

Управляемые диски с включенными общими дисками не предлагают полностью управляемую файловую систему, доступ к которой можно получить с помощью SMB/NFS. Вам нужно будет использовать кластерный менеджер, например кластер Windows Server Failover Cluster (WSFC) или Pacemaker, который обрабатывает связь кластерных узлов, а также записывает блокировку.

## <a name="limitations"></a>Ограничения

[!INCLUDE [virtual-machines-disks-shared-limitations](virtual-machines-disks-shared-limitations.md)]

## <a name="disk-sizes"></a>Размеры диска

[!INCLUDE [virtual-machines-disks-shared-sizes](virtual-machines-disks-shared-sizes.md)]

## <a name="sample-workloads"></a>Примеры рабочих нагрузок

### <a name="windows"></a>Windows

Большинство кластеров на базе Windows основываются на WSFC, которая обрабатывает всю основную инфраструктуру для связи кластерных узлов, позволяя приложениям использовать параллельные шаблоны доступа. WSFC предоставляет опции на основе CSV и non-CSV в зависимости от вашей версии Windows Server. Для получения подробной информации обратитесь к [Созданию кластера failover.](https://docs.microsoft.com/windows-server/failover-clustering/create-failover-cluster)

Некоторые популярные приложения, работающие на WSFC, включают:

- Кластерные инстанции сервера S'L (FCI)
- Масштабирование файлового сервера (SoFS)
- Файловый сервер для общего использования (рабочая нагрузка IW)
- Удаленный диск профиля пользователя на настольного сервера (RDS UPD)
- SAP ASCS/SCS

### <a name="linux"></a>Linux

Кластеры Linux могут использовать кластерные менеджеры, такие как [Pacemaker.](https://wiki.clusterlabs.org/wiki/Pacemaker) Pacemaker основывается на [Corosync,](http://corosync.github.io/corosync/)позволяя кластерные коммуникации для приложений, развернутых в высокодоступных средах. Некоторые общие кластерные файловые системы включают [ocfs2](https://oss.oracle.com/projects/ocfs2/) и [gfs2.](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/global_file_system_2/ch-overview-gfs2) Вы можете манипулировать бронированием и регистрацией с помощью утилит, таких как [fence_scsi](http://manpages.ubuntu.com/manpages/eoan/man8/fence_scsi.8.html) и [sg_persist.](https://linux.die.net/man/8/sg_persist)

## <a name="persistent-reservation-flow"></a>Постоянный поток резервирования

Следующая диаграмма иллюстрирует образец 2-узлового кластерного приложения базы данных, которое использует SCSI PR, чтобы включить сбой от одного узла к другому.

![Двухузловое скопление. Приложение, работая на кластере, обрабатывает доступ к диску](media/virtual-machines-disks-shared-disks/shared-disk-updated-two-node-cluster-diagram.png)

Процесс выглядит следующим образом:

1. Кластерное приложение, работая на Azure VM1 и VM2, регистрирует свое намерение читать или писать на диск.
1. Экземпляр приложения на VM1 затем требует эксклюзивного бронирования для записи на диск.
1. Это резервирование обеспечивается на диске Azure, и база данных теперь может записываться исключительно на диск. Любые записи из экземпляра приложения на VM2 не увенчаются успехом.
1. Если экземпляр приложения на VM1 выходит из строя, экземпляр на VM2 теперь может инициировать сбой и поглощение диска базой данных.
1. Это резервирование теперь осуществляется на диске Azure, и диск больше не будет принимать записи с VM1. Он будет принимать только записи с VM2.
1. Кластерное приложение может завершить сбой базы данных и обслуживать запросы от VM2.

Следующая диаграмма иллюстрирует другую общую кластерную рабочую нагрузку, состоящую из нескольких узлов, считывающих данные с диска для выполнения параллельных процессов, таких как обучение моделям машинного обучения.

![Четыре узла VM кластера, каждый узла регистрирует намерение написать, приложение занимает эксклюзивное бронирование, чтобы правильно обрабатывать результаты записи](media/virtual-machines-disks-shared-disks/shared-disk-updated-machine-learning-trainer-model.png)

Процесс выглядит следующим образом:

1. Кластерное приложение, работая на всех ВМ, регистрирует намерение читать или писать на диск.
1. Экземпляр приложения на VM1 требует эксклюзивного резервирования для записи на диск при открытии считывания на диск ес других ВМ.
1. Это резервирование выполняется на диске Azure.
1. Все узлы в кластере теперь могут читать с диска. Только один узло записывает результаты на диск от имени всех узлов кластера.